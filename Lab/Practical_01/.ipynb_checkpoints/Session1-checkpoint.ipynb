{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuD-LG1kbKqs"
   },
   "source": [
    "# Session 1: Introducing Text in Python\n",
    "\n",
    "In this notebook we will start woking with Python and text strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9fk15RybKqy"
   },
   "source": [
    "## 1.1 Strings and lists\n",
    "\n",
    "A 'string' is simply a sequence of characters used to represent a document in a programming language such as Python.\n",
    "- Let's create a Python variable called 'doc' that contains a short document as a string.\n",
    "- After defining the variable, we repeat its name so as to print out its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzuBscVnbKqz"
   },
   "outputs": [],
   "source": [
    "doc = 'Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do'\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PILAgmKDbKqz"
   },
   "source": [
    "We can calculate the length of the string (in characters) by using the len() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0KzLRCjbKq0"
   },
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlH3zCkHbKq0"
   },
   "source": [
    "We can divide up the sentence into individual words by splitting it on whitespace (spaces, tabs, etc.). \n",
    "- This process is called 'tokenisation':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "826CWmTnbKq0"
   },
   "outputs": [],
   "source": [
    "doc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Fky0cvtbKq0"
   },
   "source": [
    "Note that the ouptut above is in the form of comma-separated list of strings [s1,s2,...,sn]\n",
    "- The layout above is vertical, but if you use print() command you can get a more compact horizontal ouptut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwTtRQIwbKq1"
   },
   "outputs": [],
   "source": [
    "print(doc.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7VvLRsobKq1"
   },
   "source": [
    "We didn't have to split the sentence on whitespace, we could have split it around any substring. \n",
    "- For example we could split on the comma ',' character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahRXQem1bKq1"
   },
   "outputs": [],
   "source": [
    "doc.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04A8J71QbKq1"
   },
   "source": [
    "How many words are there in the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03XDB4C1bKq2"
   },
   "outputs": [],
   "source": [
    "words = doc.split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bwb2-TTbKq2"
   },
   "source": [
    "Often in text-processing pipelines we convert all text to lower-case. \n",
    "- Since the sentence is almost all in lower-case already, let's convert it to upper-case instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-utEyCjZbKq2"
   },
   "outputs": [],
   "source": [
    "doc.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgR1MpJWbKq2"
   },
   "source": [
    "## 1.2 Loading text from a file\n",
    "Let's now read in a longer document form a text file 'Alice_Chapter1.txt'\n",
    "\n",
    "- Download the file \"Alice_Chapter1.txt\" from the \"data\" directory in the Google Drive link (where you found this notebook) and place it in the same directory as the notebook.\n",
    "- If you are using Google Colab, you will then need to upload the file by clicking on the Folder icon to the left of the notebook, then clicking on the Upload icon, and finding the file on your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw9JB10EbKq2"
   },
   "outputs": [],
   "source": [
    "f = open(\"Alice_Chapter1.txt\") \n",
    "doc2 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gPJhHrEbKq3"
   },
   "source": [
    "Print out the text as Python sees it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27CsU1gbbKq3"
   },
   "outputs": [],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEDHeg0EbKq3"
   },
   "source": [
    "Note all the backslash characters '\\\\' in the text above.  \n",
    "- Python stores text as one big string (sequence of characters). \n",
    "- Special characters such as newlines and tabs are represented by '\\\\n' and '\\\\t' respectively.\n",
    "- The quote character is used to mark the start and end of the string ('string'), so quote characters that are present in the string are prefixed by a backslash to prevent the string from ending early ('str\\\\'ing'). \n",
    "- Using the print() command, we can output the string in a format that we're more used to seeing it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m9q8V4QbKq3"
   },
   "outputs": [],
   "source": [
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiCwKJ_VbKq3"
   },
   "source": [
    "## 1.3 Splitting lines and finding words\n",
    "\n",
    "We can split the text into separate lines using splitlines() method. \n",
    "- Since there are lot of lines, we'll only print the first 5 of them by appending '[:5]' to the name of the variable contianing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLyhzsTQbKq3"
   },
   "outputs": [],
   "source": [
    "lines = doc2.splitlines()\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8AFF-WibKq4"
   },
   "source": [
    "Note that: \n",
    "- Some of the lines contain no text at all.\n",
    "- Some of the lines are surrounded by the double quote character \" becuase they contain the single quote character in the text. \n",
    "\n",
    "How many lines are there in total in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfWU5HQhbKq4"
   },
   "outputs": [],
   "source": [
    "len(doc2.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD9FbeMnbKq4"
   },
   "source": [
    "We can search for a particular word in the text: \n",
    "- For example, let's search for the word 'Rabbit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRAD91wXbKq4"
   },
   "outputs": [],
   "source": [
    "doc2.find('Rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZCPevxbbKq4"
   },
   "source": [
    "The number tells us that the word appears at the 552nd character position. \n",
    "\n",
    "We can format the output to state this explicitly:\n",
    "- We use the '+' command to concatenate strings, \n",
    "- and the str() command for converting an integer to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yT6aWipbKq4"
   },
   "outputs": [],
   "source": [
    "word = \"Rabbit\"\n",
    "mystring = \"The word '\" + word + \"' appeared at character position \" + str(doc2.find(word)) + \" in the text\"\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j34S1khxpIH3"
   },
   "source": [
    "What happens if we search for a string that does't exist in the document? \n",
    "- Try it... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCrROJ69pCa_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_1gK8GIbKq5"
   },
   "source": [
    "## 1.4 Investigating the vocabulary of a document\n",
    "\n",
    "Now let's find the vocabulary of this text by: \n",
    "- first converting the text to lowercase\n",
    "- then splitting the words on whitespace\n",
    "- then selecting only distinct words by using the set() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zkUbepNbKq5"
   },
   "outputs": [],
   "source": [
    "lowercase_doc = doc2.lower()\n",
    "words = lowercase_doc.split()\n",
    "vocab = set(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLXC6_vLbKq5"
   },
   "source": [
    "To make it easier to read, we could sort the vocabulary alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc_XT1nBbKq5"
   },
   "outputs": [],
   "source": [
    "sorted_vocab = sorted(vocab)\n",
    "print(sorted_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4ZzLRipp5KE"
   },
   "source": [
    "That looks a bit weird. What are all those bracket '(' characters doing there? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIB-6xDabKq5"
   },
   "source": [
    "## 1.5 Removing punctuation with a regular expression\n",
    "\n",
    "Notice that many of the vocabulary terms, particularly those at the start of the list, contain punctuation characters like quotes '\"', brackets '(' and exclamation marks '!'. We'll now see how to remove these puntuation characters:\n",
    "- First get a list of punctuation characters from the 'string' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCFQzseTbKq5"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wct3c1EAbKq6"
   },
   "source": [
    "The list is provided as a single string. To convert it to a list of individual characters, just call the list function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW-2BMVjbKq6"
   },
   "outputs": [],
   "source": [
    "list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxnuEuSjbKq6"
   },
   "source": [
    "Notice the double backslash character '\\\\\\\\' in the list. This is needed because backslash is used as the escape character. So if we don't put a double backslash, Python will interpret the single backslash as escaping the quote character that follows it.\n",
    "\n",
    "We can create a regular expression that will match any of those puncutation characters by simply surrounding the string of punctuation characters with square brackets: \"[]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSb8VgZrbKq6"
   },
   "outputs": [],
   "source": [
    "regex = '[' + string.punctuation + ']'\n",
    "print(regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJK_3gYDbKq6"
   },
   "source": [
    "We can use the new punctuation matching regular expression with the sub() command in the *re* (regular expression) libarary to remove the unwanted punctuation.\n",
    "- Note that the sub() routine actually performs a substitution each time it finds a match, but we will simply replace the punctuation character with an empty string: ''\n",
    "- Let's print out the first 1000 characters of the text after removing all punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRYPdI3sbKq7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "doc2_nopunctuation = re.sub(regex,'',doc2)\n",
    "print(doc2_nopunctuation[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDrmqJTbbKq7"
   },
   "source": [
    "Compare this output with the original text for the first 1000 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-7aO0mxbKq7"
   },
   "outputs": [],
   "source": [
    "print(doc2[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBjg-w_pbKq7"
   },
   "source": [
    "Now that we've removed the punctuation, let's generate the sorted vocabulary again, by:\n",
    "- converting to lowercase\n",
    "- splitting on whitespace\n",
    "- select only distinct words\n",
    "- and sorting the words alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apHh-4PQbKq7"
   },
   "outputs": [],
   "source": [
    "words = doc2_nopunctuation.lower().split()\n",
    "sorted_vocab = sorted(set(words))\n",
    "print(sorted_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY6icNSJbKq7"
   },
   "source": [
    "## 1.6 Counting term frequencies\n",
    "\n",
    "We often represent documents by their vocbulary, and in particular by their most frequently occuring terms, since those words are most likely to describe well the topic of the document.\n",
    "- We can count the frequency of the terms in the document using the Counter() function from the NLTK (Natural Language Tool Kit) library. \n",
    "- A online book describing the functionality that the NLTK library provides is available here: http://www.nltk.org/book/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJQwv6TpbKq8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "counts = nltk.Counter(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7_W-wMMbKq8"
   },
   "source": [
    "Note that the words are ordered according to their frequency. \n",
    "\n",
    "Lets display them again, but this time only the top 20, using the most_common() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K5O9Jp7bKq8"
   },
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcxV1cjHbKq9"
   },
   "source": [
    "## 1.7 Filtering Stopwords\n",
    "\n",
    "The most frequent terms: 'the', 'she', 'to', 'and', 'it', 'was', 'a', 'of', and 'i' aren't very interesting or descriptive of the story.\n",
    "- They are in fact frequent across *all documents* in the English language, and thus convey very little (if any) information about the topic of the document.\n",
    "- These terms are referred to as **'stop-words'**, because they can be removed from the description of the document without adversely affecting (indeed usually improving) the performance of a text search engine indexing the document.\n",
    "- The NLTK library contains lists of stop-word for English, Italian and many other languages. Let's print out the stop-word lists for English and Italian.\n",
    "\n",
    "Before we can get the stopword lists we need to download them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDz79vDebKq9"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnWF6C-KbKq9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print('English stopwords:')\n",
    "print(stopwords.words('english'))\n",
    "print()\n",
    "print('Italian stopwords:')\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ0U8qCnbKq9"
   },
   "source": [
    "Now let's remove the stop-words from the tokenised text before counting the frequency of the words in the document. \n",
    "- We can easily remove items from a list using some special syntax in Python: **[x for x in list1 if x not in list2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIHOmIwebKq9"
   },
   "outputs": [],
   "source": [
    "words_nostopwords = [w for w in words if w not in stopwords.words('english')]\n",
    "counts_nostopwords = nltk.Counter(words_nostopwords)\n",
    "counts_nostopwords.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R24uBJ9VbKq-"
   },
   "source": [
    "These words look a little bit better ... \n",
    "- The words 'Alice', 'time', 'eat', 'door' and 'rabbit' might be useful for describing the document\n",
    "- but many of the other words, like 'little', 'like', 'could' and 'get', migh not be as useful.\n",
    "\n",
    "\n",
    "To get an even better list of words for describing the document we would need to make use of information about *how common each word is in general in the English language*, since the more common a particular word is, the less likely it is to be useful for describing the document. \n",
    "- More on that later in the course ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu-ex9CDbKq-"
   },
   "source": [
    "## 1.8 Downloading content from the Web\n",
    "\n",
    "One common source of text documents is the Web. Let's now download an article from Wikipedia, and then extract the text from it.\n",
    "\n",
    "First download the HTML page using the urllib library and print out just the first 2000 bytes of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0BJs4iUbKq-"
   },
   "outputs": [],
   "source": [
    "import urllib.request  \n",
    "html_doc = urllib.request.urlopen('https://en.wikipedia.org/wiki/Bank_of_Italy').read()\n",
    "html_doc[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEroaWgRbKq-"
   },
   "source": [
    "Wow, that looks pretty ugly! \n",
    "\n",
    "Let's use another library (called Beautiful Soup) to parse the content of the page. \n",
    "- When printing out the parsed document we will use the prettify() method to indent all the HTML tags so that we can see the structure of the HTML document. (This is called 'pretty printing' in HTML/XML.)\n",
    "- Note that the printed output is very long, so after looking at it, you may want to edit the code to comment out the print line and re-run the cell. \n",
    " - To comment out the last line, simply place a hash character '#' in front of it: #print(parsed_doc.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oHjPDNxbKq-"
   },
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "parsed_doc = bs.BeautifulSoup(html_doc,'lxml')\n",
    "print(parsed_doc.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DniaRHMebKq_"
   },
   "source": [
    "## 1.9 Extracting text from the HTML\n",
    "\n",
    "Now let's extract the text from the HTML page. \n",
    "- First find all paragraph \\<p\\> ... \\</p\\> elements within the HTML page.\n",
    "- The find_all() method returns a list of the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qetw2l_8bKq_"
   },
   "outputs": [],
   "source": [
    "paragraph_elements = parsed_doc.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GStEGbJrbKq_"
   },
   "source": [
    "Now print out the first of the paragraph elements to see what it looks like:\n",
    "- Note that Python starts counting from zero, not one, so the first element is: paragraph_elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chpj_uCTbKq_"
   },
   "outputs": [],
   "source": [
    "print(paragraph_elements[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j88DzCBDbKq_"
   },
   "source": [
    "Well that was pretty uninteresting. The first paragraph was empty!\n",
    "- Print out the second paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG1QIZ2LbKq_"
   },
   "outputs": [],
   "source": [
    "print(paragraph_elements[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaHrFocybKq_"
   },
   "source": [
    "OK, now let's get the text of each paragraph, without all of the HTML markup:\n",
    "- To do that we'll use the same python construct we saw before for iterating over the elements of a list.\n",
    "- This time though, we'll perform an operation on each element (extract the text) before returning the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ4e7kyVbKrA"
   },
   "outputs": [],
   "source": [
    "paragraph_texts = [p.text for p in paragraph_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcgx_IwAbKrA"
   },
   "source": [
    "Print out the second paragraph to see how it looks without all of the HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cepBdE6ObKrA"
   },
   "outputs": [],
   "source": [
    "print(paragraph_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSkaGgtxbKrA"
   },
   "source": [
    "Print out the whole list to see text from the entire document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O38oIsntbKrA"
   },
   "outputs": [],
   "source": [
    "print(paragraph_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLVjuK81bKrA"
   },
   "source": [
    "So there we have it, a list of paragraphs that have been extracted from a webpage.\n",
    "\n",
    "What shall we do with this text? \n",
    "- First let's join all the paragraphs together in a single string, separating them with a newline '\\\\n' character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LqUUP36bKrA"
   },
   "outputs": [],
   "source": [
    "complete_text = '\\n'.join(paragraph_texts)\n",
    "print(complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYoolZfobKrB"
   },
   "source": [
    "## 1.10 Searching within extracted text\n",
    "\n",
    "Now that we have the text in a convenient format, we can start doing some analysis on the it. \n",
    "- We could search for somebody's name, e.g. the director 'Ignazio Visco', by using the 'search' command from the regular expression package 're' imported above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-kI5ERqbKrB"
   },
   "outputs": [],
   "source": [
    "re.search('Ignazio Visco', complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da5XgFHObKrB"
   },
   "source": [
    "This tells us that the director is first mentioned in between characters 256 and 269\n",
    "\n",
    "Let's find out how many times the director has been mentioned in the article. To do that we need to use the findall() command rather than search() command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlE3ND0bbKrB"
   },
   "outputs": [],
   "source": [
    "name = 'Ignazio Visco'\n",
    "matches = re.findall(name, complete_text)\n",
    "print(matches)\n",
    "print(\"The name '\"+name+\"' occurs \"+str(len(matches))+\" times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM8y7KHxbKrB"
   },
   "source": [
    "More than just knowing that the director is being mentioned, we'd like to know what is being said about him. So we'd like to extract the sentences mentioning him. \n",
    "- We can do that by changing the regular expression that we are using to be more than just a string of keywords.\n",
    "\n",
    "The required regular expression is a little complicated, so let's build up to it slowly. \n",
    "- First let's write a simple expression to capture the first 10 characters immediately after his name. \n",
    "- In regular expressions, the dot character '.' is a wild-card that matches any character\n",
    "- so to match the next 10 characters, we can simply add ten dots to his name: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06Y1SEXQbKrB"
   },
   "outputs": [],
   "source": [
    "regex = name + '..........'\n",
    "print(\"Regular expression: '\"+regex+\"'\")\n",
    "print(\"Returns:\")\n",
    "re.findall(regex, complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCZs2Aa0bKrB"
   },
   "source": [
    "That text window is far too short to be useful, and the regular expresssion is also particularly ugly. \n",
    "- Let's simplify regular expression by using the notation: '{n}' to repeat the previous character n times\n",
    "- and extend the window out to 100 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frUSbdxabKrB"
   },
   "outputs": [],
   "source": [
    "regex = name + '.{100}'\n",
    "print(\"Regular expression: '\"+regex+\"'\")\n",
    "print(\"Returns:\")\n",
    "re.findall(regex, complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ESWmmtAbKrB"
   },
   "source": [
    "Well the regular expression worked, but we lost one of the results because the required character window was too big. \n",
    "- A newline character was encountered less than 100 characters after the director's name.\n",
    "- To fix this, let's change the number of repetitions to be minimum zero, maximum 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrgXaV_ZbKrC"
   },
   "outputs": [],
   "source": [
    "regex = name + '.{,100}'\n",
    "re.findall(regex, complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rs0jgcPbKrC"
   },
   "source": [
    "OK, so that was fun, but what we'd really like to do is get the whole sentence around his name.\n",
    "- To do that we'll have to find all of the characters both before and after his name that do not include the period '.' character. \n",
    "- To choose any character except '.' we can write '[^.]' and to repeat that pattern zero or more times, we simply append '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NOpnAEebKrC"
   },
   "outputs": [],
   "source": [
    "regex = '[^.]*' + name + '[^.]*'\n",
    "re.findall(regex, complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beROSLPcbKrC"
   },
   "source": [
    "Finally, let's clean up the output a little: \n",
    "- by stripping off spaces and newline characters at the start of each sentence using the strip() method\n",
    "- and reappending the missing period at the end with: +'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHg6gew9bKrC"
   },
   "outputs": [],
   "source": [
    "name = 'Ignazio Visco'\n",
    "regex = '[^.]*' + name + '[^.]*'\n",
    "matches = re.findall(regex, complete_text)\n",
    "[m.strip() + '.' for m in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzOu1jlTbKrC"
   },
   "source": [
    "## 1.11 Loading text from a PDF\n",
    "\n",
    "Much of the text on the internet is present inside PDF documents, and often we'd like to extract text from them. \n",
    "\n",
    "There are many different ways to do that in Python. Today we'll use the pdfplumber API: https://github.com/jsvine/pdfplumber\n",
    "- In order to use pdfplumber module, we first need to install it. \n",
    "- We can do that from inside the jupyter notebook by calling the pip3 command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ_Q1rEPbKrC"
   },
   "outputs": [],
   "source": [
    "!pip3 install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sic3jHSObKrC"
   },
   "source": [
    "Now we can import the module and use it to extract content from a PDF. \n",
    "- Let's try extracting content from this Annual Report from Hershey's: https://www.thehersheycompany.com/content/dam/corporate-us/documents/investors/2020-proxy-statement-2019-annual-report.pdf\n",
    "- The HTTPS server won't allow us direct programmatic access, so you'll need to download the file and upload it to Google colab (as we did with the original Alice text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN-4B_UIbKrD"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "filename = '2020-proxy-statement-2019-annual-report.pdf'\n",
    "pdf = pdfplumber.open(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C49W1ARdbKrD"
   },
   "source": [
    "How many pages are in the report? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzALGQywbKrD"
   },
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZXe04dobKrD"
   },
   "source": [
    "Wow. That's a lot of pages!\n",
    "\n",
    "Have a look at the first couple of pages by extracting the text from them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM4yYdfjbKrD"
   },
   "outputs": [],
   "source": [
    "text = pdf.pages[0].extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXRuv5mibKrD"
   },
   "outputs": [],
   "source": [
    "text = pdf.pages[1].extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oNjvSm1bKrD"
   },
   "source": [
    "---\n",
    "Extract the text from all the pages of the document into a list\n",
    "- Note: this might take a minute. There are a lot of pages ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCuTTuvSbKrD"
   },
   "outputs": [],
   "source": [
    "texts = [page.extract_text() for page in pdf.pages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFdEPAa0bKrD"
   },
   "source": [
    "Now concatenate all the text together into a single string. \n",
    "- We'll separate them from one another using a couple of newline characters and some spaces too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA2Je3JpbKrE"
   },
   "outputs": [],
   "source": [
    "text = \"  \\n\\n\".join(texts)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8XeouXkbKrE"
   },
   "source": [
    "## 1.12 Using Regular Expressions to search PDF \n",
    "\n",
    "Use some regular expressions to search through the text for some interesting content. \n",
    "- You could look for email addresses, phone numbers, addresses, ...\n",
    "- Let's try first to look for email addresses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXMhf0aNbKrE"
   },
   "outputs": [],
   "source": [
    "regex = '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "emails = re.findall(regex,text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqzxkQ28bKrE"
   },
   "source": [
    "Did you find any? \n",
    "\n",
    "Mars and Nestle are competitors of Hersheys. \n",
    "- Are they mentioned anywhere in the report? \n",
    "- Write a regular expression to find out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JabBh00YbKrE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-eWNcQbbKrE"
   },
   "source": [
    "Other ideas to try:\n",
    "- The name of Hershey's CEO is 'Michele Buck'. Find out where she is mentioned in the report and in what context.\n",
    "- The report was written after the outbreak of COVID-19. Search for references to it in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPGXsTFFbKrE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBR_N4eQbKrE"
   },
   "source": [
    "Load another PDF and write regular expressions to search for content in it. \n",
    "- For example, you can find reports for Ferrari here: https://corporate.ferrari.com/en/investors/results/reports\n",
    "- Let's load an interim report from September 2020 (you can find it in the same Data folder as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzSw45EtbKrF"
   },
   "outputs": [],
   "source": [
    "filename = 'ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf'\n",
    "pdf = pdfplumber.open(filename)\n",
    "text = '\\n\\n'.join([page.extract_text() for page in pdf.pages])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd8hAX4aV0JK"
   },
   "source": [
    "## 1.13 Extracting Tables from a PDF \n",
    "\n",
    "Sometimes it can be useful to extract tabular data from a PDF. \n",
    "- Tools exist that allow you to do this programmatically, making the extraction process semi-automatic.\n",
    "- One tool that can do this is the *tabula* library. Let's install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_rwPuYmGCVc"
   },
   "outputs": [],
   "source": [
    "!pip install tabula-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naFDi4yNWyY9"
   },
   "source": [
    "Now we can use *tabula* to extract all the tables from Ferrari's interim report above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pQwzInjG8Rt"
   },
   "outputs": [],
   "source": [
    "import tabula \n",
    "tables = tabula.read_pdf(filename, pages = \"all\", multiple_tables = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNrQUIGrX6Rn"
   },
   "source": [
    "Let's have a look at some of the tables produced\n",
    "- the first table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86E4O5vDH8Hq"
   },
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXPN-2SDYCao"
   },
   "source": [
    "- the third table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fOAuloxYKvp"
   },
   "outputs": [],
   "source": [
    "tables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDCG8UeAYbwM"
   },
   "source": [
    "It can be seen that the tables are in need of a bit of cleaning to make them usable. \n",
    "- The tables are Pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWqHAlmoJSaS"
   },
   "outputs": [],
   "source": [
    "type(tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reWHSqBOYwcU"
   },
   "source": [
    "So we can clean-up the table by:\n",
    "- dropping some columns\n",
    "- dropping some rows\n",
    "- renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlpIdJI0Jt-n"
   },
   "outputs": [],
   "source": [
    "df = tables[0]\n",
    "df = df.drop(df.columns[[1,5]], axis=1)  # drop columns: 1,5\n",
    "df = df.drop([0,5,12])                   # drop rows: 0,5,12\n",
    "df = df.reset_index(drop=True)           # reset index\n",
    "df.columns = ('Field','3months_to_30092020','3months_to_30092019','9months_to_30092020','9months_to_30092019') # rename columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUsE_YcMbjZ_"
   },
   "source": [
    "During the cleaning phase, some of the values may need to be updated too (e.g. certain values in the Field column above).\n",
    "- Ideally the above cleaning operations would be done automatically.\n",
    "- In practice, tables have lots of nested structure (including the one we just extracted), \n",
    "- and it's still a hard research problem to do the cleaning reliably, (particularly the generation of the column names that we provided manually).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYsMXRyJbKrF"
   },
   "source": [
    "## 1.13 Loading text from scanned document\n",
    "\n",
    "But what if my documents have been scanned? \n",
    "- In that case the task of extracting text from them is much more difficult.\n",
    "\n",
    "It is possible to extract text also from images, but you will need to have an Optical Character Recognition (OCR) system installed. \n",
    "- Tesseract is an opensource OCR system provided by Google. Some systems (such as Linux) come with Tesseract pre-installed. Others need to install it from here: https://tesseract-ocr.github.io/tessdoc/Home.html \n",
    "- If you have Tesseract installed, you can follow the instructions here to use it from Python: https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXQu2_fiQFE0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
