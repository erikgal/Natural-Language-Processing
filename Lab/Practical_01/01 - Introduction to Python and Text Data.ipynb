{"cells":[{"cell_type":"markdown","metadata":{"id":"R67NmC2GWaoR"},"source":["# Text manipulation and extraction in Python\n","\n","This notebook introduces the basic operation on strings that can be done with the Python programming language.\n","The notebook then focuses on text manipulation and extraction from different sources:\n","- Text files\n","- Web\n","- PDF documents\n","- OCR scanned PDF documents\n","\n","For an introduction or recap on Python, refer to the WeBeep page of this course"]},{"cell_type":"markdown","metadata":{"id":"H9fk15RybKqy"},"source":["## Strings and lists\n","\n","A 'string' is simply a sequence of characters used to represent a document in a programming language such as Python.\n","- Let's create a Python variable called 'doc' that contains a short document as a string.\n","- After defining the variable, we repeat its name so as to print out its content.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677085847271,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"XzuBscVnbKqz","outputId":"2a666d68-b87b-4293-bfe0-56e741a3a770"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["doc = 'In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell'\n","doc"]},{"cell_type":"markdown","metadata":{"id":"PILAgmKDbKqz"},"source":["We can calculate the length of the string (in characters) by using the len() function:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677085853100,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"B0KzLRCjbKq0","outputId":"cbf9e8e6-e3f3-4949-cac3-375b4068b556"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["123"]},"metadata":{},"execution_count":2}],"source":["len(doc)"]},{"cell_type":"markdown","metadata":{"id":"NlH3zCkHbKq0"},"source":["We can divide up the sentence into individual words by splitting it on whitespace (spaces, tabs, etc.). \n","- This process is called 'tokenisation':"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1677085860282,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"826CWmTnbKq0","outputId":"8b5e3d12-8cd6-4cec-84dd-b74fd99b099b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['In',\n"," 'a',\n"," 'hole',\n"," 'in',\n"," 'the',\n"," 'ground',\n"," 'there',\n"," 'lived',\n"," 'a',\n"," 'hobbit.',\n"," 'Not',\n"," 'a',\n"," 'nasty,',\n"," 'dirty,',\n"," 'wet',\n"," 'hole,',\n"," 'filled',\n"," 'with',\n"," 'the',\n"," 'ends',\n"," 'of',\n"," 'worms',\n"," 'and',\n"," 'an',\n"," 'oozy',\n"," 'smell']"]},"metadata":{},"execution_count":3}],"source":["doc.split()"]},{"cell_type":"markdown","metadata":{"id":"_Fky0cvtbKq0"},"source":["Note that the ouptut above is in the form of comma-separated list of strings [s1,s2,...,sn]\n","- The layout above is vertical, but if you use print() command you can get a more compact horizontal ouptut."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1677085876316,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"UwTtRQIwbKq1","outputId":"36d720ad-186e-4383-9208-044aeeac87ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit.', 'Not', 'a', 'nasty,', 'dirty,', 'wet', 'hole,', 'filled', 'with', 'the', 'ends', 'of', 'worms', 'and', 'an', 'oozy', 'smell']\n"]}],"source":["print(doc.split())"]},{"cell_type":"markdown","metadata":{"id":"X7VvLRsobKq1"},"source":["We didn't have to split the sentence on whitespace, we could have split it around any substring. \n","- For example we could split on the comma ',' character:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1677087572977,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"ahRXQem1bKq1","outputId":"dc01e4a0-f4e2-4a60-d0fd-d9823988f633"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['In a hole in the ground there lived a hobbit. Not a nasty',\n"," ' dirty',\n"," ' wet hole',\n"," ' filled with the ends of worms and an oozy smell']"]},"metadata":{},"execution_count":5}],"source":["doc.split(',')"]},{"cell_type":"markdown","metadata":{"id":"04A8J71QbKq1"},"source":["How many words are there in the document?"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":640,"status":"ok","timestamp":1677087582684,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"03XDB4C1bKq2","outputId":"8f523eae-f3b1-4df6-a19e-c1c93499658d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{},"execution_count":6}],"source":["words = doc.split()\n","len(words)"]},{"cell_type":"markdown","metadata":{"id":"3Bwb2-TTbKq2"},"source":["Often in text-processing pipelines we convert all text to lower-case. \n","- Since the sentence is almost all in lower-case already, let's convert it to upper-case instead:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677087587578,"user":{"displayName":"Erik Galler","userId":"10641819115597978698"},"user_tz":-60},"id":"-utEyCjZbKq2","outputId":"509f3146-3687-4aa3-c78c-29b33c3a947a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'IN A HOLE IN THE GROUND THERE LIVED A HOBBIT. NOT A NASTY, DIRTY, WET HOLE, FILLED WITH THE ENDS OF WORMS AND AN OOZY SMELL'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["doc.upper()"]},{"cell_type":"markdown","metadata":{"id":"qgR1MpJWbKq2"},"source":["## Loading text from a file\n","Let's now read in a longer document form a text file 'Alice_Chapter1.txt'\n","\n","- Make sure you have downloaded the file \"Alice_Chapter1.txt\" from the \"docs\" directory in the WeBeep directory where you found this notebook (I'd suggest you downlod the entire directory each time to be sure every file is in the right place).\n","- If you are using Google Colab, you will then need to upload the file by clicking on the Folder icon to the left of the notebook, then clicking on the Upload icon, and finding the file on your drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw9JB10EbKq2"},"outputs":[],"source":["with open(\"docs/Alice_Chapter1.txt\") as f:\n","    doc2 = f.read()"]},{"cell_type":"markdown","metadata":{"id":"4gPJhHrEbKq3"},"source":["Print out the text as Python sees it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27CsU1gbbKq3"},"outputs":[],"source":["doc2"]},{"cell_type":"markdown","metadata":{"id":"JEDHeg0EbKq3"},"source":["Note all the backslash characters '\\\\' in the text above.  \n","- Python stores text as one big string (sequence of characters). \n","- Special characters such as newlines and tabs are represented by '\\\\n' and '\\\\t' respectively.\n","- The quote character is used to mark the start and end of the string ('string'), so quote characters that are present in the string are prefixed by a backslash to prevent the string from ending early ('str\\\\'ing'). \n","- Using the print() command, we can output the string in a format that we're more used to seeing it in:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m9q8V4QbKq3"},"outputs":[],"source":["print(doc2)"]},{"cell_type":"markdown","metadata":{"id":"TiCwKJ_VbKq3"},"source":["### Splitting lines and finding words\n","\n","We can split the text into separate lines using splitlines() method. \n","- Since there are lot of lines, we'll only print the first 5 of them by appending `[:5]` to the name of the variable contianing them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLyhzsTQbKq3"},"outputs":[],"source":["lines = doc2.splitlines()\n","lines[:5]"]},{"cell_type":"markdown","metadata":{"id":"a8AFF-WibKq4"},"source":["Note that: \n","- Some of the lines contain no text at all.\n","- Some of the lines are surrounded by the double quote character \" becuase they contain the single quote character in the text. \n","\n","How many lines are there in total in the text?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfWU5HQhbKq4"},"outputs":[],"source":["len(doc2.splitlines())"]},{"cell_type":"markdown","metadata":{"id":"sD9FbeMnbKq4"},"source":["We can search for a particular word in the text: \n","- For example, let's search for the word 'Rabbit'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRAD91wXbKq4"},"outputs":[],"source":["doc2.find('Rabbit')"]},{"cell_type":"markdown","metadata":{"id":"WZCPevxbbKq4"},"source":["The number tells us that the word appears at the 552nd character position. \n","\n","We can format the output to state this explicitly:\n","- We use the '+' command to concatenate strings, \n","- and the str() command for converting an integer to a string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yT6aWipbKq4"},"outputs":[],"source":["word = \"Rabbit\"\n","mystring = f\"The word '{word}' appeared at character position {str(doc2.find(word))}in the text\"\n","print(mystring)"]},{"cell_type":"markdown","metadata":{"id":"j34S1khxpIH3"},"source":["What happens if we search for a string that does't exist in the document? \n","- Try it... "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCrROJ69pCa_"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"7_1gK8GIbKq5"},"source":["### Investigating the vocabulary of a document\n","\n","Now let's find the vocabulary of this text by: \n","- first converting the text to lowercase\n","- then splitting the words on whitespace\n","- then selecting only distinct words by using the set() function\n","\n","Python sets are just regular sets from math where you can put heterogenous variables, only a single copy of each element is allowed in a set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zkUbepNbKq5"},"outputs":[],"source":["lowercase_doc = doc2.lower()\n","words = lowercase_doc.split()\n","vocab = set(words)\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"FLXC6_vLbKq5"},"source":["To make it easier to read, we could sort the vocabulary alphabetically:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc_XT1nBbKq5"},"outputs":[],"source":["sorted_vocab = sorted(vocab)\n","print(sorted_vocab)"]},{"cell_type":"markdown","metadata":{"id":"M4ZzLRipp5KE"},"source":["That looks a bit weird. What are all those bracket '(' characters doing there? "]},{"cell_type":"markdown","metadata":{"id":"DIB-6xDabKq5"},"source":["### Removing punctuation with a regular expression\n","\n","Notice that many of the vocabulary terms, particularly those at the start of the list, contain punctuation characters like quotes '\"', brackets '(' and exclamation marks '!'. We'll now see how to remove these puntuation characters:\n","- First get a list of punctuation characters from the 'string' library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCFQzseTbKq5"},"outputs":[],"source":["import string\n","string.punctuation"]},{"cell_type":"markdown","metadata":{"id":"wct3c1EAbKq6"},"source":["The list is provided as a single string. To convert it to a list of individual characters, just call the list function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW-2BMVjbKq6"},"outputs":[],"source":["list(string.punctuation)"]},{"cell_type":"markdown","metadata":{"id":"kxnuEuSjbKq6"},"source":["Notice the double backslash character '\\\\\\\\' in the list. This is needed because backslash is used as the escape character. So if we don't put a double backslash, Python will interpret the single backslash as escaping the quote character that follows it.\n","\n","We can create a regular expression that will match any of those puncutation characters by simply surrounding the string of punctuation characters with square brackets: \"[]\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSb8VgZrbKq6"},"outputs":[],"source":["regex = '[' + string.punctuation + ']'\n","print(regex)"]},{"cell_type":"markdown","metadata":{"id":"yJK_3gYDbKq6"},"source":["We can use the new punctuation matching regular expression with the sub() command in the *re* (regular expression) libarary to remove the unwanted punctuation.\n","- Note that the sub() routine actually performs a substitution each time it finds a match, but we will simply replace the punctuation character with an empty string: ''\n","- Let's print out the first 1000 characters of the text after removing all punctuation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRYPdI3sbKq7"},"outputs":[],"source":["import re\n","doc2_nopunctuation = re.sub(regex,'',doc2)\n","print(doc2_nopunctuation[:1000])"]},{"cell_type":"markdown","metadata":{"id":"VDrmqJTbbKq7"},"source":["Compare this output with the original text for the first 1000 characters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-7aO0mxbKq7"},"outputs":[],"source":["print(doc2[:1000])"]},{"cell_type":"markdown","metadata":{"id":"jBjg-w_pbKq7"},"source":["Now that we've removed the punctuation, let's generate the sorted vocabulary again, by:\n","- converting to lowercase\n","- splitting on whitespace\n","- select only distinct words\n","- and sorting the words alphabetically"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apHh-4PQbKq7"},"outputs":[],"source":["words = doc2_nopunctuation.lower().split()\n","sorted_vocab = sorted(set(words))\n","print(sorted_vocab)"]},{"cell_type":"markdown","metadata":{"id":"aY6icNSJbKq7"},"source":["### Counting term frequencies\n","\n","We often represent documents by their vocbulary, and in particular by their most frequently occuring terms, since those words are most likely to describe well the topic of the document.\n","- We can count the frequency of the terms in the document using the Counter() function from the NLTK (Natural Language Tool Kit) library. \n","- A online book describing the functionality that the NLTK library provides is available here: http://www.nltk.org/book/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJQwv6TpbKq8"},"outputs":[],"source":["import nltk\n","counts = nltk.Counter(words)\n","print(counts)"]},{"cell_type":"markdown","metadata":{"id":"W7_W-wMMbKq8"},"source":["Note that the words are ordered according to their frequency. \n","\n","Lets display them again, but this time only the top 20, using the most_common() method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_K5O9Jp7bKq8"},"outputs":[],"source":["counts.most_common(20)"]},{"cell_type":"markdown","metadata":{"id":"CcxV1cjHbKq9"},"source":["### Filtering Stopwords\n","\n","The most frequent terms: 'the', 'she', 'to', 'and', 'it', 'was', 'a', 'of', and 'i' aren't very interesting or descriptive of the story.\n","- They are in fact frequent across *all documents* in the English language, and thus convey very little (if any) information about the topic of the document.\n","- These terms are referred to as **'stop-words'**, because they can be removed from the description of the document without adversely affecting (indeed usually improving) the performance of a text search engine indexing the document.\n","- The NLTK library contains lists of stop-word for English, Italian and many other languages. Let's print out the stop-word lists for English and Italian.\n","\n","Before we can get the stopword lists we need to download them:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDz79vDebKq9"},"outputs":[],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnWF6C-KbKq9"},"outputs":[],"source":["from nltk.corpus import stopwords\n","print('English stopwords:')\n","print(stopwords.words('english'))\n","print()\n","print('Italian stopwords:')\n","print(stopwords.words('italian'))"]},{"cell_type":"markdown","metadata":{"id":"IQ0U8qCnbKq9"},"source":["Now let's remove the stop-words from the tokenised text before counting the frequency of the words in the document. \n","- We can easily remove items from a list using some special syntax in Python: **[x for x in list1 if x not in list2]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIHOmIwebKq9"},"outputs":[],"source":["words_nostopwords = [w for w in words if w not in stopwords.words('english')]\n","counts_nostopwords = nltk.Counter(words_nostopwords)\n","counts_nostopwords.most_common(20)"]},{"cell_type":"markdown","metadata":{"id":"R24uBJ9VbKq-"},"source":["These words look a little bit better ... \n","- The words 'Alice', 'time', 'eat', 'door' and 'rabbit' might be useful for describing the document\n","- but many of the other words, like 'little', 'like', 'could' and 'get', migh not be as useful.\n","\n","\n","To get an even better list of words for describing the document we would need to make use of information about *how common each word is in general in the English language*, since the more common a particular word is, the less likely it is to be useful for describing the document. \n","- More on that later in the course ..."]},{"cell_type":"markdown","metadata":{"id":"Vu-ex9CDbKq-"},"source":["## Downloading content from the Web\n","\n","One common source of text documents is the Web. Let's now download an article from Wikipedia, and then extract the text from it.\n","\n","First download the HTML page using the urllib library and print out just the first 2000 bytes of it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0BJs4iUbKq-"},"outputs":[],"source":["import urllib.request  \n","html_doc = urllib.request.urlopen('https://en.wikipedia.org/wiki/Dune_(novel)').read()\n","html_doc[:2000]"]},{"cell_type":"markdown","metadata":{"id":"DEroaWgRbKq-"},"source":["Wow, that looks pretty ugly! \n","\n","Let's use another library (called Beautiful Soup) to parse the content of the page. \n","- When printing out the parsed document we will use the prettify() method to indent all the HTML tags so that we can see the structure of the HTML document. (This is called 'pretty printing' in HTML/XML.)\n","- Note that the printed output is very long, so after looking at it, you may want to edit the code to comment out the print line and re-run the cell. \n"," - To comment out the last line, simply place a hash character '#' in front of it: #print(parsed_doc.prettify())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oHjPDNxbKq-","tags":[]},"outputs":[],"source":["import bs4 as bs  \n","parsed_doc = bs.BeautifulSoup(html_doc,'lxml')\n","print(parsed_doc.prettify())"]},{"cell_type":"markdown","metadata":{"id":"DniaRHMebKq_"},"source":["### Extracting text from the HTML\n","\n","Now let's extract the text from the HTML page. \n","- First find all paragraph \\<p\\> ... \\</p\\> elements within the HTML page.\n","- The find_all() method returns a list of the elements:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qetw2l_8bKq_"},"outputs":[],"source":["paragraph_elements = parsed_doc.find_all('p')"]},{"cell_type":"markdown","metadata":{"id":"GStEGbJrbKq_"},"source":["Now print out the first of the paragraph elements to see what it looks like:\n","- Note that Python starts counting from zero, not one, so the first element is: paragraph_elements[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chpj_uCTbKq_"},"outputs":[],"source":["print(paragraph_elements[0])"]},{"cell_type":"markdown","metadata":{"id":"j88DzCBDbKq_","tags":[]},"source":["Well that was pretty uninteresting. The first paragraph was empty!\n","- Print out the second paragraph:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uG1QIZ2LbKq_","tags":[]},"outputs":[],"source":["print(paragraph_elements[1])"]},{"cell_type":"markdown","metadata":{"id":"MaHrFocybKq_"},"source":["OK, now let's get the text of each paragraph, without all of the HTML markup:\n","- To do that we'll use the same python construct we saw before for iterating over the elements of a list.\n","- This time though, we'll perform an operation on each element (extract the text) before returning the list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZ4e7kyVbKrA"},"outputs":[],"source":["paragraph_texts = [p.text for p in paragraph_elements]"]},{"cell_type":"markdown","metadata":{"id":"Vcgx_IwAbKrA"},"source":["Print out the second paragraph to see how it looks without all of the HTML tags:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cepBdE6ObKrA"},"outputs":[],"source":["print(paragraph_texts[1])"]},{"cell_type":"markdown","metadata":{"id":"vSkaGgtxbKrA"},"source":["Print out the whole list to see text from the entire document:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O38oIsntbKrA","tags":[]},"outputs":[],"source":["print(paragraph_texts)"]},{"cell_type":"markdown","metadata":{"id":"fLVjuK81bKrA"},"source":["So there we have it, a list of paragraphs that have been extracted from a webpage.\n","\n","What shall we do with this text? \n","- First let's join all the paragraphs together in a single string, separating them with a newline `\\n` character:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LqUUP36bKrA","tags":[]},"outputs":[],"source":["complete_text = '\\n'.join(paragraph_texts)\n","print(complete_text)"]},{"cell_type":"markdown","metadata":{"id":"zYoolZfobKrB"},"source":["### Searching within extracted text\n","\n","Now that we have the text in a convenient format, we can start doing some analysis on the it. \n","- We could search for somebody's name, e.g. the author 'Frank Herbert', by using the `search` command from the regular expression package 're' imported above.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-kI5ERqbKrB"},"outputs":[],"source":["re.search('Frank Herbert', complete_text)"]},{"cell_type":"markdown","metadata":{"id":"Da5XgFHObKrB"},"source":["This tells us that the author is first mentioned in between characters 256 and 269\n","\n","Let's find out how many times the director has been mentioned in the article. To do that we need to use the `findall()` command rather than search() command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlE3ND0bbKrB"},"outputs":[],"source":["name = 'Frank Herbert'\n","matches = re.findall(name, complete_text)\n","print(matches)\n","print(f\"The name '{name}' occurs {len(matches)} times\")"]},{"cell_type":"markdown","metadata":{"id":"eM8y7KHxbKrB"},"source":["More than just knowing that the author is being mentioned, we'd like to know what is being said about him. So we'd like to extract the sentences mentioning him. \n","- We can do that by changing the regular expression that we are using to be more than just a string of keywords.\n","\n","The required regular expression is a little complicated, so let's build up to it slowly. \n","- First let's write a simple expression to capture the first 10 characters immediately after his name. \n","- In regular expressions, the dot character '.' is a wild-card that matches any character\n","- so to match the next 10 characters, we can simply add ten dots to his name: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06Y1SEXQbKrB"},"outputs":[],"source":["regex = name + '..........'\n","print(f\"Regular expression: '{regex}'\")\n","print(\"Returns:\")\n","re.findall(regex, complete_text)"]},{"cell_type":"markdown","metadata":{"id":"PCZs2Aa0bKrB"},"source":["That text window is far too short to be useful, and the regular expresssion is also particularly ugly. \n","- Let's simplify regular expression by using the notation: `{n}` to repeat the previous character n times\n","- and extend the window out to 100 characters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frUSbdxabKrB"},"outputs":[],"source":["regex = name + '.{100}'\n","print(f\"Regular expression: '{regex}'\")\n","print(\"Returns:\")\n","re.findall(regex, complete_text)"]},{"cell_type":"markdown","metadata":{"id":"2ESWmmtAbKrB"},"source":["Well the regular expression worked, but we lost one of the results because the required character window was too big. \n","- A newline character was encountered less than 100 characters after the director's name.\n","- To fix this, let's change the number of repetitions to be minimum zero, maximum 100:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrgXaV_ZbKrC"},"outputs":[],"source":["regex = name + '.{,100}'\n","re.findall(regex, complete_text)"]},{"cell_type":"markdown","metadata":{"id":"0Rs0jgcPbKrC"},"source":["OK, so that was fun, but what we'd really like to do is get the whole sentence around his name.\n","- To do that we'll have to find all of the characters both before and after his name that do not include the period '.' character. \n","- To choose any character except '.' we can write `[^.]` and to repeat that pattern zero or more times, we simply append '*'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NOpnAEebKrC"},"outputs":[],"source":["regex = '[^.]*' + name + '[^.]*'\n","re.findall(regex, complete_text)"]},{"cell_type":"markdown","metadata":{"id":"beROSLPcbKrC"},"source":["Finally, let's clean up the output a little: \n","- by stripping off spaces and newline characters at the start of each sentence using the strip() method\n","- and reappending the missing period at the end with `+ '.'`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHg6gew9bKrC"},"outputs":[],"source":["name = 'Frank Herbert'\n","regex = '[^.]*' + name + '[^.]*'\n","matches = re.findall(regex, complete_text)\n","[m.strip() + '.' for m in matches]"]},{"cell_type":"markdown","metadata":{"id":"8zM2VdMxWapN"},"source":["## Combine data from multiple files\n","\n","In some cases data sets contain many different information, as a result the content is split into different files:\n","- We can open the required files through Python\n","- We can load the required information using dictionaries for fast search over the data set\n","- We can merge the data sets into strings to obtan the final data set"]},{"cell_type":"markdown","metadata":{"id":"3Hbz_3amWapN"},"source":["### Loading files\n","\n","We are going to work with the [Cornell Movie--Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).\n","You can download a copy of the original version of the corpus from this [link](http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip).\n","\n","Extract the content of the zip archive and put it into a `docs/` directory."]},{"cell_type":"markdown","metadata":{"id":"4SHdFvy_WapN"},"source":["There are two main files in the corpus:\n","- `movie_lines.txt` is a text file where each row is an utterance in a dialogue, it contains all the lines avaialble in the corpus\n","- `movie_conversations.txt` is a text file where each row contains the list with the identifiers of the lines composing a dialogue."]},{"cell_type":"markdown","metadata":{"id":"mVo6PAnMWapN"},"source":["First we load the utterances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXDUxIoeWapO"},"outputs":[],"source":["with open('docs/cornell movie-dialogs corpus/movie_lines.txt') as f:\n","    lines = f.read()"]},{"cell_type":"markdown","metadata":{"id":"rPe2bTm6WapO"},"source":["Let's see what data we have in each row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Dr0KrGNWapO"},"outputs":[],"source":["print(lines[:1000])"]},{"cell_type":"markdown","metadata":{"id":"aZgWvwiaWapO"},"source":["There are three elements we want to keep:\n","- line identifier\n","- speaker\n","- utterance text"]},{"cell_type":"markdown","metadata":{"id":"cCI__BaXWapO"},"source":["Then we load the dialogues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20-CDIeCWapP"},"outputs":[],"source":["with open('docs/cornell movie-dialogs corpus/movie_conversations.txt') as f:\n","    lines_list = f.read()"]},{"cell_type":"markdown","metadata":{"id":"8CDFb7OgWapP"},"source":["Let's see what data we have in each row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k10-S-NDWapP"},"outputs":[],"source":["print(lines_list[:1000])"]},{"cell_type":"markdown","metadata":{"id":"CfxzXx-0WapP"},"source":["Here we are interested in keeping only the list of identifiers composing a dialogue."]},{"cell_type":"markdown","metadata":{"id":"9ryyyaJkWapP"},"source":["### Parsing content with RegEx\n","\n","Now we can use RegEx to extract the useful information we want.\n","\n","With RegEx we can define the structure of an entire string or piece of string and we can group pieces of our expressions.\n","In this way we can retreive specific pieces of a string that matches our request."]},{"cell_type":"markdown","metadata":{"id":"naZHebQGWapQ"},"source":["Each row in the lines file follows the same pattern:\n","1. Line identifier\n","2. Speaker identifier\n","3. Movie identifier\n","4. Speaker\n","5. Utterance text\n","\n","We are interested in 1, 2, and 5.\n","\n","Note that we have a peculiar separator between the elements `+++$+++`"]},{"cell_type":"markdown","metadata":{"id":"tfSaMUJ-WapQ"},"source":["Let's write a RegEx first and apply it to the first rows.\n","\n","We use round brakets `()` to isolate groups (groups can be nested)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4v_ZK56WapQ"},"outputs":[],"source":["regex = '(L\\d+) \\+\\+\\+\\$\\+\\+\\+ u\\d+ \\+\\+\\+\\$\\+\\+\\+ m\\d+ \\+\\+\\+\\$\\+\\+\\+ ([\\w\\s]+) \\+\\+\\+\\$\\+\\+\\+ (.+)'\n","print(f\"Regular expression: '{regex}'\")\n","print(\"Returns:\")\n","re.findall(regex, lines[:1000])"]},{"cell_type":"markdown","metadata":{"id":"jkUpqREqWapQ"},"source":["What do you see in output?"]},{"cell_type":"markdown","metadata":{"id":"1wKLdQn8WapR"},"source":["Now we can retrieve the desired information from each line and use it to build a list dictionaries where the keys are the IDs of the lines.\n","\n","Each element of the dictionary will contain the speaker and the uttered text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tHqch9nWapR"},"outputs":[],"source":["lines = {key: {'speaker': sp, 'text': txt} for key, sp, txt in re.findall(regex, lines)}"]},{"cell_type":"markdown","metadata":{"id":"lUpPJndGWapR"},"source":["Now we can access the elements by specific names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g-jEg5YWapR"},"outputs":[],"source":["print(type(lines))\n","print(lines['L868'])\n","print(type(lines['L868']))\n","print(lines['L867']['speaker'])\n","print(lines['L867']['text'])"]},{"cell_type":"markdown","metadata":{"id":"bB_OozjfWapR"},"source":["Now we can move to the dialogues file\n","\n","Each row in the dialogues file follows the same pattern:\n","1. First speaker identifier\n","2. Second speaker identifier\n","3. Movie identifier\n","4. List of lines identifier (expressed as a list of strings)\n","\n","We are interested only in 4.\n","\n","Note that we have again the peculiar separator between the elements `+++$+++`"]},{"cell_type":"markdown","metadata":{"id":"U5ygdmjoWapS"},"source":["We split the search into two parts, first isolate the lists and then retrieve elements from the lists. Let's write a RegEx first and apply it to the first rows.\n","\n","**NOTE: this is not the smartest way to appraoch it, but it is useful to understand how regex work**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NwvbOV5WapS"},"outputs":[],"source":["list_regex = 'u\\d+ \\+\\+\\+\\$\\+\\+\\+ u\\d+ \\+\\+\\+\\$\\+\\+\\+ m\\d+ \\+\\+\\+\\$\\+\\+\\+ \\[(.+)\\]'\n","print(f\"Regular expression: '{list_regex}'\")\n","print(\"Returns:\")\n","re.findall(list_regex, lines_list[:1000])"]},{"cell_type":"markdown","metadata":{"id":"t1ITZzv-WapS"},"source":["Each element here is a string with the code of the line. We can search in each string separately the line IDs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tqCn-3hWapS"},"outputs":[],"source":["elem_regex = 'L\\d+'\n","s = re.findall(list_regex, lines_list[:1000])[0]\n","print(f\"Regular expression: '{elem_regex}'\")\n","print(f\"String: \\\"{s}\\\"\")\n","print(\"Returns:\")\n","re.findall(elem_regex, s)"]},{"cell_type":"markdown","metadata":{"id":"5jGDOQEPWapS"},"source":["Now we are dealing with an actual list of strings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5yVCWVjWapT"},"outputs":[],"source":["print(type(re.findall(elem_regex, s)))\n","print(type(re.findall(elem_regex, s)[0]))"]},{"cell_type":"markdown","metadata":{"id":"eN3HPpl4WapT"},"source":["Now we can retrieve the desired information from each line and use it to build a list, each element of the list is a list itself.\n","The inner list contains the IDs of the lines composing the dialogue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PV1s7CrWapT"},"outputs":[],"source":["lines_list = [re.findall(elem_regex, s) for s in re.findall(list_regex, lines_list)]\n","lines_list[:10]"]},{"cell_type":"markdown","metadata":{"id":"0v99m8vsWapT"},"source":["### Composing the dialogues\n","\n","Now we have an indexed list of lines and a list of IDs composing a dialogue, we can finally put all together"]},{"cell_type":"markdown","metadata":{"id":"jRdZRwK1WapU"},"source":["For each dialogue in `lines_list` we compose a string with all the turns separated by a newline character.\n","A turn is a speaker-text pair in the sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oyw0V3u9WapU"},"outputs":[],"source":["dialogues = [\n","    '\\n'.join(f'{lines[idx][\"speaker\"]}: {lines[idx][\"text\"]}' for idx in indices) \n","    for indices in lines_list if all(idx in lines for idx in indices)  # There are some missing \n","]\n","len(dialogues)"]},{"cell_type":"markdown","metadata":{"id":"adW8wds-WapU"},"source":["We can give a look to one of the extracted dialogues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFiyNdDjWapU"},"outputs":[],"source":["print(dialogues[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aH9T6tWNWapU"},"outputs":[],"source":["print(dialogues[2307])"]},{"cell_type":"markdown","metadata":{"id":"kzOu1jlTbKrC","tags":[]},"source":["## Loading text from a PDF\n","\n","Much of the text on the internet is present inside PDF documents, and often we'd like to extract text from them. \n","\n","There are many different ways to do that in Python. Today we'll use the pdfplumber API: https://github.com/jsvine/pdfplumber\n","- In order to use pdfplumber module, we first need to install it. \n","- We can do that from inside the jupyter notebook by calling the pip3 command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQ_Q1rEPbKrC"},"outputs":[],"source":["!pip3 install pdfplumber"]},{"cell_type":"markdown","metadata":{"id":"Sic3jHSObKrC"},"source":["Now we can import the module and use it to extract content from a PDF. \n","- Let's try extracting content from this NLP reasearch paper: https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\n","- The HTTPS server won't allow us direct programmatic access, so you'll need to use the file in the `docs/` directory on WeBeep where you found this notebook (as we did with the original Alice text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN-4B_UIbKrD"},"outputs":[],"source":["import pdfplumber\n","filename = 'docs/collobert11a.pdf'\n","pdf = pdfplumber.open(filename)"]},{"cell_type":"markdown","metadata":{"id":"C49W1ARdbKrD"},"source":["How many pages are in the report? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzALGQywbKrD"},"outputs":[],"source":["len(pdf.pages)"]},{"cell_type":"markdown","metadata":{"id":"NZXe04dobKrD"},"source":["Wow, that's not a lot of pages!\n","\n","We can have a look at the first couple of pages extracting the text from them: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM4yYdfjbKrD"},"outputs":[],"source":["text = pdf.pages[0].extract_text(x_tolerance=1)\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXRuv5mibKrD"},"outputs":[],"source":["text = pdf.pages[1].extract_text(x_tolerance=1)\n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"5oNjvSm1bKrD"},"source":["Extract the text from all the pages of the document into a list\n","- Note: this might take a minute. There are a lot of pages ;-)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCuTTuvSbKrD"},"outputs":[],"source":["texts = [page.extract_text(x_tolerance=1) for page in pdf.pages]"]},{"cell_type":"markdown","metadata":{"id":"fFdEPAa0bKrD"},"source":["Now concatenate all the text together into a single string. \n","- We'll separate them from one another using a couple of newline characters and some spaces too"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA2Je3JpbKrE","tags":[]},"outputs":[],"source":["text = \"  \\n\\n\".join(texts)\n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"R8XeouXkbKrE"},"source":["#### Using Regular Expressions to search PDF \n","\n","Use some regular expressions to search through the text for some interesting content. \n","- You could look for email addresses, phone numbers, addresses, ...\n","- Let's try first to look for email addresses: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXMhf0aNbKrE"},"outputs":[],"source":["regex = '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n","emails = re.findall(regex,text)\n","print(emails)"]},{"cell_type":"markdown","metadata":{"id":"zqzxkQ28bKrE"},"source":["Did you find any? "]},{"cell_type":"markdown","metadata":{"id":"y5XPlU-1WapY"},"source":["POS, Chunking, NER, and SRL are all NLP tasks. \n","- Are they mentioned anywhere in the paper? \n","- Write a regular expression to find out:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JabBh00YbKrE"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"F-eWNcQbbKrE"},"source":["Other ideas to try:\n","- In this period reaserchers started using neural networks to solve NLP. Find out where neural networks are mentioned in the report and in what context.\n","- Theauthors use a data set composed of text coming from the Wall Street Journal (WSJ). Search for references to it in the PDF."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPGXsTFFbKrE"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"XBR_N4eQbKrE"},"source":["Load another PDF and write regular expressions to search for content in it. \n","- For example, you can find reports for Ferrari here: https://corporate.ferrari.com/en/investors/results/reports\n","- Let's load an interim report from September 2020 (you can find it in the same `docs` folder as before):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzSw45EtbKrF","tags":[]},"outputs":[],"source":["filename = 'docs/ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf'\n","pdf = pdfplumber.open(filename)\n","text = '\\n\\n'.join([page.extract_text() for page in pdf.pages])\n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"nd8hAX4aV0JK"},"source":["### Extracting Tables from a PDF \n","\n","Sometimes it can be useful to extract tabular data from a PDF. \n","- Tools exist that allow you to do this programmatically, making the extraction process semi-automatic.\n","- One tool that can do this is the *tabula* library. Let's install it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_rwPuYmGCVc"},"outputs":[],"source":["!pip install tabula-py"]},{"cell_type":"markdown","metadata":{"id":"naFDi4yNWyY9"},"source":["Now we can use *tabula* to extract all the tables from Ferrari's interim report above: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pQwzInjG8Rt"},"outputs":[],"source":["import tabula \n","tables = tabula.read_pdf(filename, pages=\"all\", multiple_tables=True)"]},{"cell_type":"markdown","metadata":{"id":"RNrQUIGrX6Rn"},"source":["Let's have a look at some of the tables produced\n","- the first table:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86E4O5vDH8Hq"},"outputs":[],"source":["tables[0]"]},{"cell_type":"markdown","metadata":{"id":"WXPN-2SDYCao"},"source":["- the third table:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fOAuloxYKvp"},"outputs":[],"source":["tables[2]"]},{"cell_type":"markdown","metadata":{"id":"tDCG8UeAYbwM"},"source":["It can be seen that the tables are in need of a bit of cleaning to make them usable. \n","- The tables are Pandas dataframes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWqHAlmoJSaS"},"outputs":[],"source":["type(tables[0])"]},{"cell_type":"markdown","metadata":{"id":"reWHSqBOYwcU"},"source":["So we can clean-up the table by:\n","- dropping some columns\n","- dropping some rows\n","- renaming the columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlpIdJI0Jt-n"},"outputs":[],"source":["df = tables[0]\n","df = df.drop(df.columns[[1,5]], axis=1)  # drop columns: 1,5\n","df = df.drop([0,5,12])                   # drop rows: 0,5,12\n","df = df.reset_index(drop=True)           # reset index\n","df.columns = ('Field','3months_to_30092020','3months_to_30092019','9months_to_30092020','9months_to_30092019') # rename columns\n","df"]},{"cell_type":"markdown","metadata":{"id":"TUsE_YcMbjZ_"},"source":["During the cleaning phase, some of the values may need to be updated too (e.g. certain values in the Field column above).\n","- Ideally the above cleaning operations would be done automatically.\n","- In practice, tables have lots of nested structure (including the one we just extracted), \n","- and it's still a hard research problem to do the cleaning reliably, (particularly the generation of the column names that we provided manually).  "]},{"cell_type":"markdown","metadata":{"id":"Udj4WngyWapb"},"source":["## Loading text from scanned document\n","\n","But what if my documents have been scanned? \n","- In that case the task of extracting text from them is much more difficult.\n","\n","It is possible to extract text also from images, but you will need to have an Optical Character Recognition (OCR) system installed. \n","We can use a combination of layout parsing and OCR to extract the text.\n","- Layout parser is an opensource library to detect leyoutis in images: https://towardsdatascience.com/analyzing-document-layout-with-layoutparser-ed24d85f1d44\n","- Tesseract is an opensource OCR system provided by Google. Some systems (such as Linux) come with Tesseract pre-installed. Others need to install it from here: https://tesseract-ocr.github.io/tessdoc/Home.html \n","- If you have Tesseract installed, you can follow the instructions here to use it from Python: https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052\n","\n","Give a look at the first link to see how it works, and try it yourselves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzsfKKdGe2wd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}