{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5uKcuNwee3c"
   },
   "source": [
    "# Building a Text Classifier\n",
    "\n",
    "This notebook shows how to train and evaluate a simple text classifier.\n",
    "\n",
    "We'll make use of:\n",
    "- a Twitter dataset provided by [NLTK](http://www.nltk.org/)\n",
    "- the classification models provided by [Scikit-Learn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRos-XTdrbYH"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "Data preparation is a crucial step, you must take maximum care at this point.\n",
    "\n",
    "There is a data mining principle saying \"**trash in, trash out**\". \n",
    "If you have bad data you will learn bad models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1qIioK4ee3i"
   },
   "source": [
    "### Downloading the Twitter dataset \n",
    "\n",
    "We will make use of an existing Twitter dataset available in NLTK\n",
    "- Ideally we would download our own dataset using the Twitter API (http://www.nltk.org/howto/twitter.html) but that would require an API key\n",
    "- So instead we will just download a sample of Twitter data that is provided by the NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odh-19QGee3k",
    "outputId": "0268e836-78fe-41f3-f011-89399c4f2c5b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSMMTGixee3k"
   },
   "source": [
    "Once the data is downloaded, have a look at it by printing out the first 20 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IdMJzYXee3k",
    "outputId": "ad89388c-dcb4-412d-d09e-1b7168265356"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "tweets = twitter_samples.strings()\n",
    "tweets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRmz2H69ee3k"
   },
   "source": [
    "Lots of semi-coherent musings there ...\n",
    "\n",
    "- How many tweets are there in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSklIey6ee3l",
    "outputId": "320f8451-90c9-4839-9ded-52a300c3aa07"
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of6hl8jyee3l"
   },
   "source": [
    "### Investigating the classes\n",
    "\n",
    "The tweets in twitter_samples are divided into 3 JSON files. The names of the files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-gh453vee3l",
    "outputId": "6026e439-1342-492e-9874-a19dcd607a63"
   },
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XJ6dO1Cee3l"
   },
   "source": [
    "The first two files contain tweets judged to have negative and positive sentiment, respectively. Load tweets from the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmzgzIJ3ee3m"
   },
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYbKRwhree3m"
   },
   "source": [
    "How many tweets are there in each group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WScJKfA8ee3m",
    "outputId": "1152beb6-d78d-42e3-d267-3ff8d49d7c2f"
   },
   "outputs": [],
   "source": [
    "print(f\"# positive: {len(positive_tweets)}\")\n",
    "print(f\"# negative: {len(positive_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xyHHlw1ee3m"
   },
   "source": [
    "Have a look at the first 10 tweets from the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBuD1L9-ee3m",
    "outputId": "22bf8d3a-ab3d-40f7-e984-fc429ba43e0f"
   },
   "outputs": [],
   "source": [
    "positive_tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFxmCYKaee3n"
   },
   "source": [
    "And the first 10 from the negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrIlT3Xqee3n",
    "outputId": "b0066097-2595-4421-95c6-a872bac1a2b2"
   },
   "outputs": [],
   "source": [
    "negative_tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONDUpLeGrLOl"
   },
   "source": [
    "Can you guess how the tweets have been labelled? Has someone manually inspected each tweet to decide which are positive and which are negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7uhLoqvrbYP"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2Fm5PbYee3n"
   },
   "source": [
    "#### Finding emoticons:\n",
    "\n",
    "Let's run a regular expression over the tweets to see what emoticons we can find in them.\n",
    "- Rather than develop our regular expression, we'll use one that's been designed for the task: https://regex101.com/r/aM3cU7/4\n",
    "- As you can see, it's rather long (and looks more complicated than it is).\n",
    "- If we had had a list of emoticons, we could use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "TTkPpjJ3ee3n",
    "outputId": "f5c2861e-1159-4f19-90fa-1f89204fa84e"
   },
   "outputs": [],
   "source": [
    "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "emoticon_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ceZS7xee3o"
   },
   "source": [
    "To apply the regular expression to all the positive tweets at once:\n",
    "- We'll first concatenate all of the positive tweets together into one long string separated by spaces.\n",
    "- Then apply the findall() command to look for occurrences of the regular expresssion.\n",
    "- That will return all the emoticon occurrences, and we'll use NLTK's Counter() to count their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrAjWzCsee3o",
    "outputId": "391b2c71-01eb-4774-c25a-fcd838deaffe"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "concatenated_pos_tweets = ' '.join(positive_tweets)\n",
    "emoticons_in_pos_tweets = re.findall(emoticon_regex,concatenated_pos_tweets)\n",
    "counts = nltk.Counter(emoticons_in_pos_tweets)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSWuBejbee3o"
   },
   "source": [
    "Repeat the calculation for the negative tweets (by copying and modifying the code above). \n",
    "- Are there any differences between the emoticons present in the positive and negative tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqRpGiBbee3o"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-BWUKuPee3o"
   },
   "source": [
    "#### Removing emoticons\n",
    "\n",
    "We will shortly train a classifier to detect positive versus negative tweets. \n",
    "- Before we do that, we had better remove the emoticons from the positive and negative examples. \n",
    "- Why do we need to do that?\n",
    "\n",
    "The code below runs over each tweet in the positive or negative set, and replaces any emoticon with an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5gNJ228ee3p"
   },
   "outputs": [],
   "source": [
    "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
    "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YY8nEwAee3p"
   },
   "source": [
    "Let's print out some of the tweets to check that it worked properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2HH1xg9ee3p",
    "outputId": "2f60429b-b69d-4176-c54d-c346b7cb9cf0"
   },
   "outputs": [],
   "source": [
    "positive_tweets_noemoticons[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cHmELK5ee3p",
    "outputId": "7bd29130-f492-4d73-e9ce-44e6acdd3adc"
   },
   "outputs": [],
   "source": [
    "negative_tweets_noemoticons[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut31mQqmee3p"
   },
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "Before we can train a sentiment classifier, we need to format the dataset appropriately, merging the positive and negative examples into a single list:\n",
    "- We can do this using the '+' operator in Python: [1,2,3]+[4,5,6] => [1,2,3,4,5,6]\n",
    "- We call the combined list 'tweets_x' (since 'x' is often used to denote the input to a classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAUJlI3toVh2",
    "outputId": "22325f7a-6a95-4b10-d44c-7ee28fd5ed61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
    "print(tweets_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrVT3a4Lee3q"
   },
   "source": [
    "We also need to make a list of class labels (the target variable that the classifier should learn to predict).\n",
    "- That can be done using the '\\*' operator in Python to repeat elements in a list: [1]\\*5 = [1,1,1,1,1]\n",
    "- We'll call the target variable: tweets_y (since 'y' is often used to denote the output of a classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sed0xZ33ee3q",
    "outputId": "f9723278-5154-4398-ece1-2134be06e4c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_y = ['positive'] * len(positive_tweets) + ['negative'] * len(negative_tweets)\n",
    "print(tweets_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LuKhzRHvmYA"
   },
   "source": [
    "#### Dividing the data into training and test sets\n",
    "\n",
    "In order to train a classifier we need to provide it with training data. The more data the better a classification model we will learn.\n",
    "- We could provide **all** the data we have to the learner, but then when we evaluate the model, we would record performance that is **too high** (better than expected on future data), because the model is being evaluated on data it has already seen during training.\n",
    "- So before training the model we first divide the available data into a training set and a (hold-out) test set. \n",
    "\n",
    "We'll use 80% of the data to train the model and keep 20% of the data to evaluate it.\n",
    "- Note that the default behaviour of sklearn's train_test_split() routine is to shuffle the data before assigning it to the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWghucxwufOf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2, random_state=2307)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG95HyfHso7x"
   },
   "source": [
    "NOTE: I used a random seed to make this split reproducible (the `random_state` parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI4r13RRee3r"
   },
   "source": [
    "#### Converting text into feature vectors\n",
    "\n",
    "We need to **convert the text data into a vector of feature values** that can be given to a classifier. There are many different ways to do this. If training data is scarce one might look to compute hand-crafted features, such as: \n",
    "- *syntax based features*, such as the number of capitalised words in the text, \n",
    "- *part-of-speech based features*, such as the number of verbs versus the number of proper nouns, etc., or \n",
    "- *reading difficulty based features*, like the average length of words/sentences, the amount of punctuation used, etc.\n",
    "\n",
    "But by far the **most common features** to extract from the text aree simply the *frequencies of the words* themselves. The vocabulary present in a document provides a very strong signal about the meaning of the document and thus the category it should belong to:\n",
    "- One can represent documents (especially short ones like Tweets) as binary vectors (i.e. by their vocabulary). \n",
    "- But the number of times each word occcurs in the document provides further information about its meaning, so we normally represent documents as vectors of word counts. This is known as a **'bag-of-words'** representation of a document.\n",
    "\n",
    "We can extract such a representation automatically using the CountVectorizer object from sklearn. \n",
    "- We first 'fit' the vectorizer on the training data. During this phase, sklearn determines what the vocabulary of the collection of documents is, so that it knows how many dimensions each feature vector must contain (and what word each dimension corresponds to)\n",
    "- Once the vectorizer has been fit to the training set, we can go back to the training data, applying the vectorizer to generate the bag-of-words representation for each document.\n",
    "\n",
    "Let's fit the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNy0sTleee3r",
    "outputId": "0c57987a-226f-4413-a7bb-2b5611654a9e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQOEW5Z8ee3r"
   },
   "source": [
    "The output lists all the parameters with which the vectorizer object was instanciated. We'll talk about many of these parameters shortly, but for the moment, note the following: \n",
    "- We are not currently removing any stopwords\n",
    "- A regular expression, called *token_pattern*, is used to extract the tokens from the text: **'(?u)\\\\b\\\\w\\\\w+\\\\b'**\n",
    "  - (?u) says the pattern accepts unicode characters, so characters like ü should work (https://docs.python.org/2/library/re.html#re.U)\n",
    "  - \\\\w\\\\w+ says the pattern is looking for 2 or more consecutive word characters \n",
    "  - \\\\b is a special character denoting word boundaries, so only full words will be returned (i.e. '\\\\bat\\\\b' only matches the word 'at' and not 'cat', https://www.regular-expressions.info/wordboundaries.html)\n",
    "\n",
    "The vectorizer refers to each vocabulary element as a 'feature'. The number of features is:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syOpCyReee3s",
    "outputId": "fd10c48d-4a83-4053-937a-9a925ba7d639"
   },
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdaYcMqDee3s"
   },
   "source": [
    "That looks big, but is actually quite normal for a vocabulary. \n",
    "- Adults tend to have a vocabulary of at least 20,000 terms (see https://www.economist.com/johnson/2013/05/29/lexical-facts), and in NLP applications, vocabularies of 100,000+ are usual. \n",
    "\n",
    "Since we don't have that much training data (only 4,000 instances of each class), such a large vocabulary could possibly be an issue, however. \n",
    "- Let's have a look at the first 1000 terms in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_FGrv08ee3s",
    "outputId": "d24160f1-f088-41cd-a273-67441e6589dc"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjMqY8lCee3s"
   },
   "source": [
    "### Limiting the Vocabulary\n",
    "\n",
    "\n",
    "There seem to be a large number of unusual/rare words in there (although we are looking at the start of the list).\n",
    "- We could reduce the vocabulary to include *only the most frequent terms* across the collection. \n",
    "- Or we could limit the vocabulary to include *only words that occur in at least a certain number of documents*. \n",
    "\n",
    "Both of these changes can be implemented by passing parameters to the CountVectorizer() method: \n",
    "- the first requires setting a value for **max_features** \n",
    "- the second a value for **min_df** (here 'df' stands for 'document frequency' = # of documents that the word appears in across the collection) \n",
    "\n",
    "If a word occurs in only a few documents across the training set, we won't have seen enough evidence to associate its presence with one of the classes. So let's try to reduce the vocabulary by requiring that each word appears in at least 5 documents. We'll also make use of an english stopword list to get rid of the highly frequent words that are unlikely to provide much useful information about the class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBKD7j3Hee3s",
    "outputId": "98c65871-87e4-4658-f438-63a650d6d9e8"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, stop_words='english')\n",
    "vectorizer.fit(train_x)\n",
    "print(f\"vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QopmkO-dee3t"
   },
   "source": [
    "Wow, that significantly reduced the size of the vocabulary!\n",
    "\n",
    "Now if we print out the vocabulary again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foobIt6Pee3t",
    "outputId": "d9295711-865a-4015-84ec-756606df0fb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNDT65ONee3t"
   },
   "source": [
    "We see that it has far fewer rare words in it.\n",
    "- Note: information has been lost by removing the rare words, so the best setting for what vocabulary to use for a classfication problem should be determined empirically. (More on that later.)\n",
    "\n",
    "Now that we have established what the feature representation will be, \n",
    "- we can generate the bag-of-words/vector representation for each document in the training data, by simply calling the transform() method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-qApmLyee3t",
    "outputId": "1e0bbe1c-7138-4ca8-8755-76fa33504034"
   },
   "outputs": [],
   "source": [
    "train_x_vector = vectorizer.transform(train_x)\n",
    "train_x_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3pmZE8uee3t"
   },
   "source": [
    "The output tells us that: \n",
    "- We now have a matrix of 8000 training examples, where each instance is a vector of length the size of the vocabulary. \n",
    "- The information is stored as a 'sparse matrix', which is a memory efficient data structure for holding matrices where most values are zero. The matrix is sparse because most documents only contain a very small vocabulary, i.e. the count for almost all vocabulary terms in each document is zero. (In fact, the number of non zero elements in the entire matrix is only arond 27k.)\n",
    "- The sparse format is critical if we have a large document collection (and/or a large vocabulary) in order to prevent the system from needlessly running out of memory. \n",
    "\n",
    "Let's have a look at the vector representation of a random tweet from the collection. \n",
    "- Run the following code a few times until you find an interesting tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VFMgZ6Cee3t",
    "outputId": "b80d4152-8db3-4e04-cf27-3176c5049719"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randrange(len(train_x))\n",
    "\n",
    "print('random tweet:')\n",
    "print(train_x[i])\n",
    "\n",
    "print('\\nsparse vector:')\n",
    "print(train_x_vector[i])\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print('\\nvocabulary of tweet:')\n",
    "[(j,vocab[j]) for j in train_x_vector[i].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EJYhJo8rbYb",
    "tags": []
   },
   "source": [
    "## Model building\n",
    "\n",
    "Now that our data set is ready for use, we can start training our first text classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfJUKdWeee3u"
   },
   "source": [
    "### Training a classifier\n",
    "\n",
    "Now that we have feature vectors to represent each tweet, we can finally start training a classfication model. \n",
    "- We will start with Logistic Regression, which is a work-horse of a linear classifier that reliably gives good classification performance. \n",
    "- To train the model, we simply create a model and fit it to the training data (both the instances and the class labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAScoeaPee3u",
    "outputId": "93054ee0-376e-4939-a998-7a458748de44"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(train_x_vector, train_y)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icu7qc7_ee3u"
   },
   "source": [
    "Fitting a model can sometimes take time, but for this small amount of data it is very quick. \n",
    "\n",
    "Printing the model didn't actually print out the model, just the arguments that LogisticRegression used when fitting it.\n",
    "- These includes technical details regarding the solver used ('lbfgs') and the maximum number of iterations (100) run to fit the model. \n",
    "- The most interesting parameters for us are probably the 'C' term, which controls the amount of regularisation applied to prevent the model from overfitting the training data, and the 'penalty', which determines what type of regularisation is being performed. In this case, an 'l2' penalty has been applied. Interesting would be to try an 'l1' penalty which will try to reduce the number of features used in the model. (More on that later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmHOX8Qnee3u"
   },
   "source": [
    "### Making Predictions\n",
    "\n",
    "Now that we have fit the model, we can use it to make a prediction!\n",
    "\n",
    "Let's create some examples, transform them into feature vectors, and then use the model.predict() function to make predictions for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTMZMIlRee3v",
    "outputId": "711be4af-5815-4620-8542-1536ce23c2d4"
   },
   "outputs": [],
   "source": [
    "tweet0 = 'I can\\'t believe how much fun I\\'m having learning to train a text classifier!'\n",
    "tweet1 = 'I am really confused. I want my mommy.'\n",
    "tweet2 = 'The internet connection has been pretty annoying today!'\n",
    "tweet3 = 'They just played my favourite song on the radio.'\n",
    "tweet4 = \"I don't like going to the dentist.\"\n",
    "transformed_tweets = vectorizer.transform([tweet0,tweet1,tweet2,tweet3,tweet4])\n",
    "predictions = model.predict(transformed_tweets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uNvpdwDee3v"
   },
   "source": [
    "Well, that seemed to work!\n",
    "- Try entering some other examples and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYuEMLji6lt0",
    "outputId": "854c68a1-e7e8-4e18-fa14-cb2a14dad2b9"
   },
   "outputs": [],
   "source": [
    "tweet5 = 'This classifier is working really well. I\\'m happy'\n",
    "tweet6 = 'I had a really bad day today'\n",
    "transformed_tweets = vectorizer.transform([tweet0,tweet1,tweet2,tweet3,tweet4,tweet5,tweet6])\n",
    "predictions = model.predict(transformed_tweets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8e3TGoo6mnx"
   },
   "source": [
    "As well as providing predictions, we can get the model to produce probabilities that quantify the classifier's confidence in the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRqi__Vuee3v",
    "outputId": "2cdd2994-8210-4dd0-bece-2c3a3cece1c5"
   },
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict_proba(transformed_tweets)\n",
    "print(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDxyiC-h69g1"
   },
   "source": [
    "The first value is the predicted probablity of the negative class, the second of the positive class.\n",
    "- For which tweet is the classifier least confident of its prediction?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WGSL82hrbYe"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIOg0afiA-OF"
   },
   "source": [
    "### Investigating the Model\n",
    "\n",
    "Logistic Regression is a **linear classifier**: \n",
    "- That means that it divides the feature space into two regions separated by a linear boundary (called a hyperplane). \n",
    "- It also means that the classification model consists of one parameter for each feature, and that the predictions of the model are computed by multiplying each feature value by its corresponding parameter value. \n",
    "- Thus, since all our features are merely word counts, we can investigate the importance of the different words for the prediction by inspecting the parameters of the model used. \n",
    "\n",
    "Let's start by printing out the parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNUBj_1hEM8E",
    "outputId": "a8cef46f-d081-4a64-c77f-47bc36881d31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = [(vocab[j],model.coef_[0][j]) for j in range(len(vocab))]\n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yAgz8YGUpAR"
   },
   "source": [
    "That's a big list! \n",
    "- But did you notice some of the more negative words having negative coefficients?\n",
    "- Let's have a look at the 50 words with the highest negative values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pc9FeR6RU7qP",
    "outputId": "262b3ddc-f12f-4763-9914-c342179dfe62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted(model_params, key=lambda x: x[1])[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETpTOKc4WFBt"
   },
   "source": [
    "And those with the highest positive coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U75UnToTWSG1",
    "outputId": "da255d9c-8622-4fb9-dec3-2a6b815cc5e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted(model_params, key=lambda x: -x[1])[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNluHNLsee3v"
   },
   "source": [
    "It seems like the model is learning well to distinguish between words with positive and negative sentiment.  \n",
    "\n",
    "We can also look at individual tweets to understand why the model makes the predictions it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JuaHYifee3v",
    "outputId": "936cbd44-3bb1-4e8c-bff3-b2fdbd3a9df7"
   },
   "outputs": [],
   "source": [
    "tweet = \"I don't like going to the dentist.\"\n",
    "vectorized_tweet = vectorizer.transform([tweet])[0]\n",
    "print('tweet:')\n",
    "print(tweet)\n",
    "print('model coefficients:')\n",
    "[(j,vocab[j],model.coef_[0][j]) for j in vectorized_tweet.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjnXN43wee3w"
   },
   "source": [
    "### Evaluating the Accuracy of the Model's Predictions\n",
    "\n",
    "To understand how the classifier is likely to perform on average on new examples, we need to test it on a large dataset which it hasn't seen for training.\n",
    "- Thankfully, we held-out 20% of our data exactly for this purpose ;-) \n",
    "- So let's now compute the predictions on that held-out data, (which we called 'test_x' with corresponding ground-truth labels 'test_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N1iC-FIee3w"
   },
   "outputs": [],
   "source": [
    "test_x_vector = vectorizer.transform(test_x)\n",
    "pred_y = model.predict(test_x_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ddoG4WAee3w"
   },
   "source": [
    "The first measure to compute is accuracy, which is simply the number of times the classifier correctly predicted the actual class of the document.\n",
    "- If the positive and negative classes are fairly balanced (i.e. we have around the same number of positive and negative instances in our test set) and we don't care whether we misclassify a positive as a negative or vice versa, then accuracy is the measure to use.  \n",
    "- We can calculate accuracy by importing the accuracy_score() function from sklearn's metrics. The function takes both the predicted and true labels as input and returns the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzzOZ-xzee3w",
    "outputId": "0ee8986b-0dff-4326-826c-4ae62cd93054"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'accuracy: {accuracy_score(pred_y, test_y):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_XSda0tee3w"
   },
   "source": [
    "Well, that's not terrible -- but it's not fantastic either. \n",
    "- The model predicts the sentiment correctly **76%** of the time.\n",
    "- For short and noisy documents like tweets, and where the ground-truth labels were extracted automatically from emoticons, a success rate around 3/4 is probably not too bad.\n",
    "- For longer and less noisy documents, we would expect the accuracy for a binary prediction problem to be much higher -- in the nineties usually.\n",
    "\n",
    "How could we improve performance? \n",
    "- The obvious and most reliable way to improve the performance of a text classifier is to **collect more data**. The more data the classifier has, the better the performance will be.\n",
    "- The second most obvious way would be to try to collect less noisy data.\n",
    "- It may be the case, however, that for this particular task, an accuracy much higher than the current value is not achievable because the sentiment of short tweets isn't that predictable.  \n",
    "\n",
    "Let's try to understand where the model is making its errors by printing out the confusion-matrix, which for a binary classification task is simply the matrix of true positive, false negative, false positive and true positive counts (https://en.wikipedia.org/wiki/Confusion_matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "GRctoTcVzvh8",
    "outputId": "befffcf5-f94b-4dfd-8d4f-d87347ed4d56"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(model, test_x_vector, test_y, values_format='d')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dACCivH_st8"
   },
   "source": [
    "It looks like the model is relatively balanced, but confusing a few more positives for negatives than the other way around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZIOgu15ee3w"
   },
   "source": [
    "### Precision, Recall and F-Measure\n",
    "\n",
    "Finally, we can generate more useful metrics, such as Precision, Recall and F-measure for the two classes. What are they?\n",
    "- Precision (for the positive class) is the percentage of instances predicted as positive that actually were positive\n",
    "- Recall (for the positive class) is the percentage of positive instances that were predicted as positive by the model\n",
    "- F-measure is a metric that combines (via the harmonic mean) the precision and recall values. We often try to optimise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4JwasqJee3w",
    "outputId": "2fbc9ba9-8c38-4275-a333-efb60761bf7b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('\\nclassification report:\\n')\n",
    "print(classification_report(test_y, pred_y, target_names=['positive','negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6xL7sbQee3x"
   },
   "source": [
    "### Comparing different Models on a Validation Set\n",
    "\n",
    "There are many ways one could go about improving the quality of the classifier trained above. \n",
    "- One could change the model, its arguments, or the feature space. This process is called **model selection**.  \n",
    "\n",
    "**Before** embarking on any of these tests, we must first further partition the training data into a training and validation set. \n",
    "- Why would we do that? Because we want to make sure we don't end up learning a model that works well on the test set, but doesn't generalise well to new data. \n",
    "- By keeping the test set for the evaluation of the final model, we know that we have not inadvertantly optimised the model on it. \n",
    "\n",
    "In practice, if we are working with relatively small datasets as is the case here, we would perform k-fold cross-validation rather than simply repeating hold-out for the validation set. In this case, k distinct models are trained and evaluated on partitions of the data with k-1 partitions used for training and 1 for testing on each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xf9Of5E-ee3x",
    "outputId": "7649190d-ac81-43eb-cb1e-1d8a542b7fed"
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=2307)\n",
    "print(f\"# train: {len(train_x)}\")\n",
    "print(f\"# valid: {len(valid_x)}\")\n",
    "print(f\"# test: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgfQ56GCgUGi"
   },
   "source": [
    "Note: be careful to execute the cell above only once, since it replaces the input variables with new ones each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSFjetGqee3x"
   },
   "source": [
    "We could now try different linear classifiers: \n",
    "- such as MultinomialNB -- a model that is traditionally very often used with text\n",
    "- a Support Vector Machine (with a linear kernel) -- a **very reliable model for high dimensional data** \n",
    "- or even a non-linear classifier, such as XGBoost\n",
    "\n",
    "Each of these classifiers have hyperparameters that one might try to tune, such as:\n",
    "- the type of regularisation (l1 versus l2) -- l1 can work well to reduce the feature set for text problems\n",
    "- the parameter controlling the strength of the regularisation \n",
    "\n",
    "Perhaps more important than the classification model trained is the set of features used. We could to **change the feature space** by: \n",
    "- increasing or reducing the vocabulary\n",
    "  - add stopwords \n",
    "  - change the maximum feature count or the min_df value\n",
    "- adding **new features**\n",
    "  - in particular **ngrams** are often used to improve text classifiers\n",
    "  - we include bigrams pairs of consecutive words as features\n",
    "- rescaling the features\n",
    "  - using TF-IDF weighting scheme that is particularly useful for web search since it can sometimes help the classifier to find the appropriate value for the coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRE85hPYdDxH"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-hYsuojdEwf"
   },
   "source": [
    "Here I train an SVM classifier on the the reduced training data and check it's performance on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9PtkKluee3x",
    "outputId": "308dc909-1c69-4b86-973a-a1201ba4d595"
   },
   "outputs": [],
   "source": [
    "train_x_vector = vectorizer.transform(train_x)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, tol=None).fit(train_x_vector, train_y)\n",
    "\n",
    "valid_x_vector = vectorizer.transform(valid_x)\n",
    "pred_y = model.predict(valid_x_vector)\n",
    "\n",
    "print('Results for the basic SVM classifier:')\n",
    "print(f'accuracy: {accuracy_score(pred_y, valid_y)}')\n",
    "print(classification_report(valid_y, pred_y, target_names=['positive','negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvlTdehsrbYj"
   },
   "source": [
    "NOTE: the SGD classifier is a generic tool for linear classifiers, SVMs are a special case of SGD classifier using the hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XjHo-Vnee3x"
   },
   "source": [
    "Try out some other models! \n",
    "- What setting results in the best peformance? \n",
    "- What is the test accuracy in that case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2Oj5lQEee3x"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Z9ZCoItp3d"
   },
   "source": [
    "For example, we could try Logistic Regression with an L1 penalty, aimed at removing features that are not useful for the classification (i.e. performing feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBJtl_bGrbr1",
    "outputId": "fa359a30-feab-4b63-bad4-d92452959536"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l1',solver='saga',max_iter=4000).fit(train_x_vector, train_y)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BimO2gteuIlw"
   },
   "source": [
    "The L1 penalty causes a reduction in the number of non-zero coefficients in the model. \n",
    "- This is referred to as a sparse model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyLmubbDsRe5",
    "outputId": "714981c4-54cd-4801-f54e-9bf35a939bd8"
   },
   "outputs": [],
   "source": [
    "print(f'Number of vocabulary elements {len(vocab)}')\n",
    "print(f'Number of non-zero coefficients: {sum(sum(model.coef_ != 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKKf5vs0de6T"
   },
   "source": [
    "The performance of the sparse model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGagyB8zdfi4",
    "outputId": "d7559a2c-4bb2-444b-b1f7-d6b57a61d888"
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(valid_x_vector)\n",
    "\n",
    "print('Results for L1-regularized Logistic Regression classifier:')\n",
    "print(f'accuracy: {accuracy_score(pred_y, valid_y)}')\n",
    "print(classification_report(valid_y, pred_y, target_names=['positive','negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDibvJKLuMM_"
   },
   "source": [
    "Let's have a look at the coefficients of the sparse model\n",
    "- Note all of the words deemed to not be discriminative of the class and given a coefficient of zero:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQoqTz6ouBMh",
    "outputId": "63d94c55-b80d-494e-b864-85658d0181b6"
   },
   "outputs": [],
   "source": [
    "model_params = [(vocab[j],model.coef_[0][j]) for j in range(len(vocab))]\n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFOrYSgaee3x"
   },
   "source": [
    "## Further readings\n",
    "\n",
    "This recent blog post is worth reading. The author develops a text classifier for identifying COVID-19 missinformation: \n",
    "- https://towardsdatascience.com/automatically-detect-covid-19-misinformation-f7ceca1dc1c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE_FzqUTee3y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
