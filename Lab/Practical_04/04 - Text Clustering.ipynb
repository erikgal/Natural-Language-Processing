{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-yhMehNjx0A"
      },
      "source": [
        "# Text Clustering\n",
        "\n",
        "In this session we will learn how to cluster text documents. Clustering is an unsupervised learning technique that is useful when we don't know what categories exist in the data. \n",
        "\n",
        "Parts of the following tutorial are based on this scikit-learn page: [link](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg72HPvadv3J"
      },
      "source": [
        "## Data prepration\n",
        "\n",
        "As usual we start from data prepration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0YtXd0bjx0I"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "First thing we need to do is load a set of documents that we can cluster. \n",
        "\n",
        "We will use the *News Category Dataset*, openly available on Kaggle ([data set link](https://www.kaggle.com/datasets/rmisra/news-category-dataset)). The dataset contains more than 200,000 news headlines from 2012 to 2022 from HuffPost.\n",
        "\n",
        "The data set is available as a single JSON file, we can load it using Python's built in json package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6dqkbnyjx0I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('docs/News_Category_Dataset_v3.json') as f:\n",
        "    dataset = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glk4bo711Gvf"
      },
      "source": [
        "Since the data set consists of a large set of documents it may be useful for the scope of this tutorial to subsample it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3d5NDFB1HIE"
      },
      "outputs": [],
      "source": [
        "# Skip this cell if you want to use the entire data set\n",
        "dataset = dataset[::5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaX10FEnjx0J"
      },
      "source": [
        "Let's check the size of the data set: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqqT_RyYjx0J"
      },
      "outputs": [],
      "source": [
        "print(f\"No. of documents: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA6LMxs_jx0K"
      },
      "source": [
        "Each document is composed of an headline and a short description of the news. \n",
        "\n",
        "In this data set documents are organised into Python dictionaries to store some additional information. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXlURLDXjx0K"
      },
      "outputs": [],
      "source": [
        "for key, value in dataset[0].items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1wpmDOyjx0K"
      },
      "source": [
        "Each document is assocated with one of out of 42 different categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSFtzrcjx0K"
      },
      "outputs": [],
      "source": [
        "print(f\"Label for the first example is: {dataset[0]['category']}\")\n",
        "print('\\nCategories:')\n",
        "{sample['category'] for sample in dataset}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blngQ9A8Pysg"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "Before moving to clustering and topic modelling we need to preprocess our data set to extract the documents and the labels inside.\n",
        "\n",
        "We are going to create our collection of documents combining the headline and the short description of a sample in our data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9szzLc_6QZxc"
      },
      "outputs": [],
      "source": [
        "documents = [f\"{sample['headline']}\\n\\n{sample['short_description']}\" for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsQi86eQ867"
      },
      "source": [
        "Now the first document looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT2Rae1uRACK"
      },
      "outputs": [],
      "source": [
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujr9Myc7REuv"
      },
      "source": [
        "Apart from the documents, we also need to retain the associated labels. Even though this is an unsupervised learning task, it would be useful to keep the labels for the assessment of the clustering results (more on this later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp7BZ2b5R3c7"
      },
      "outputs": [],
      "source": [
        "labels = [sample['category'] for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaY9T0_1XazR"
      },
      "source": [
        "Let's give a look at the labels distribution, first we need to count the occurrences of each label.\n",
        "\n",
        "To do so we can use Python's `Counter`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzQB3HSfYhb1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_count = Counter(labels)\n",
        "label_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v2sP64KYgtZ"
      },
      "source": [
        "Now we can visualise the counts of each label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4aiQHU5ap71"
      },
      "outputs": [],
      "source": [
        "label_count.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPjoEsyNYBsk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(8, 16))\n",
        "plt.barh(range(len(label_count)), label_count.values())\n",
        "plt.yticks(range(len(label_count)), label_count.keys())\n",
        "plt.xscale('log')\n",
        "plt.xlim([1, 10000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ilZ5gbX9ku"
      },
      "source": [
        "Some labels are way more frequent than the others, note that we are using a logarithmic scale on the x-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVrttz4Ejx0L"
      },
      "source": [
        "### Documents vectorisation\n",
        "\n",
        "In order to cluster the documents, we need to first convert them into a vector format. We will use the `TfidfVectorizer` from Scikit-Learn to do this.\n",
        "\n",
        "The `TfidfVectorizer` is very similar to the `CountVectorizer` we used in the text classification tutorial except that it multiplies the term frequency in the document by the inverse document frequecy of the term across the corpus: $\\mathrm{tf}(t, d) \\cdot \\mathrm{idf}(t)$\n",
        "- here $\\mathrm{tf}(t, d)$ is the count of the term $t$ in the document $d$\n",
        "- idf is inverse document frequency: $\\mathrm{idf}(t) = \\log{\\frac{n + 1}{\\mathrm{df}(t) + 1}} + 1$\n",
        "- $n$ is the number of documents in collection\n",
        "- and the document frequency, $\\mathrm{df}(t)$ is the number of documents that contain the term $t$\n",
        "- see https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "\n",
        "The use of idf-weighting can be justified in term of information theory:\n",
        "- the amount of **information** associated with a term quantifies the **amount of surprise** at seeing a term in a document \n",
        "- the surprise decreases with its (prior) probability of occurrence and information must be additive, so we have:\n",
        "  - $I(t) = \\log{\\frac{1}{P(t)}} = -\\log{P(t)}$\n",
        "- the probability of observing a particular term in a document is just the percentage of documents that contain the term:\n",
        "  - $P(t) = \\frac{\\mathrm{df}(t)}{n}$\n",
        "- We smooth this estimate so that small df(t) values don't cause unreasonably high idf values, we have:\n",
        "  - $P(t) = \\frac{\\mathrm{df}(t)+1}{n+1}$\n",
        "  - so $I(t) = \\log{\\frac{n+1}{\\mathrm{df}(t)+1}}$\n",
        "- The idf value used is then $\\mathrm{idf}(t) = I(t) + 1$, where the $+1$ prevents idf from going to zero as df approaches $n$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRQ9pk7zjx0L"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, stop_words='english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ5V_9zxjx0L"
      },
      "source": [
        "The vectorisation method takes a number of arguments that control the resulting vocabulary. We have set the following arguments:  \n",
        "- **max_df = 0.5**: remove words occuring in more than half of the documents (note: this will get rid of any corpus-specific tags) \n",
        "- **min_df = 5**: remove words occuring in less than 5 documents  \n",
        "- **stop_words = 'english'**: remove stopwords using an english stopword list \n",
        "\n",
        "We can now fit the vectorizer to the data: \n",
        "- Note: we could transform the data at the same time, using the `fit_transform()` method, but we'll wait for now to transform the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYbZbdOHjx0M"
      },
      "outputs": [],
      "source": [
        "vectorizer.fit(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Li4efibjx0M"
      },
      "source": [
        "Let's have a quick look at the vocabulary. How big is it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B04JJixbjx0M"
      },
      "outputs": [],
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"Length of vocabulary: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpkniToMjx0M"
      },
      "source": [
        "Wow, that's quite big! \n",
        "\n",
        "Let's print out a random sample of 100 terms from it to see what they look like: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB0p0LRwjx0M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sorted(random.sample(vocab.tolist(),100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRjq-QKjjx0N"
      },
      "source": [
        "Most of the terms look pretty good, but the representation looks reasonable. \n",
        "\n",
        "Now let's vectorize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ0GLWugUOh9"
      },
      "outputs": [],
      "source": [
        "vector_documents = vectorizer.transform(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfuUeeP9UO7b"
      },
      "source": [
        "Here is the sparse vector for the first document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj4V1lvAjx0N"
      },
      "outputs": [],
      "source": [
        "print(vector_documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isn7Zcg346xq"
      },
      "source": [
        "The decimial values are the TF-IDF scores for the terms. We can sort the terms by their TF-IDF values, and print them out as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsqhVl4ikD9O"
      },
      "outputs": [],
      "source": [
        "sorted([(vocab[j], vector_documents[0, j]) for j in vector_documents[0].nonzero()[1]], key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLn6JsX6jx0N"
      },
      "source": [
        "Do the top terms agree with what you expected for the document?\n",
        "\n",
        "Print it out below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_JR3ghZjx0N"
      },
      "outputs": [],
      "source": [
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK9UEdzTdv3U"
      },
      "source": [
        "## Data exploration\n",
        "\n",
        "Let's play a bit with our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lgXYcLcjx0N"
      },
      "source": [
        "### Measuring the similarity between vectorised documents \n",
        "\n",
        "The vectorizer also normalizes the resulting document representations such that their vectors have length one.\n",
        "- We can see this by computing the dot-product between a vector representation and itself. \n",
        "- For example, for the first document in the collection we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51HAr3GD9j5_"
      },
      "outputs": [],
      "source": [
        "vec = vector_documents[0]\n",
        "vec.multiply(vec).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HirY1HDqjx0O"
      },
      "source": [
        "To calculate the dot-product we multiplied the sparse vector by itself and then took the sum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbPYGVOgjx0O"
      },
      "source": [
        "The fact that the vectors have length one (almost length of one due to approximations in the representation) means that the dot-product between vectors computes the cosine of the angle between them. \n",
        "- The cosine of the angle between tf-idf vectors provides a value in the range [0,1], that is often used to measure the similarity between documents.\n",
        "- Let's compute the similarity between the first two documents in the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8knZ-8g_RTf"
      },
      "outputs": [],
      "source": [
        "vector_documents[0].multiply(vector_documents[1]).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWAqP-akVGj"
      },
      "source": [
        "Here is the second document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ib2zLYtkMHF"
      },
      "outputs": [],
      "source": [
        "print(documents[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EWh6NCajx0O"
      },
      "source": [
        "The similarity value is zero, which isn't surprising since most documents don't share vocabulary. \n",
        "\n",
        "The average vocabulary size of a document in the collection is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8TJsl0fAq5B"
      },
      "outputs": [],
      "source": [
        "nonzero_count = vector_documents.count_nonzero()\n",
        "doc_count = vector_documents.get_shape()[0]\n",
        "\n",
        "print(f\"Average document vocabulary size: {nonzero_count/doc_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMMX-zLSjx0P"
      },
      "source": [
        "The first two documents came from different categories: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iu6v4Bbjx0P"
      },
      "outputs": [],
      "source": [
        "print(f\"Category of 1st document: {labels[0]}\")\n",
        "print(f\"Category of 2nd document: {labels[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOPhfcTUjx0P"
      },
      "source": [
        "What if we compute the compute similarity between  documents from the same newsgroup? Let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4rK6qmQFEKu"
      },
      "outputs": [],
      "source": [
        "for i in range(1,200):\n",
        "    if (labels[i] == labels[0]):\n",
        "      print('Similarity:', vector_documents[0].multiply(vector_documents[i]).sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEdTj7IG5VcW"
      },
      "source": [
        "Here documents are quite short, so it is hard to get a proper overlapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qace1GLwjx0Q"
      },
      "source": [
        "### Searching the collection based on keywords\n",
        "\n",
        "We could even use the same approach to compute the similarity between a search query and each of the documents in a collection in order to find the one that best matches with a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghNnSfaUImnU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "query = 'australia'\n",
        "# query = 'computer'\n",
        "# query = 'covid'\n",
        "# query = 'rock \\'n roll'\n",
        "\n",
        "query_vec = vectorizer.transform([query])[0]\n",
        "\n",
        "index = np.argmax([query_vec.multiply(vector_documents[i]).sum() for i in range(len(documents))])\n",
        "print(documents[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkCUYnGpjx0Q"
      },
      "source": [
        "That worked!\n",
        "- Try some other queries. \n",
        "- Does that reminds you of something we saw recently?\n",
        " - You could try implement the BM25 and try it on this data set https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjN-RbrXdv3b"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "Now we can start applying clustering to our vectorised documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW6NlB7Bjx0Q"
      },
      "source": [
        "### Clustering with k-Means\n",
        "\n",
        "Now that we have computed a vector representation, we can perform the clustering.\n",
        "\n",
        "There are many different clustering algorithms in common use, from k-Means and Hierarchical clustering to DBScan and Spectral clustering.  \n",
        "- Many of them are implemented in Scikit-learn: https://scikit-learn.org/stable/modules/clustering.html\n",
        "- You can easily change the code below to try them out. \n",
        "\n",
        "We will use the k-Means algorithm since it is very popular, fast, scalable and relatively robust.\n",
        "- k-Means models the dataset using k circlular clusters\n",
        "- where each cluster is represented by the centroid of the datapoints it contains.\n",
        "\n",
        "To apply k-Means we need to decide in advance how many clusters to look for. \n",
        "- We will set the number of clusters to be exactly the number of categories in the dataset to see if the clustering can recover the original groups from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06KgiHgRjx0R"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = len(set(labels))\n",
        "\n",
        "kmeans = KMeans(n_clusters=k, max_iter=100, n_init=2, verbose=True, random_state=2307)\n",
        "kmeans.fit(vector_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6_HOjgjx0R"
      },
      "source": [
        "The inertia values are actually the *sum of squared distances between each sample to its closest cluster centroid*\n",
        "- This is the measure that k-Means seeks to minimise \n",
        "- Note: It is not the cosine distance between document and the cluster center (which is what we would like to minimise), but given that each vector has length one, the squared Euclidean distance is similar to it.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp9Sz7qwfzb8"
      },
      "source": [
        "### Clustering evaluation\n",
        "\n",
        "Once we have run our clustering algorithm, we can "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk5-p7pHjx0R"
      },
      "source": [
        "#### Investigating the Clusters\n",
        "\n",
        "The clustering routine has now produced clusters in a very high dimensional feature space (with tens of thousands of dimansions). \n",
        "- We can't plot such high dimensional data to see whether the clusters look coherent or not. \n",
        "- Instead we will need to investigate the coherency of the clusters by looking at the terms occuring in them. \n",
        "\n",
        "We can see the important terms for each cluster by inspecting the centroid vector for the cluster.\n",
        "- Let's have a look at the terms with high weights in the centroid of the first cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z9K8BbQjx0R"
      },
      "outputs": [],
      "source": [
        "# Get the centroid for the first cluster\n",
        "centroid = kmeans.cluster_centers_[0]\n",
        "\n",
        "# Sort terms according to their weights \n",
        "# (argsort goes from lowest to highest, we reverse the order through slicing)\n",
        "sorted_terms = centroid.argsort()[::-1]\n",
        "\n",
        "# Print out the top 10 terms for the cluster\n",
        "[vocab[j] for j in sorted_terms[:20]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJeH223Jjx0S"
      },
      "source": [
        "The top 10 terms look pretty consistent to me. What do you think?\n",
        "- Note that the k-Means algorithm makes use of random initialisations, \n",
        "- so every time we run it, we will get a different assignment of cluster IDs to the clusters \n",
        "- (although the clusters themselves should remain relatively stable). \n",
        "\n",
        "We could use our dot-product search trick from before to find the document in the collection that is the best exemplar of the cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch7_mkEgjx0S"
      },
      "outputs": [],
      "source": [
        "index = np.argmax([np.dot(centroid,vec) for vec in vector_documents.toarray()])\n",
        "\n",
        "print(documents[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAxCqdZ7jx0S"
      },
      "source": [
        "We now know what terms were important for defining cluster 0 and have seen a representative document, \n",
        "- but still don't know how many documents were assigned to to the cluster. \n",
        "- Let's find out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ke_rNyjx0T"
      },
      "outputs": [],
      "source": [
        "sum(kmeans.labels_ == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjxa7rz2jx0T"
      },
      "source": [
        "We can see which clusters the first 10 documents have been assigned to: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smbJ3CZijx0T"
      },
      "outputs": [],
      "source": [
        "for i in range(10): \n",
        "    print(f\"document {i} is in cluster {kmeans.labels_[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgFEc-ZIjx0T"
      },
      "source": [
        "Let's now repeat the process (algorithmically) to get the top terms for all the clusters.\n",
        "- In the code below, we iterate over the clusters, getting the centroid for each, sort the values and printing out the top terms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "988vRwwLjx0T"
      },
      "outputs": [],
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i in range(kmeans.n_clusters):\n",
        "    centroid = kmeans.cluster_centers_[i]    \n",
        "    sorted_terms = centroid.argsort()[::-1]\n",
        "    print(f\"Cluster {i}:\\t{[vocab[j] for j in sorted_terms[:10]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te2DU6k-jx0T"
      },
      "source": [
        "That's interesting. \n",
        "- It looks like each of the clusters contains relatively consistent terms.\n",
        "\n",
        "How many documents have been assigned to each cluster? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZJ6coRvjx0T"
      },
      "outputs": [],
      "source": [
        "print('Number of docs in: ')\n",
        "\n",
        "for i in range(kmeans.n_clusters):\n",
        "    print(f\"Cluster {i}: {np.sum(kmeans.labels_ == i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h62RxhbNjx0U"
      },
      "source": [
        "Looks like there is a very large cluster corresponding to terms that might be more common across the corpus. Mostly, though, the counts are pretty well distributed across the clusters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xxv5DUjjx0U"
      },
      "source": [
        "#### Quantitative Evaluation of Clustering Results \n",
        "\n",
        "Let's now move on to the task of quantitatively evaluating the clustering algorithm. There are two ways to evaluate the results of a clustering algorithm:\n",
        "- **Intrinsic evaluation** - If the ground truth is not known, you could use:\n",
        "    - Within-cluster sum-of-squares: that is the *inertia* of the K-Means clustering\n",
        "    - [Silhouette](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) - \"a higher Silhouette Coefficient score relates to a model with better defined clusters\"\n",
        "**Extrinsic evaluation** - If the ground truth is known (as in this case, since we have the categories) we can compute:\n",
        "    - [Homogeneity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score)\n",
        "    - [Completeness](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score)\n",
        "    - [V-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score)\n",
        "\n",
        "Let's first have a look at the *intrinsic measures* \n",
        "- print out within-cluster sum-of-squares and the silhouette coefficient:\n",
        "\n",
        "\n",
        "There are many possible metrics for clustering evaluation: some can be used when the ground truth labels are known, some can be used when the true labels are unknown.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OHNdyE2jx0U"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Intrinsic evaluation measures:\")\n",
        "print(\"Within-cluster sum-of-squares:\", str(kmeans.inertia_))\n",
        "print(\"Silhouette coefficient:\", str(metrics.silhouette_score(vector_documents, kmeans.labels_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvPcxgbVjx0U"
      },
      "source": [
        "The intrinsic evaluation measures are hard to interpret, since their values depend heavily on the difficulty of the clustering task, the amount of data, etc. \n",
        "- Moreover, the sum-of-squares value will improve monotonically as the number of clusters is increased.\n",
        "\n",
        "Extrinsic evaluation measures: \n",
        "- in this case we compare the labels assigned to the documents by the clustering algorithm with the ground truth assignments (newsgroups).\n",
        "- Since the clusters found could be (i) fewer or more than the number of classes, and (ii) won't be ordered the same as the labels, \n",
        "- a process of aligning the labels and clusters is performed internally for each of these metrics. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwik_NVejx0U"
      },
      "outputs": [],
      "source": [
        "print('Extrinsic evaluation measures:')\n",
        "print(\"Homogeneity:\", str(metrics.homogeneity_score(labels, kmeans.labels_)))\n",
        "print(\"Completeness:\", str(metrics.completeness_score(labels, kmeans.labels_)))\n",
        "print(\"V-measure:\", str(metrics.v_measure_score(labels, kmeans.labels_)))\n",
        "print(\"Adjusted Rand-Index:\", str(metrics.adjusted_rand_score(labels, kmeans.labels_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GyGuOf2jx0U"
      },
      "source": [
        "### Minibatch K-means clustering\n",
        "\n",
        "MiniBatch k-Means is an approximate verions of the k-Means that is designed to scale up to massive datasets by making use of small samples (minibatches) in order to find the k centroids.\n",
        "- It should be faster to run than k-Means. Let's give it a try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5B35GZjx0U"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "k = len(set(labels))\n",
        "\n",
        "mb_kmeans = MiniBatchKMeans(n_clusters=k,batch_size=500, random_state=2307)\n",
        "mb_kmeans.fit(vector_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMSEFw6_jx0U"
      },
      "source": [
        "Evaluate the minibatch clustering algorithm to see if the performance compares with the original k-Means on this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OURZzP7Fjx0V"
      },
      "outputs": [],
      "source": [
        "print(\"Intrinsic evaluation measures:\")\n",
        "print(\"Within-cluster sum-of-squares:\", str(mb_kmeans.inertia_))\n",
        "print(\"Silhouette coefficient:\", str(metrics.silhouette_score(vector_documents, mb_kmeans.labels_)))\n",
        "print(\"\\n\")\n",
        "print('Extrinsic evaluation measures:')\n",
        "print(\"Homogeneity:\", str(metrics.homogeneity_score(labels, mb_kmeans.labels_)))\n",
        "print(\"Completeness:\", str(metrics.completeness_score(labels, mb_kmeans.labels_)))\n",
        "print(\"V-measure:\", str(metrics.v_measure_score(labels, mb_kmeans.labels_)))\n",
        "print(\"Adjusted Rand-Index:\", str(metrics.adjusted_rand_score(labels, mb_kmeans.labels_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb3Zl5zFjx0V"
      },
      "source": [
        "### Selecting a value for k\n",
        "\n",
        "When used on low dimensional data, k-Means is often combined with the 'elbow method' (https://en.wikipedia.org/wiki/Elbow_method_(clustering)) for finding the 'right' number of clusters k\n",
        "\n",
        "The method involves:\n",
        "- running the clustering algorithm with increasing values of *k*\n",
        "- plotting the intrinsic evaluation measure (within-cluster sum-of-squares) \n",
        "- and looking for a point in which improvement in the measure decreases substantially from one time step to the next.\n",
        "\n",
        "Let's try out the method using the MiniBatch version of k-Means since it is a bit faster to run.\n",
        "- First generate the performance evaluation measure values across the range of k values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaKlfhpmjx0V"
      },
      "outputs": [],
      "source": [
        "performance = [MiniBatchKMeans(n_clusters=k, batch_size=500, random_state=2307).fit(vector_documents).inertia_ for k in range(1,50)]\n",
        "performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_sdECwjx0V"
      },
      "source": [
        "Note that the within-cluster sum-of-squares almost always improves from one iteration to the next as k is increased. \n",
        "- In theory it should always increase since the more cluster centroids there are, the more flexibility the model has for describing datapoints (assigning them to clusters) \n",
        "- but in practice the stochasticity of k-Means and also the mini-batch procedure can cause the algorithm to not find the global optimum and thus produce a higher sum-of-squares value than a previous iteration (with lower k).\n",
        "\n",
        "We'll now use some standard code to plot the performance measure against the value k:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpScHGinjx0V"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(performance)\n",
        "plt.ylabel('Within-cluster sum-of-squares')\n",
        "plt.xlabel('k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwjrozaDjx0V"
      },
      "source": [
        "Does it look to you like there is a point on the graph where performance suddenly stops getting a lot better? \n",
        "- I don't see one ...\n",
        "- That is likely because we have (i) very high dimensional data and (ii) quite a large number of documents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QRmUsD0dv3i"
      },
      "source": [
        "## Visualisation\n",
        "\n",
        "Often visualisng the samples and the model output is useful to understand what's going on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb2TKpfFjx0V"
      },
      "source": [
        "### Problems visualising high dimensional data\n",
        "\n",
        "For a bit of fun, we'll try to transform the high dimensional data into low dimensional data (just 3 dimensions) using a linear dimensionality reduction technique called Singular Value Decomposition. \n",
        "- We'll transform the `vector_documents` to be 3 dimensional. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEIdf8MEjx0V"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(3)\n",
        "reduced_data = svd.fit_transform(vector_documents)\n",
        "\n",
        "[x,y,z] = np.transpose(reduced_data)\n",
        "[x,y,z]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqvOAdXyjx0W"
      },
      "source": [
        "Now that we have the data in the right form, let's plot it with colours determined by the category it comes from to see if we can see structure in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChDejNOkjx0W"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=LabelEncoder().fit_transform(labels), marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVQnTndHjx0W"
      },
      "source": [
        "Can you see some structure in there?\n",
        "\n",
        "We can plot the same data coloured according to the clusters found by the original k-Means algorithm: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCdbZ9Anjx0W"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=kmeans.labels_, marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAEg1kpOjx0W"
      },
      "source": [
        "Do you think the clustering algorithm working correclty according to this visualisation? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1eoXy5jx0W"
      },
      "source": [
        "### Investigating effect of TF-IDF weighting\n",
        "\n",
        "It would be interesting to repeat the clustering using the CountVectorizer rather than the TfidfVectorizer to see if the resuting clusters are any less representative of the data. \n",
        "\n",
        "Give it a go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUR10aJgjx0W"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtlOnyn7i6Iw"
      },
      "source": [
        "## Topic Models\n",
        "\n",
        "We now investigate a different type of clustering of documents called Topic Modeling.\n",
        "- Unlike k-Means clustering where the cluster assignment are exclusive and each document can belong to one cluster only, in Topic Modeling each document belongs to many clusters at the same time. \n",
        "- Each document is represented as a mixture of those clusters (refered to as topics).\n",
        "- The most famous topic modeling technique is an algorithm called Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "To learn a topic model, we'll use the sklearn implementation of LDA and apply it to the same headline categories data that we used above.\n",
        "- This time though, we'll load the data into two subsets: a training set and a test set. \n",
        "- LDA learns a low dimensional representation of the documents. We will learn the LDA model on the training data only, so that later we can demonstrate how to transform new documents into the same document representation.\n",
        "- We will also used a slightly cleaner version of the newsgroup data this time by removing 'headers', 'footers' and quoted messages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWP--wK1jBFG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "documents_train, documents_test, label_train, label_test = train_test_split(documents, labels, test_size=0.4)\n",
        "print('No. training docs:', len(documents_train))\n",
        "print('No. testing docs:', len(documents_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8SmSf0ajMv7"
      },
      "source": [
        "We make use of the `CountVectorizer` rather than the TfidfVectorizer, since LDA is a probabilistic model for explaining term counts and thus expects counts rather than weights.\n",
        "- Let's set the `CountVectorizer` to remove stopwords and use the same the max/min document frequency settings as we had for k-Means clustering.\n",
        "- We'll use the `fit_transform()` method of the vectorizer to both fit the vocabulary and transform the data to the vector representation at the same time (performing both steps at once is a little more efficient than doing them separately). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-_xMc38YHlA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',min_df=5,max_df=.5)\n",
        "vector_documents_train = vectorizer.fit_transform(documents_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rltPoIdoIkw2"
      },
      "source": [
        "The vocabulary size is slightly smaller than before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4sosyFDYHlB"
      },
      "outputs": [],
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caN6mpYSYHlB"
      },
      "source": [
        "### Extracting topics using Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Now we'll fit an LDA topic model to the dataset. \n",
        "- Just like with k-Means where we needed to choose the number of clusters\n",
        "- with topic models we need (usually) to choose the number of topics to use to represent each document.\n",
        "- It's common to choose values up to 1000 for the number of topics.\n",
        "\n",
        "For the scope of this tutorual, we'll set the number of topics to just 30 (you can try larger values later): \n",
        "- Note that fiting the model will take a few minutes. \n",
        "- We use the 'online' learning method since there may be a bug in the 'batch' method: https://github.com/scikit-learn/scikit-learn/issues/6777"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p39ZzD9YHlB"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=30, verbose=1, learning_method='online', max_iter=30)\n",
        "lda.fit(vector_documents_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEdnc3GpNFLA"
      },
      "source": [
        "Once the model is learnt, we could measure the performance of the model in terms of its perplexity: \n",
        "- Perplexity measues how confused/surprise a model is at seeing new data, and is a commonly used measure of the prediction performance of a language model. \n",
        "- It computes the average number of bits required to encode the test data using the model. (Equivalently, it can be seen as 2 raised to the power of the cross-entropy. See: https://en.wikipedia.org/wiki/Perplexity)\n",
        "- We'll use the test dataset as the new data here, so we'll first need to vectorize it before computing the perplexity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuB8k02ePRCk"
      },
      "outputs": [],
      "source": [
        "vector_documents_test = vectorizer.transform(documents_test)\n",
        "lda.perplexity(vector_documents_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T08dj6uTYHlC"
      },
      "source": [
        "Let's now have a look at the topics found:\n",
        "- As we did for k-Means, we'll print out the 10 most important terms for each topic\n",
        "- The code below looks a little more complicated than it is:\n",
        "  - The for loop iterates over each topic in turn, sorting the terms for the topic from most important to least, \n",
        "  - it then grabs the vocabulary term for tht top 10 and concatenates them together before printing them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDnbmh-iYHlC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range(len(lda.components_)):\n",
        "    sorted_terms = lda.components_[i].argsort()[::-1]\n",
        "    concatenated_terms = '[' + ', '.join(vocab[i] for i in sorted_terms[:10]) + ']'\n",
        "    print (f'Topic {i + 1}:\\t', concatenated_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr39VfxyYHlC"
      },
      "source": [
        "What do you think about the quality of the topics? \n",
        "- Do they look coherent to you?\n",
        "- Note that the topic model doesn't make any use of the characters in the words, so 'gun' and 'guns' are related only because they occur in similar contexts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPxBTunCL98E"
      },
      "source": [
        "We could visualise the prevalence of the words in each topics using some code adapted from: \n",
        "- https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
        "- Note that the code at the link above also compares LDA with Non-negative Matrix Factorization (NMF) applied to tf-idf matrices, which is an alternative non-probabilistic approach to developing topic models with comparable performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92NCOicONWpb"
      },
      "outputs": [],
      "source": [
        "num_words = 10\n",
        "cols = 5\n",
        "rows = int(len(lda.components_)/5)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 4 * rows), sharex=True)\n",
        "axes = axes.flatten()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_terms_index = topic.argsort()[:-num_words - 1:-1]\n",
        "    top_terms = [vocab[i] for i in top_terms_index]\n",
        "    weights = topic[top_terms_index]\n",
        "    ax = axes[topic_idx]\n",
        "    ax.barh(top_terms, weights, height=0.7)\n",
        "    ax.set_title(f'Topic {topic_idx +1}',fontdict={'fontsize': 15})\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xscale('log')\n",
        "\n",
        "plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYocTAGYHlC"
      },
      "source": [
        "### Investigating the topic description for a single document\n",
        "\n",
        "The interesting aspect of topic modeling with respect to normal document clustering is that each document is represented by a vector of topics, rather than a single cluster. Thus the topic representation can be useful for: \n",
        "- describing the document content, and \n",
        "- calculating similarities/distances between documents that takes synonymy (different term, same meaning) and polysemy (same term, different meaning) into account. \n",
        "\n",
        "Let's have a look at that description for a particular document:\n",
        "- To make things more interesting, we'll select the document from the test set that wasn't used to train the LDA model. \n",
        "- We'll take the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM2tOSZpYHlD"
      },
      "outputs": [],
      "source": [
        "doc = documents_train[6033]\n",
        "\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS1SxRykYHlD"
      },
      "source": [
        "Now to see its topic vector we need to vectorize and then transform (using LDA) the document to the topic space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2FS7rV9YHlD"
      },
      "outputs": [],
      "source": [
        "doc_vec = vectorizer.transform([doc])[0]\n",
        "topic_vec = lda.transform(doc_vec)[0]\n",
        "topic_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzPqMSNjYHlD"
      },
      "source": [
        "Well there we have it: the topic vector representation of the document above. \n",
        "- Notice that many topics have the same small same value associated with them. \n",
        "- That just means that no words from that topic were found in the document. (A topic model is a generative statistical model, so it can't allow probabilities to actually be zero.)\n",
        "\n",
        "Let's now generate a more interpretable representation of the document by \n",
        "- getting rid of the low frequency topics (less than 1% prevalence) and \n",
        "- listing the common words for the topic.\n",
        "\n",
        "To do this, first sort the topics from most important to least (both the index and the value): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEbKg-2DYHlE"
      },
      "outputs": [],
      "source": [
        "sorted_topics = topic_vec.argsort()[::-1]\n",
        "sorted_prevalence = sorted(topic_vec)[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUL5XjjHYHlE"
      },
      "source": [
        "Now iterate over the top 10 topics, disregarding those with frequency less than 1%, and print out the prevelance and the top terms for each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0b7AZybYHlE"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    if sorted_prevalence[i] < .01: break\n",
        "    topic = sorted_topics[i]\n",
        "    sorted_terms = np.flip(lda.components_[topic].argsort())\n",
        "    print (f'{100 * sorted_prevalence[i]:.1f}% Topic {topic}: {\" \".join(vocab[i] for i in sorted_terms[:10])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQvPtn8YHlE"
      },
      "source": [
        "What do you think? Is it a good description? \n",
        "- Do the set of topics agree with the content of the document?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65L0uOblYHlF"
      },
      "source": [
        "### Visualising Topic Representations of Documents with t-SNE\n",
        "\n",
        "We tried to visualise the extremely high dimensional tfidf bag-of-words data in the last session using Singular Value Decomposition (SVD) and it didn't work very well at all. \n",
        "- The dimension of the data is MUCH smaller now, since there are very few topics (30) compared to the size of the vocabulary (\\~10,000).\n",
        "- Thus it should be much easier to find a 3 dimensional represenation of the data.\n",
        "\n",
        "This time we'll use a different dimensionality reduction technique called t-SNE: \n",
        "- t-SNE stands for t-distributed Stochastic Neighbor Embedding.\n",
        "- It is a non-linear technique that, unlike SVD, acts to preserve the relative distances between *nearby objects* (similar documents in the original space) when building the mapping. \n",
        "- (SVD is a linear technique that acts instead to preserve relative distances between the most distant objects in the space.) \n",
        "\n",
        "We'll run t-SNE on the test data set that wasn't used to estimate the LDA model, and create an 3-dimensional embedding from the high-dimensional data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MEShkwJYHlG"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne_embedding = TSNE(n_components=3).fit_transform(lda.transform(vector_documents_test))\n",
        "tsne_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCG7QCcsYHlG"
      },
      "source": [
        "Convert the data to the (x,y,z) format needed for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJEDTCLqYHlG"
      },
      "outputs": [],
      "source": [
        "[x, y, z] = np.transpose(tsne_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntsrd0o_YHlG"
      },
      "source": [
        "And plot it in a 3d scatter plot with the ground-truth newsgroup labels used to colour the datapoints:\n",
        "- Note: If you are using Jupyter Notebook rather than Google Colab to run this notebook, you can uncomment the first line ('%matplotlib notebook') below to make the 3d plot interactive. Don't do that if you are using colab as it will prevent you form seeing the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ibx1psjYHlG"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=LabelEncoder().fit_transform(label_test), marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8HgMyzmYHlG"
      },
      "source": [
        "Seems like there might be a little more structure there than last time, but still quite hard to tell. \n",
        "\n",
        "Instead of colouring based on the newsgroups, we can assign each document to its most frequent topic and colour based on the topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M__Z91w8YHlH"
      },
      "outputs": [],
      "source": [
        "freq_topic = [topic_vec.argsort()[-1] for topic_vec in lda.transform(vector_documents_test)]\n",
        "\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=freq_topic, marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNQg_XGcwDCO"
      },
      "source": [
        "What if we want to see it in 2D?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iZYbRKpJlfT"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
