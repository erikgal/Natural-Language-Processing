{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giKsby3Lpdlp"
   },
   "source": [
    "# Sequence Labelling and Classification\n",
    "\n",
    "In this session we'll first investigate Part-of-Speech (POS) tagging and Named-entity recognition (NER). \n",
    "- For this we will make use of the spaCy natural langauge processing API: https://spacy.io/\n",
    "- spaCy is an opensource API that provides state-of-the-art performance on sequence labeling tasks such as POS tagging and NER. \n",
    "- Parts of this tutorial are based on code from: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "In the second part of the tutorial we will train a Text classifier that makes use of a Bidirectional LSTM (Long Short-term Memory) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSNGW-JpWqVO"
   },
   "source": [
    "Before starting I need to connect the drive storage to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKB0LWYCWiku",
    "outputId": "3e835e2b-ca90-4729-97be-ed4ff0673af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "80PFRRJ5XsLD",
    "outputId": "eb39a7d5-1fc5-4362-e3fd-54e9aee25e7a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Colab Notebooks/NLP'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/NLP')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OgRPEb6pdlv"
   },
   "source": [
    "## Installing spaCy and downloading models\n",
    "\n",
    "First we need to check whether the spaCy library is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3083U81cpdlv"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ1xLIcHpdlw"
   },
   "source": [
    "Then we need to download pretrained models for use with spaCy. We will load models for both English and Italian:\n",
    "- The models are called 'en_core_web_sm' and 'it_core_news_sm', where the 'web'/'news' indicates what type of collection the model was trained on and the 'sm' at the end indicates that we are using the 'small' version of the models\n",
    "- Other models are available here: https://spacy.io/models\n",
    "- The following code calls the python executable instructing it to run the module 'spacy', which in turn download the models. See discussion here: https://stackoverflow.com/questions/46148033/unable-to-load-en-from-spacy-in-jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyv-NpFbpdlw"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "!{sys.executable} -m spacy download it_core_news_sm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYRlbtvspdlx"
   },
   "source": [
    "We are now ready to import spacy and load a model. Let's start with the English model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KHrsza0pdlx"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp_model = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy4K8dRApdlx"
   },
   "source": [
    "Consider the following piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MGjLJU0pdlx"
   },
   "outputs": [],
   "source": [
    "text = 'Melbourne is to re-enter Stage 3 lockdown after a record increase in cases. Victorian state premier Daniel Andrews said there was “simply no alternative” to reimposing stay at home restrictions in Australia’s second-biggest city.'\n",
    "text = \"Good evening, London. Allow me first to apologize for this interruption. I do, like many of you, appreciate the comforts of everyday routine, the security of the familiar, the tranquillity of repetition. I enjoy them as much as any bloke. But in the spirit of commemoration, whereby those important events of the past, usually associated with someone's death or the end of some awful bloody struggle, are celebrated with a nice holiday, I thought we could mark this November the fifth, a day that is sadly no longer remembered, by taking some time out of our daily lives to sit down and have a little chat.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_rCI0Hbpdly"
   },
   "source": [
    "Parse the text using the NLP engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmpvXvjapdly"
   },
   "outputs": [],
   "source": [
    "parsed_text = nlp_model(text)\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnXyGOx0pdlz"
   },
   "source": [
    "Did it do something? It looks like it has just output the same text.\n",
    "- Actually, yes. It has parsed the input and built its internal datastructure from it. \n",
    "- Note that the length of the parsed object is in words, not characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbAOI2Mnpdlz"
   },
   "outputs": [],
   "source": [
    "print(f'The length of the original text is {len(text)} chacacters')\n",
    "print(f'The length of the parsed text is {len(parsed_text)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xImJ-i_Lpdlz"
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "While parsing the text, spaCy performs part-of-speech (POS) tagging. \n",
    "- We can see the POS tag for each token as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho4HEJTLpdlz"
   },
   "outputs": [],
   "source": [
    "[(w,w.pos_) for w in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFEzvleepdlz"
   },
   "source": [
    "Who remembers their grammar from high school? What do all those POS symbols mean?\n",
    "- You can find an explanation of the POS tags on this website https://spacy.io/api/annotation in the section \"Universal Part-of-speech Tags\" \n",
    "\n",
    "What can we do with POS tags? \n",
    "- Well, we could select all terms that have a certain tag, such as all adjectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kcjc28lpdl0"
   },
   "outputs": [],
   "source": [
    "set(w for w in parsed_text if w.pos_=='ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftw5HEYEpdl0"
   },
   "source": [
    "That was a little underwhelming. \n",
    "- Let's try it on Alice in Wonderland chapter 1 text. (You'll need to upload it again to Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGRt3HF8pdl0"
   },
   "outputs": [],
   "source": [
    "adjectives = sorted(set(w.text for w in nlp_model(open(\"docs/Alice_Chapter1.txt\", \"r\").read()) if w.pos_=='ADJ'))\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pct1jNdEpdl0"
   },
   "source": [
    "You can see how descriptive a writer Lewis Carroll was! \n",
    "\n",
    "This leads us to one explanation as to why we might want to extract POS tags from text: \n",
    "- They can sometimes be useful for **extracting features** (often handcrafted ones) for certain text classification tasks (such as authorship identification).\n",
    "- This is particularly the case if only a small amount of training data is available.  \n",
    "- For example, in this article (https://towardsdatascience.com/automatically-detect-covid-19-misinformation-f7ceca1dc1c7) hand-crafted features are extracted for classifying covid misinformation. \n",
    "\n",
    "Another reason why we might consider POS tagging is to **reduce ambiguity** in our bag-of-words representation by appending POS tags to word occurrences. \n",
    "- Consider the following two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iO992Q-pdl0"
   },
   "outputs": [],
   "source": [
    "ex1 = 'I catch the train to and from work.'       # This is Prof. Mark Carman speaking\n",
    "ex2 = 'I like to train at least 6 times a week.'  # This is Prof. Jacked Carman speaking\n",
    "\n",
    "print(ex1, '     <-- \\'train\\' is a', nlp_model(ex1)[3].pos_)\n",
    "print(ex2, '<-- \\'train\\' is a', nlp_model(ex2)[3].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHlqof06pdl1"
   },
   "source": [
    "The second sentence has nothing to do with trains, despite containing the word 'train'!\n",
    "- We could deal with this issue by appending the POS tag to the observed literal to form vocabulary elements: train_NOUN, train_VERB\n",
    "\n",
    "A final reason why we might think about running POS tagging would be to extract proper nouns from the text, since they refer to real entities that are being discussed in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kW4TbHffpdl1"
   },
   "outputs": [],
   "source": [
    "[w.text for w in parsed_text if w.pos_=='PROPN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDl1pPoSpdl1"
   },
   "source": [
    "Shortly though, we will talk about Entity-extraction, which is the task of identifying and categorising the entities discussed in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDh0iS65pdl1"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "While parsing, spaCy also performs lemmatization. \n",
    "- Lemmatization is the process of extracting the 'lemma' for each token, which is the canonical form of the word that would be found in the dictionary, (see https://en.wikipedia.org/wiki/Lemma_(morphology))\n",
    "- Basically, verbs converted to their root form, e.g.: **went, going, goes, gone => go**\n",
    "- And nouns are retuned to singular form: **kittens => kitten**\n",
    "- Lemmatization is a more complicated POS-aware process than stemming (https://en.wikipedia.org/wiki/Stemming). Stemmers simply apply a set of language-specific syntax rules to recover the stem of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EevBrQRbpdl1"
   },
   "outputs": [],
   "source": [
    "[(x, x.lemma_) for x in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEIoq1RFpdl1"
   },
   "source": [
    "Why would one want to perfom lemmatization? -- Or stemming for that matter?\n",
    "- to **reduce the vocabulary size** and thereby generalise the representation. -- This used to be very important for improving performance of search engine performance (better similarity measures between documents) and also classifiers on small datasets, (before word embeddings came along).\n",
    "- to **look-up information** about the word in a dictionary/ontology, such as WordNet (https://en.wikipedia.org/wiki/WordNet). This used to be an important way to compute semantic similarity between words, but again, word embeddngs probably do a better job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vBQ7f0tpdl2"
   },
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Tradititonally in Natural Language Processing, text is processed in a pipeline that first tokenizes, then POS tags, lemmatizes and finaly dependency parses a piece of text. \n",
    "- The idea with dependency parsing is to determine what function each of the word instances is fulfilling in the sentence. \n",
    "- What is the subject and object of the sentence? \n",
    "- Which noun is each adjective referring to?\n",
    "\n",
    "So while parsing the text, the spaCy model also generates a **dependency parse tree**, which can be displayed using 'displacy':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dbz57hc5pdl2"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(parsed_text, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quwWOjbOpdl3"
   },
   "source": [
    "Such dependency trees are interesting for understanding and visualising language (particularly for linguists) and could possibly be used for some downstream tasks (say checking ambiguity in legal documents).  \n",
    "\n",
    "Consider the sentences:\n",
    "- *The girl saw a man carrying a telescope.*\n",
    "- *The girl saw a man with a telescope.*\n",
    "\n",
    "Who had the telescope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AxJLNswpdl3"
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp_model('The girl saw a man carrying a telescope.'),jupyter=True,style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abgGOdwpdl3"
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp_model('The girl saw a man with a telescope.'),jupyter=True,style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fIZq6zIhPbk"
   },
   "source": [
    "The second sentence is ambiguous: The girl may have made use of her telescope or the man may have been using the girl's telescope...\n",
    "- Language is full of such ambiguities which we as humans naturally deal with using our prior knowledge and abilty to construct mental models of the situations described.\n",
    "- This process is not without its biases:\n",
    "  - *The doctor went over to talk to the nurse. She told him that she had just given the patient 5mg of Vicodin and the child had started convulsing. He listened attentively as she explained what had happened. The doctor was worried that the patient should not be given any more painkillers. The nurse told the doctor not to worry, that the patient was in good hands, and that he would let her know immediately if the child's condition changed.*\n",
    "  - What gender are the doctor and the nurse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uE1efbqpdl3"
   },
   "source": [
    "## Extracting Entities\n",
    "\n",
    "A more important output than the depency parse, from a text mining perspective, is the list of named-entities present in the text\n",
    "- **named entities** are objects in the real world, e.g. persons, products, organizations, locations, etc. \n",
    "  - see https://en.wikipedia.org/wiki/Named_entity\n",
    "- if spacy has found any named entities while parsing the text, we can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irYwH5WVpdl3"
   },
   "outputs": [],
   "source": [
    "parsed_text.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv6krbXmpdl3"
   },
   "source": [
    "Note that the entities are not single word tokens but short sequences of words: 'Stage 3' and 'Daniel Andrews'.\n",
    "\n",
    "Not only does spacy extract the entities, but also categorises them based on their type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXA2FRzXpdl4"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in parsed_text.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyp09Uelpdl4"
   },
   "source": [
    "The city and country locations have been labeled 'GPE' for 'geopolitical entity', while the Premier of Victoria has been correctly identified as a person. \n",
    "- Here is the list of all entity types that spaCy looks for: https://spacy.io/api/annotation#section-named-entities\n",
    "\n",
    "Internally, the output of the Named Entity Recogniser is a sequence annotated with entities using inside-outside-beginning encoding: \n",
    "- see https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "- We can print out this labeling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E1H-RM6pdl4"
   },
   "outputs": [],
   "source": [
    "[(X, X.ent_iob_, X.ent_type_) for X in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tU8n44pdl4"
   },
   "source": [
    "The above format is a bit hard to read though, so spaCy also provides a far more natural visualisation of the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PmUl2y5pdl4"
   },
   "outputs": [],
   "source": [
    "displacy.render(parsed_text, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS1PK7ifpdl5"
   },
   "source": [
    "## Extracting entities from a web document\n",
    "\n",
    "Now that we know how to perform entity recognition on text, let's apply it to a full document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pY-nN0Zpdl5"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news/world-latin-america-53319517'\n",
    "#url = 'https://en.wikipedia.org/wiki/Apple_(disambiguation)'\n",
    "\n",
    "import requests\n",
    "html_doc = requests.get(url).text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "parsed_doc = BeautifulSoup(html_doc, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRe5AUvCpdl5"
   },
   "source": [
    "Now lets extract the title and paragraph text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5YGoQISpdl5"
   },
   "outputs": [],
   "source": [
    "title = parsed_doc.find('title').text\n",
    "paragraphs = [p.text for p in parsed_doc.find_all('p')]\n",
    "\n",
    "# Combine the title and paragraphs into a single text:\n",
    "article_text = title + '\\n\\n' + '\\n'.join(paragraphs)\n",
    "print(article_text)\n",
    "\n",
    "#article_text = parsed_doc.get_text()\n",
    "#print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_dqist0pdl5"
   },
   "source": [
    "---\n",
    "\n",
    "Parse the article to identify the entities and display them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMTsfAhYpdl6"
   },
   "outputs": [],
   "source": [
    "parsed_article = nlp_model(article_text)\n",
    "displacy.render(parsed_article,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I8_zJQrpdl6"
   },
   "source": [
    "---\n",
    "\n",
    "What do you think? Did it work?\n",
    "\n",
    "Let's have a bit of a better look at the entities found\n",
    "- List all the distinct entities found in the article, sorted alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SakcaXuypdl6"
   },
   "outputs": [],
   "source": [
    "sorted(set(x.text for x in parsed_article.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyGeLXs8pdl6"
   },
   "source": [
    "We can count the number of times each **entity type** occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcmsbiUkpdl6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [x.label_ for x in parsed_article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb3g9y5Kpdl6"
   },
   "source": [
    "We can also count the number of times each **entity name** occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPdOsoiApdl6"
   },
   "outputs": [],
   "source": [
    "items = [x.text for x in parsed_article.ents]\n",
    "Counter(items).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEV2BJCTpdl7"
   },
   "source": [
    "Note that some of the phrases refer to the same entity, e.g. 'Mr Bolsonaro' and just 'Bolsonaro'.\n",
    "- Entity Linking and Reference Resolution are the NLP problems that deal with resolving the different references to the same entity in the text.\n",
    "\n",
    "If we were only interested in what was being said about Bolsonaro, \n",
    "- we could select only sentences refering to him:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNXjt46Jpdl7"
   },
   "outputs": [],
   "source": [
    "sentences_containing_Bolsonaro = [x for x in parsed_article.sents if 'Bolsonaro' in x.text]\n",
    "displacy.render(sentences_containing_Bolsonaro,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkJvASO9pdl7"
   },
   "source": [
    "## Named Entity Extraction in Italian\n",
    "\n",
    "But wait, SpaCy can speak Italian too!\n",
    "- Let's make use of the pretrained italian model that we downloaded earlier: https://spacy.io/models/it\n",
    "- to recognise entities in an article from 'Il Corriere'\n",
    "\n",
    "First download the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCTkJTsopdl7"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.ansa.it/sito/notizie/mondo/2020/07/07/bolsonaro-ha-i-sintomi-del-coronavirus_40d26967-e377-4455-9b42-83c2756cf5f1.html'\n",
    "html_doc = requests.get(url).text\n",
    "parsed_doc = BeautifulSoup(html_doc, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrnqQcMOpdl7"
   },
   "source": [
    "Now let's extract the title and paragraph text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIyIysghpdl7"
   },
   "outputs": [],
   "source": [
    "title = parsed_doc.find('title').text\n",
    "paragraphs = [p.text for p in parsed_doc.find_all('p')]\n",
    "\n",
    "# Combine the title and paragraphs into a single text:\n",
    "article_text = title + '\\n\\n' + '\\n'.join(paragraphs)\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eluDQCqxpdl7"
   },
   "source": [
    "---\n",
    "\n",
    "Now we'll parse the text of the article with an Italian NLP engine to extract Named Entities.\n",
    "- First load the italian model 'it_core_news_sm' that we downloaded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj0IIHlVpdl7"
   },
   "outputs": [],
   "source": [
    "import it_core_news_sm\n",
    "nlp_it = it_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcWwDGlypdl8"
   },
   "source": [
    "Parse article and extract the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X-Kuw6lpdl8"
   },
   "outputs": [],
   "source": [
    "parsed_article = nlp_it(article_text)\n",
    "displacy.render(parsed_article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08h69wcxpdl8"
   },
   "source": [
    "That looks not great. \n",
    "- Here are the entities found in the news article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKefi_fspdl8"
   },
   "outputs": [],
   "source": [
    "sorted(set(x.text for x in parsed_article.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GNZaLqN4Dx-"
   },
   "source": [
    "Alterantively you can use Stanza (https://stanfordnlp.github.io/stanza/). It's very similar to spaCy:\n",
    "- Python package\n",
    "- Supports multiple languages\n",
    "- Uses deep neural network modules\n",
    "\n",
    "Let's start installing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6JmKZ3xLPwI"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazOsrqKLgtW"
   },
   "source": [
    "Now we can import Stanza and create a pipeline for Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2vbIhiQLfhH"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "stanza_nlp_model = stanza.Pipeline(lang='it', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN8vExE_MWGK"
   },
   "source": [
    "As before we need to parse the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHqxZTBALfm7"
   },
   "outputs": [],
   "source": [
    "stanza_parsed_article = stanza_nlp_model(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHx67HKdMtNi"
   },
   "source": [
    "Given a document, Stanza breaks it into sentences and then tokens.\n",
    "For each token adds the tags using the sleected processors (here we are using only the NER processors).\n",
    "\n",
    "Let's give a look at the identified entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw98dV1yLfsu"
   },
   "outputs": [],
   "source": [
    "for sentence in stanza_parsed_article.sentences:\n",
    "    for entity in sentence.ents:  # Hello, Treebeard!\n",
    "        print(f\"{entity.text}: {entity.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LDtG1YcN__S"
   },
   "source": [
    "That's a bit better than before, don't you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzFM8cAUpdl8"
   },
   "source": [
    "## Fine-tuning your own NER Model\n",
    "\n",
    "What if you want to update the Named Entity Extraction model yourself in order to customize it to your set of entities? We'll have a look at that now based on:\n",
    "- this instructions page: https://spacy.io/usage/training#ner\n",
    "- and this blog post: https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "\n",
    "In order to fine-tune the model, we need to prepare data in the following format: \n",
    "- a piece of text, \n",
    "- plus a list of entity types that occur it along with their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8aaj2uepdl8"
   },
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    (\"Have you heard of an associate professor from the Politecnico di Milano called Mark Carman?\", {\"entities\": [(50, 71, \"ORG\"),(79, 90, \"PERSON\")]}),\n",
    "    (\"No, I haven't, but I don't know many people at the Politecnico. What does he work on?\", {\"entities\": [(51, 62, \"ORG\")]}),\n",
    "    (\"Mainly machine learning and text mining. I met him a couple of years ago at SIGIR in Tokyo.\", {\"entities\": [(76, 81, \"EVENT\"),(85, 90, \"GPE\")]}),\n",
    "]\n",
    "my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnVIriSEpdl9"
   },
   "source": [
    "Where would this data come from? \n",
    "- either created manually, perhaps by searching for known individuals in a text collection,\n",
    "- or by using an annotation tool such as https://doccano.herokuapp.com/, see for example: https://medium.com/@justindavies/training-spacy-ner-models-with-doccano-8d8203e29bfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s9T3Whe5lZJ"
   },
   "source": [
    "The following code comes from here: https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py\n",
    "- The only change made was to remove the training data\n",
    "\n",
    "Before starting we need to install the plac package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P057B5jUQjqM"
   },
   "outputs": [],
   "source": [
    "!pip install plac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-oMt4jRQj2k"
   },
   "source": [
    "Now we define function with the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVjhWJ1Zpdl9"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1pM_VOIpdl9"
   },
   "source": [
    "Once the above model has been defined, we can update and save the model\n",
    "- Note that this code doesn't currently run in Google colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1cpkZGWpdl9"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA=my_data  # The method expects the training data to have this name\n",
    "main(model='en_core_web_sm',output_dir='spacy_model',n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlicBTgepdl-"
   },
   "source": [
    "## Entity Linking in spaCy\n",
    "\n",
    "We don't want to just find entity mentions in a document but link them to a known entity in a knowledge base. \n",
    "- The task of linking the entity mentions to the corresponding entity in the knowledge base is called 'Entity Linking'.\n",
    "- I don't have time here, but watch this video to learn more: \n",
    "https://spacy.io/universe/project/video-spacy-irl-entity-linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BKB-yMKpdl-"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7Yi8kv_SPL8"
   },
   "source": [
    "## Sequence labelling with embeddings and a Recurrent Neural Network\n",
    "\n",
    "In this section of the notebook I will run through an example of using LSTM (Long Short-term Memory) network for text sequence labelling.\n",
    "We can train our own model for POS-tagging or NER.\n",
    "Moreover, we can use pre-trained embedding models to encode the input text.\n",
    "\n",
    "- We are going to use PyTorch (https://pytorch.org) to build and train our model. Pytorch is a state-of-the-art framework for deep leaning (much better than TensorFlow IMHO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TOBlQWzTeZv"
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "As usula we start from data preparation. \n",
    "We can use the [WikiNER](https://www.sciencedirect.com/science/article/pii/S0004370212000276) corpus, which provides corpora for POS-tagging and NER in multiple languages.\n",
    "Today we are going to focus on NER.\n",
    "\n",
    "You can find a copy the English split in the `doc/` directory. \n",
    "All the parts of the corpus are avaialble here: https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500\n",
    "\n",
    "Let's start by loading the file in memory and reading it line by line. Each line corresponds to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwzHtg6oTn8h",
    "outputId": "bac2c2ef-8c00-45cd-c568-d2b707c18733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The|DT|I-MISC Oxford|NNP|I-MISC Companion|NNP|I-MISC to|TO|I-MISC Philosophy|NNP|I-MISC says|VBZ|O ,|,|O \"|LQU|O there|EX|O is|VBZ|O no|DT|O single|JJ|O defining|VBG|O position|NN|O that|IN|O all|DT|O anarchists|NNS|O hold|VBP|O ,|,|O and|CC|O those|DT|O considered|VBN|O anarchists|NNS|O at|IN|O best|JJS|O share|NN|O a|DT|O certain|JJ|O family|NN|O resemblance|NN|O .|.|O \"|RQU|O',\n",
       " 'In|IN|O the|DT|O end|NN|O ,|,|O for|IN|O anarchist|JJ|O historian|JJ|O Daniel|NNP|I-PER Guerin|NNP|I-PER \"|LQU|O Some|DT|O anarchists|NNS|O are|VBP|O more|RBR|O individualistic|JJ|O than|IN|O social|JJ|O ,|,|O some|DT|O more|JJR|O social|JJ|O than|IN|O individualistic|JJ|O .|.|O',\n",
       " 'From|IN|O this|DT|O climate|NN|O William|NNP|I-PER Godwin|NNP|I-PER developed|VBD|O what|WP|O many|NN|O consider|VBP|O the|DT|O first|JJ|O expression|NN|O of|IN|O modern|JJ|O anarchist|NN|O thought|NN|O .|.|O',\n",
       " 'Godwin|NNP|I-PER was|VBD|O ,|,|O according|VBG|O to|TO|O Peter|NNP|I-PER Kropotkin|NNP|I-PER ,|,|O \"|LQU|O the|DT|O first|JJ|O to|TO|O formulate|VB|O the|DT|O political|JJ|O and|CC|O economical|JJ|O conceptions|NNS|O of|IN|O anarchism|NN|O ,|,|O even|RB|O though|IN|O he|PRP|O did|VBD|O not|RB|O give|VB|O that|DT|O name|NN|O to|TO|O the|DT|O ideas|NNS|O developed|VBN|O in|IN|O his|PRP$|O work|NN|O \"|RQU|O ,|,|O while|IN|O Godwin|NNP|I-PER attached|VBD|O his|PRP$|O anarchist|NN|O ideas|NNS|O to|TO|O an|DT|O early|JJ|O Edmund|NNP|I-PER Burke|NNP|I-PER .|.|O',\n",
       " 'The|DT|O first|JJ|O to|TO|O describe|VB|O himself|PRP|O as|IN|O an|DT|O anarchist|NN|O was|VBD|O Pierre-Joseph|JJ|I-PER Proudhon|NNP|I-PER ,|,|O a|DT|O French|JJ|I-MISC philosopher|NN|O and|CC|O politician|NN|O ,|,|O which|WDT|O led|VBD|O some|DT|O to|TO|O call|VB|O him|PRP|O the|DT|O founder|NN|O of|IN|O modern|JJ|O anarchist|NN|O theory|NN|O .|.|O',\n",
       " 'Its|PRP$|O classical|JJ|O period|NN|O ,|,|O which|WDT|O scholars|NNS|O demarcate|VBP|O as|IN|O from|IN|O 1860|CD|O to|TO|O 1939|CD|O ,|,|O is|VBZ|O associated|VBN|O with|IN|O the|DT|O working-class|JJ|O movements|NNS|O of|IN|O the|DT|O nineteenth|JJ|O century|NN|O and|CC|O the|DT|O Spanish|NNP|I-MISC Civil|NNP|I-MISC War-era|NNP|I-MISC struggles|VBZ|O against|IN|O fascism|NN|O .|.|O',\n",
       " \"Due|JJ|O to|TO|O its|PRP$|O links|NNS|O to|TO|O active|JJ|O workers|NNS|O '|POS|O movements|NNS|O ,|,|O the|DT|I-MISC International|NNP|I-MISC became|VBD|O a|DT|O significant|JJ|O organization|NN|O .|.|O\",\n",
       " 'Karl|NNP|I-PER Marx|NNP|I-PER became|VBD|O a|DT|O leading|JJ|O figure|NN|O in|IN|O the|DT|I-MISC International|NNP|I-MISC and|CC|O a|DT|O member|NN|O of|IN|O its|PRP$|O General|NNP|I-ORG Council|NNP|I-ORG .|.|O',\n",
       " \"Proudhon|NNP|I-PER 's|POS|O followers|NNS|O ,|,|O the|DT|O mutualists|NNS|O ,|,|O opposed|VBD|O Marx|NNP|I-PER 's|POS|O state|NN|O socialism|NN|O ,|,|O advocating|VBG|O political|JJ|O abstentionism|NN|O and|CC|O small|JJ|O property|NN|O holdings|NNS|O .|.|O\",\n",
       " 'They|PRP|O allied|VBD|O themselves|PRP|O with|IN|O the|DT|O federalist|JJ|O socialist|JJ|O sections|NNS|O of|IN|O the|DT|I-MISC International|NNP|I-MISC ,|,|O who|WP|O advocated|VBD|O the|DT|O revolutionary|JJ|O overthrow|NN|O of|IN|O the|DT|O state|NN|O and|CC|O the|DT|O collectivization|NN|O of|IN|O property|NN|O .|.|O']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('docs/aijwikinerenwp3') as f:\n",
    "    wiki_ner_data = f.read().strip().split('\\n')\n",
    "wiki_ner_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pKqvTd1WYuZ"
   },
   "source": [
    "Now we can parse the data. \n",
    "Tokens are separated by spaces and for each token we have the associated POS tag and the NER tag written as: `<token>|<POS tag>|<NER tag>`.\n",
    "\n",
    "Here NER tags are written using a system called BIO-tagging.\n",
    "The 'B' stands for \"begin\" and introduces (starts) a new named entity, the tags are written as \"B-PER\" to indicate a person or \"B-LOC\" to indicate a location and so on. \n",
    "The 'I' stands for \"inside\" and continues a started named entity, the tags are written as \"I-PER\" to indicate a person or \"I-LOC\" to indicate a location and so on. \n",
    "The 'O' stands for outside, it means that the token is outside any named entity. \n",
    "There are other tagging systems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGHVXha_ToEl",
    "outputId": "3d5cb8be-bf9c-420b-d0ad-d1178571e7af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The', 'pos_tag': 'DT', 'ner_tag': 'I-MISC'},\n",
       " {'text': 'Oxford', 'pos_tag': 'NNP', 'ner_tag': 'I-MISC'},\n",
       " {'text': 'Companion', 'pos_tag': 'NNP', 'ner_tag': 'I-MISC'},\n",
       " {'text': 'to', 'pos_tag': 'TO', 'ner_tag': 'I-MISC'},\n",
       " {'text': 'Philosophy', 'pos_tag': 'NNP', 'ner_tag': 'I-MISC'},\n",
       " {'text': 'says', 'pos_tag': 'VBZ', 'ner_tag': 'O'},\n",
       " {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       " {'text': '\"', 'pos_tag': 'LQU', 'ner_tag': 'O'},\n",
       " {'text': 'there', 'pos_tag': 'EX', 'ner_tag': 'O'},\n",
       " {'text': 'is', 'pos_tag': 'VBZ', 'ner_tag': 'O'},\n",
       " {'text': 'no', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       " {'text': 'single', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       " {'text': 'defining', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       " {'text': 'position', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       " {'text': 'that', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       " {'text': 'all', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       " {'text': 'anarchists', 'pos_tag': 'NNS', 'ner_tag': 'O'},\n",
       " {'text': 'hold', 'pos_tag': 'VBP', 'ner_tag': 'O'},\n",
       " {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       " {'text': 'and', 'pos_tag': 'CC', 'ner_tag': 'O'},\n",
       " {'text': 'those', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       " {'text': 'considered', 'pos_tag': 'VBN', 'ner_tag': 'O'},\n",
       " {'text': 'anarchists', 'pos_tag': 'NNS', 'ner_tag': 'O'},\n",
       " {'text': 'at', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       " {'text': 'best', 'pos_tag': 'JJS', 'ner_tag': 'O'},\n",
       " {'text': 'share', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       " {'text': 'a', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       " {'text': 'certain', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       " {'text': 'family', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       " {'text': 'resemblance', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       " {'text': '.', 'pos_tag': '.', 'ner_tag': 'O'},\n",
       " {'text': '\"', 'pos_tag': 'RQU', 'ner_tag': 'O'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['text', 'pos_tag', 'ner_tag']\n",
    "wiki_ner_data = [[dict(zip(keys, token.split('|'))) for token in sentence.split()] for sentence in wiki_ner_data]\n",
    "\n",
    "wiki_ner_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jFlDXEXYHYe"
   },
   "source": [
    "Now all the labels are properly organised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ1C75ufYHm3"
   },
   "source": [
    "At this point we need a system ti encode and decode the labels into categorical entities. \n",
    "We can use the label encoder from Scikit-Learn for that (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gtsVjf7pToKs"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pos_le = LabelEncoder().fit([token['pos_tag'] for sentence in wiki_ner_data for token in sentence])\n",
    "ner_le = LabelEncoder().fit([token['ner_tag'] for sentence in wiki_ner_data for token in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJrbdDZ2Z_-K"
   },
   "source": [
    "Now we have a module mapping from tags to IDs and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UoHsG_1XToPI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag = ['I-PER']\n",
    "# ner_tag = ['I-LOC']\n",
    "# ner_tag = ['B-PER']\n",
    "# ner_tag = ['O']\n",
    "\n",
    "ner_le.transform(ner_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0BFX3l8Jau4S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-LOC'], dtype='<U6')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_id = [0]\n",
    "\n",
    "ner_le.inverse_transform(ner_tag_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsL58BEua48X"
   },
   "source": [
    "How many NER tags do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MluEFeCFbOYl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVGvRsmTatEq"
   },
   "source": [
    "Which are those tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fIZUZQJabHT-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG',\n",
       "       'I-PER', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbAeRowAatSg"
   },
   "source": [
    "Collect the same info for POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hfkNqH4ObDcM"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQRJ2JTgb6tW"
   },
   "source": [
    "Finally we can do our train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qgQjMJIQb_Ju"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tmp_data, test_data = train_test_split(wiki_ner_data)\n",
    "train_data, valid_data = train_test_split(wiki_ner_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOmLfsEhTerk"
   },
   "source": [
    "### Defining and training the RNN model for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbYECVf6cygn"
   },
   "source": [
    "We start by installing PyTorch and importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "laZ0X6Q1c7o8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/vincenzoscotti_polimi/anaconda3/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/vincenzoscotti_polimi/.local/lib/python3.7/site-packages (from torch) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jiZ9aqFdcy5p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOnFKlCNfuS2"
   },
   "source": [
    "The we load the Word Embedding model we want to use. We can re-use the 50 dimensional GloVe emebddings from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "V_1tfiIYfuhs"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "we_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YX30FkkckpT"
   },
   "source": [
    "Before creating the LSTM we decide where to train our model, either cpu or gpu, depending on which is avaialble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "e6v_WgFyclBl"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WAxAGcPcUeH"
   },
   "source": [
    "Now we can create an RNN module.\n",
    "Here are the available modules for PyTorch: https://pytorch.org/docs/stable/nn.html#recurrent-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "Q16YNMfycU5e",
    "outputId": "5d97bdd2-ce54-446f-90eb-bc517e5ff7b3"
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=50,  # We use the dimensions of the input embeddings\n",
    "    hidden_size=len(ner_le.classes_),  # We use the otput of the LSTM directly for classification\n",
    "    batch_first=True,  # To specify that the first dimension of the input tensor in the batch size (more on this later)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0IKmAl7fXxl"
   },
   "source": [
    "Now we can move the LSTM to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B8Oz89mtfYPJ"
   },
   "outputs": [],
   "source": [
    "lstm = lstm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HKv1-rrcVRs"
   },
   "source": [
    "Once we have the model we need to create an optimizer that will take care of updating the weights. Here are the available optimizers: https://pytorch.org/docs/stable/optim.html.\n",
    "I'm going to use RMSProp.\n",
    "\n",
    "The optimizer needs to receive the parameters of the neural network and the selected learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9LyIZYvUcVjA"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.RMSprop(params=lstm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNC5JzcpcVxx"
   },
   "source": [
    "Now we need to prepare our data. \n",
    "Before doing so we prepare a function that maps a mini-batch of samples (i.e., a subset of the lists of dictionaries we prepared) to input tensors to use with our model. The function willl take care of:\n",
    "- Mapping all words to their embeddings with size $d$\n",
    "- Collect the emebddings of the same sentence into a matrix with shape $(n_\\textit{words in sentence}, d)$\n",
    "- Collect the different matrices of the same batch into a single tensor with shape $(n_\\textit{batch elements}, n_\\textit{words in longest sentence}, d)$ (this will be input).\n",
    "- Mapping all the labels to their IDs \n",
    "- Collecting all the IDs of the same sentence into a vector with size $n_\\textit{words in sentence}$ \n",
    "- Collecting all the different vectors into a matrix with shape $(n_\\textit{batch elements}, n_\\textit{words in longest sentence})$ (this will be target output).\n",
    "\n",
    "Note that different sentences have different lengths, to cope with this issue we apply a process called padding: we are going to add to the input tensor and the target output matrix  dummy values to have all the same \"sentence length\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XiCDLg0qcWCR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def collate(mini_batch):\n",
    "    # Get the length of the longest sentence\n",
    "    longest_len = max(len(sample) for sample in mini_batch)\n",
    "    # Create an input tensor with all zero values\n",
    "    input_embeds = np.zeros((len(mini_batch), longest_len, 50))\n",
    "    # Create a target output matrix with all -100 (PyTorch ignores this value by default)\n",
    "    output_lbl = np.full((len(mini_batch), longest_len), -100)\n",
    "    # Fill the tensor and the matrix\n",
    "    for i, sample in enumerate(mini_batch):\n",
    "        for j, token in enumerate(sample):\n",
    "            # Manage missing tokens in vocabulary\n",
    "            if token['text'].lower() in we_model:\n",
    "                input_embeds[i,j] = we_model[token['text'].lower()]\n",
    "            output_lbl[i,j] = ner_le.transform([token['ner_tag']])\n",
    "    # Convert to PyTorch tensor\n",
    "    input_embeds = torch.tensor(input_embeds, dtype=torch.float)\n",
    "    output_lbl = torch.tensor(output_lbl)\n",
    "\n",
    "    return input_embeds, output_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzEg6gg9k62U"
   },
   "source": [
    "How does an encoded batch looks like? Let's enode the first three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZgvoOTG4k7Lx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'The', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'A300', 'pos_tag': 'NNP', 'ner_tag': 'I-MISC'},\n",
       "  {'text': 'provided', 'pos_tag': 'VBD', 'ner_tag': 'O'},\n",
       "  {'text': 'Airbus', 'pos_tag': 'NNP', 'ner_tag': 'I-ORG'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'experience', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': 'of', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'manufacturing', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       "  {'text': 'and', 'pos_tag': 'CC', 'ner_tag': 'O'},\n",
       "  {'text': 'selling', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       "  {'text': 'airliners', 'pos_tag': 'NNS', 'ner_tag': 'O'},\n",
       "  {'text': 'competitively', 'pos_tag': 'RB', 'ner_tag': 'O'},\n",
       "  {'text': '.', 'pos_tag': '.', 'ner_tag': 'O'}],\n",
       " [{'text': 'After', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'spending', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       "  {'text': 'over', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'a', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'year', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': 'considering', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'difficulties', 'pos_tag': 'NNS', 'ner_tag': 'O'},\n",
       "  {'text': 'of', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'developing', 'pos_tag': 'VBG', 'ner_tag': 'O'},\n",
       "  {'text': 'a', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'new', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       "  {'text': 'game', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       "  {'text': 'they', 'pos_tag': 'PRP', 'ner_tag': 'O'},\n",
       "  {'text': 'received', 'pos_tag': 'VBD', 'ner_tag': 'O'},\n",
       "  {'text': 'a', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'call', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': 'from', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'Kazuhiko', 'pos_tag': 'NNP', 'ner_tag': 'I-PER'},\n",
       "  {'text': 'Aoki', 'pos_tag': 'NNP', 'ner_tag': 'I-PER'},\n",
       "  {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       "  {'text': 'who', 'pos_tag': 'WP', 'ner_tag': 'O'},\n",
       "  {'text': 'offered', 'pos_tag': 'VBD', 'ner_tag': 'O'},\n",
       "  {'text': 'to', 'pos_tag': 'TO', 'ner_tag': 'O'},\n",
       "  {'text': 'produce', 'pos_tag': 'VB', 'ner_tag': 'O'},\n",
       "  {'text': '.', 'pos_tag': '.', 'ner_tag': 'O'}],\n",
       " [{'text': '\"', 'pos_tag': 'LQU', 'ner_tag': 'O'},\n",
       "  {'text': 'Faced', 'pos_tag': 'VBN', 'ner_tag': 'O'},\n",
       "  {'text': 'with', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'a', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'revved', 'pos_tag': 'VBN', 'ner_tag': 'O'},\n",
       "  {'text': 'up', 'pos_tag': 'RP', 'ner_tag': 'O'},\n",
       "  {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       "  {'text': 'war-ready', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       "  {'text': 'population', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       "  {'text': 'and', 'pos_tag': 'CC', 'ner_tag': 'O'},\n",
       "  {'text': 'all', 'pos_tag': 'PDT', 'ner_tag': 'O'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'editorial', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       "  {'text': 'encouragement', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'two', 'pos_tag': 'CD', 'ner_tag': 'O'},\n",
       "  {'text': 'competitors', 'pos_tag': 'NNS', 'ner_tag': 'O'},\n",
       "  {'text': 'could', 'pos_tag': 'MD', 'ner_tag': 'O'},\n",
       "  {'text': 'muster', 'pos_tag': 'VB', 'ner_tag': 'O'},\n",
       "  {'text': ',', 'pos_tag': ',', 'ner_tag': 'O'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'US', 'pos_tag': 'NNP', 'ner_tag': 'I-LOC'},\n",
       "  {'text': 'jumped', 'pos_tag': 'VBD', 'ner_tag': 'O'},\n",
       "  {'text': 'at', 'pos_tag': 'IN', 'ner_tag': 'O'},\n",
       "  {'text': 'the', 'pos_tag': 'DT', 'ner_tag': 'O'},\n",
       "  {'text': 'opportunity', 'pos_tag': 'NN', 'ner_tag': 'O'},\n",
       "  {'text': 'to', 'pos_tag': 'TO', 'ner_tag': 'O'},\n",
       "  {'text': 'get', 'pos_tag': 'VB', 'ner_tag': 'O'},\n",
       "  {'text': 'involved', 'pos_tag': 'VBN', 'ner_tag': 'O'},\n",
       "  {'text': 'and', 'pos_tag': 'CC', 'ner_tag': 'O'},\n",
       "  {'text': 'showcase', 'pos_tag': 'VB', 'ner_tag': 'O'},\n",
       "  {'text': 'its', 'pos_tag': 'PRP$', 'ner_tag': 'O'},\n",
       "  {'text': 'new', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       "  {'text': 'steam-powered', 'pos_tag': 'JJ', 'ner_tag': 'O'},\n",
       "  {'text': 'Navy', 'pos_tag': 'NNP', 'ner_tag': 'I-LOC'},\n",
       "  {'text': '\"', 'pos_tag': 'RQU', 'ner_tag': 'O'},\n",
       "  {'text': '.', 'pos_tag': '.', 'ner_tag': 'O'}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZFQlJoPUlyIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input is: torch.Size([3, 38, 50])\n",
      "The shape of the output is: torch.Size([3, 38])\n"
     ]
    }
   ],
   "source": [
    "embeds, lbl = collate(train_data[:3])\n",
    "\n",
    "print(f\"The shape of the input is: {embeds.size()}\")\n",
    "print(f\"The shape of the output is: {lbl.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zGD62Kj4mNsA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  ..., -1.8411e-01,\n",
       "          -1.1514e-01, -7.8581e-01],\n",
       "         [ 1.3736e+00, -7.6518e-01, -2.2713e-01,  ...,  4.5972e-01,\n",
       "           2.6261e-01, -5.0583e-01],\n",
       "         [ 5.2014e-01,  3.8829e-01,  1.1381e-01,  ...,  8.4034e-01,\n",
       "          -2.9246e-02,  1.8248e-01],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 3.8315e-01, -3.5610e-01, -1.2830e-01,  ..., -4.2580e-01,\n",
       "           1.3681e-01, -7.7731e-01],\n",
       "         [ 1.2570e-03, -4.6617e-01,  7.5832e-01,  ...,  6.0802e-01,\n",
       "           4.2680e-01,  9.9916e-01],\n",
       "         [ 1.2972e-01,  8.8073e-02,  2.4375e-01,  ...,  9.0912e-02,\n",
       "          -6.0515e-01, -9.8270e-01],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 2.5769e-01,  4.5629e-01, -7.6974e-01,  ..., -3.1438e-01,\n",
       "           7.5004e-01,  9.7065e-01],\n",
       "         [-2.6932e-01,  4.5133e-02,  1.2981e-02,  ..., -3.1100e-02,\n",
       "          -3.0802e-03, -3.3912e-01],\n",
       "         [ 2.5616e-01,  4.3694e-01, -1.1889e-01,  ..., -7.5730e-02,\n",
       "          -2.5868e-01, -3.9339e-01],\n",
       "         ...,\n",
       "         [ 2.5906e-02, -2.0825e-01, -2.9398e-01,  ...,  2.1497e-01,\n",
       "          -4.9118e-01,  3.5245e-01],\n",
       "         [ 2.5769e-01,  4.5629e-01, -7.6974e-01,  ..., -3.1438e-01,\n",
       "           7.5004e-01,  9.7065e-01],\n",
       "         [ 1.5164e-01,  3.0177e-01, -1.6763e-01,  ..., -3.5652e-01,\n",
       "           1.6413e-02,  1.0216e-01]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E3FAiBTJmN16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   8,    5,    8,    6,    8,    8,    8,    8,    8,    8,    8,    8,\n",
       "            8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100],\n",
       "        [   8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,\n",
       "            8,    8,    8,    8,    8,    8,    8,    7,    7,    8,    8,    8,\n",
       "            8,    8,    8, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100],\n",
       "        [   8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,\n",
       "            8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    4,    8,\n",
       "            8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    4,\n",
       "            8,    8]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3TpDkJ8cWUR"
   },
   "source": [
    "Now we can finally wrap a DataLoader around our samples. PyTorch data loaders take care of generating batches on a given data set. We just need to set the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "moRFz8fycWhv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-SKOksvmxuq"
   },
   "source": [
    "We can train the lst iterating over the data set for a given number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "92NG8pwTmyIp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/3483 [00:00<07:26,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3483/3483 [01:34<00:00, 36.66it/s]\n",
      "  0%|                                          | 4/3483 [00:00<01:37, 35.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3483/3483 [01:37<00:00, 35.90it/s]\n",
      "  0%|                                          | 3/3483 [00:00<01:59, 29.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3483/3483 [01:34<00:00, 36.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import nice loading bar using tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set model in training mode\n",
    "lstm.train()\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "# Iterate over epochs\n",
    "for i in range(n_epochs):\n",
    "    print(f\"Starting epoch {i + 1}/{n_epochs}\")\n",
    "    # Iterate over training batches\n",
    "    for embeds, lbl in tqdm(train_loader):\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Move input and output to target device\n",
    "        embeds = embeds.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        # Compute logits (i.e., softmax values before exponential normalisation)\n",
    "        logits, _ = lstm(embeds)\n",
    "        # Flatten logits to a shape (batch_size * max_sentence_len, n_classes)\n",
    "        logits = logits.reshape(-1, len(ner_le.classes_))\n",
    "        # Flatten targets to a shape (batch_size * max_sentence_len)\n",
    "        lbl = lbl.reshape(-1)\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, lbl)\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf28zwGSmyXE"
   },
   "source": [
    "We can also test out model using the dedicated split. We iterate over the mini batches, colelct the prediction as the value with the highest logit value and we store these values until we go through the entire data set. Then we compute the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "v6_NYT35myps"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1161/1161 [00:24<00:00, 47.66it/s]\n",
      "/Users/vincenzoscotti_polimi/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gk/xpx9_6hj2ggfwlz94zwr_1_80000gp/T/ipykernel_38612/3377364760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Finally compute classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mner_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m     )\n\u001b[1;32m   2150\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0msamplewise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamplewise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m     )\n\u001b[1;32m   1555\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0msorted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(diff)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_check_unknown\u001b[0;34m(values, known_values, return_mask)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# check for nans in the known_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mdiff_is_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff_is_nan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set model in evaluation mode\n",
    "lstm.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Disable gradients\n",
    "with torch.no_grad():\n",
    "    # Iterate over validation batches\n",
    "    for embeds, lbl in tqdm(test_loader):\n",
    "        # Move input and output to target device\n",
    "        embeds = embeds.to(device)\n",
    "        # Compute logits (i.e., softmax values before exponential normalisation)\n",
    "        logits, _ = lstm(embeds)\n",
    "        # Get predictions as the index corresponding to the highest logit score\n",
    "        pred_lbl = torch.argmax(logits, dim=-1)\n",
    "        # Append predicted labels\n",
    "        y_pred.append(pred_lbl.reshape(-1).cpu().numpy())  \n",
    "        # Append target labels\n",
    "        y_true.append(lbl.reshape(-1).numpy())\n",
    "\n",
    "# Concatenate all the vectors of target labels and predicted labels\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "# Remove elements to ignore (the -100 labels)\n",
    "mask = y_true == -100\n",
    "y_true = y_true[~mask]\n",
    "y_pred = y_pred[~mask]\n",
    "\n",
    "# Finally compute classification report\n",
    "print(classification_report(y_true, y_pred, target_names=ner_le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QXsI0jumzPu"
   },
   "source": [
    "We can play a bit with the model directly.\n",
    "\n",
    "Let's define a function to compute the predictions give a sentence. We will use the NLTK tokenizer to split the sentence into word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def predict(sample):\n",
    "    # Tokenize sample\n",
    "    tokenized_sample = word_tokenize(sample)\n",
    "    # Create an input tensor with all zero values\n",
    "    input_embeds = np.zeros((1, len(tokenized_sample), 50))\n",
    "    # Fill the tensor and the matrix\n",
    "    for i, token in enumerate(tokenized_sample):\n",
    "        # Manage missing tokens in vocabulary\n",
    "        if token.lower() in we_model:\n",
    "            input_embeds[0, i] = we_model[token.lower()]\n",
    "    # Convert to PyTorch tensor\n",
    "    input_embeds = torch.tensor(input_embeds, dtype=torch.float, device=device)\n",
    "    # Run model over input\n",
    "    logits, _ = lstm(embeds)\n",
    "    # Get predictions as the index corresponding to the highest logit score\n",
    "    pred_lbl = torch.argmax(logits, dim=-1)\n",
    "    # Decode labels\n",
    "    pred_labels = ner_le.inverse_transform(pred_lbl.reshape(-1).cpu().numpy())\n",
    "    # Group together tokens and predicted NER labels\n",
    "    labelled_sample = [{'text': token, 'ner_tag': str(lbl)} for token, lbl in zip(tokenized_sample, pred_labels)] \n",
    "\n",
    "    return labelled_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now call it on a custom sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "YUSA5d6kmzgz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Hello', 'ner_tag': 'O'},\n",
       " {'text': ',', 'ner_tag': 'O'},\n",
       " {'text': 'my', 'ner_tag': 'O'},\n",
       " {'text': 'name', 'ner_tag': 'O'},\n",
       " {'text': 'is', 'ner_tag': 'O'},\n",
       " {'text': 'Vincenzo', 'ner_tag': 'O'},\n",
       " {'text': 'and', 'ner_tag': 'O'},\n",
       " {'text': 'I', 'ner_tag': 'I-LOC'},\n",
       " {'text': 'like', 'ner_tag': 'O'},\n",
       " {'text': 'pizza', 'ner_tag': 'O'},\n",
       " {'text': '.', 'ner_tag': 'O'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Hello, my name is Vincenzo and I like pizza.\"\n",
    "\n",
    "predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_jDJ6rxUEgP"
   },
   "source": [
    "### Defining and training the RNN model for POS-tagging\n",
    "\n",
    "You can do this at home to start getting familiar with PyTorch and RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZzejzJYUSAE"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDHo75GQgekV"
   },
   "source": [
    "## Text Classification with a Recurrent Neural Network\n",
    "\n",
    "In this last section of the notebook I will run through a quick example of using a Bidirectional LSTM (Long Short-term Memory) network for text classification. \n",
    "- RNNs extend embedding-based classification of text by taking word-order into account. They were, until relatively recently, the state-of-the-art when it came to training text classifiers.\n",
    "- Tensorflow is sophisticated toolkit for building Deep Neural Network models. We will use it to build the model. The tutorial follows mostly this Tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u16CK7FTIwB1"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t58f_I_8lykt"
   },
   "source": [
    "First let's load the Twitter dataset we used in the second session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjT5uSDM-198"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD1xrLckmOgk"
   },
   "source": [
    "Remove emoticons from the positive and negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdoAOyvAmjjo"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
    "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBYSfDdNnyjt"
   },
   "source": [
    "And create the examples and labels as we did before. This time we will use numeric labels (0,1) instead of text labels ('negative','positive'), since the deep learning library we will use requires numeric class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m_yGUqZb_eC"
   },
   "outputs": [],
   "source": [
    "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
    "tweets_y = [1]*len(positive_tweets) + [0]*len(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NMm42BXoFPr"
   },
   "source": [
    "And again, split the data into training, validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWFm2EAUEttp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "temp_x, test_x, temp_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(temp_x, temp_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78Up263koMI9"
   },
   "source": [
    "Now that we have the training and validation data prepared, we can import the Tensorflow library, and use it to load the training and validaton datasets into the tensorflow format. Note that:\n",
    "- Tensorflow comes installed on Google Colab. \n",
    "- If you run this notebook on your own machine you will need to first install tensorflow using '!pip install'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tryj01SOHdQD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_tf = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_tf = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bns4PMHlosKG"
   },
   "source": [
    "Training will run on *batches* of the data at a time, so we need to create them.\n",
    "- We first use the shuffle command to randomise the order of the training data. (The buffer-size limits the number of instances loaded into memory when shuffling and is only for efficiency -- you could remove it.)\n",
    "- We then create the batches. Each batch will contain 64 examples.\n",
    "- The validation data needs to have the same format as the training data, so we batch it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYHJ3N_9Fk4M"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_tf.shuffle(buffer_size=10000).batch(batch_size=64).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_tf.batch(batch_size=64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_kUXbOTp_E5"
   },
   "source": [
    "Let's have a look at the first batch in the training data. It consists of:\n",
    "- an array of strings (tweets)\n",
    "- an array of binary values (class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO4o9OCmpjHP"
   },
   "outputs": [],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "  print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kecw1hDHqDwq"
   },
   "source": [
    "Now that we have the text data in the format required, we can vectorize it. We will need to make use a specific text vectorization module from tensorflow to do this.\n",
    "- We first limit the vocabulary of the vectorizer to 5000,\n",
    "- then extract only the text portion of the training dataset,\n",
    "- and finally fit the vectorizer to the text using the 'adapt' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWBSlNHYH63Q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=5000)\n",
    "train_text = train_dataset.map(lambda text, label: text)\n",
    "vectorizer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxBb8rBmrncY"
   },
   "source": [
    "Let's print out the first tokens form the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mx6bPwIHcs"
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew9vtoMusC5x"
   },
   "source": [
    "Note that the first two tokens in the vocabulary are the empty token '', and the unknown token '[UNK]'. The latter is used to mask out-of-vocabulary tokens in the text\n",
    "\n",
    "We can now use the vectorizer to encode a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T7q-VGpINji"
   },
   "outputs": [],
   "source": [
    "text = 'This is my first tweet! It contains one out-of-vocabulary term. Any suggestions for extending this tweet?'\n",
    "encoding = vectorizer([text]).numpy()[0]\n",
    "print('Tweet:     ', text)\n",
    "print('Encoded:   ', encoding) \n",
    "print('Recovered: ',' '.join([vocab[i] for i in encoding]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnpb08OAysG0"
   },
   "source": [
    "Note that the vectorizer is not turning the text into a single vector, but is simply replacing the vocabulary words by their indices. If a word is not present in the dictionary it is replaced by the unknown token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adhzBr8gxol6"
   },
   "source": [
    "Let's have a look at some actual examples from the dataset, printing out the first 6 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVhxZQhlb5pb"
   },
   "outputs": [],
   "source": [
    "for text in batch[0][:6].numpy():\n",
    "    encoding = vectorizer([text]).numpy()[0]\n",
    "    print('Tweet:     ', text.decode(\"utf-8\"))\n",
    "    print('Encoded:   ', encoding) \n",
    "    print('Recovered: ',' '.join([vocab[i] for i in encoding]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVu5mm0BlnIU"
   },
   "source": [
    "### Defining the RNN model\n",
    "\n",
    "Now we can define the model, which contains four layers: \n",
    "- an input embedding layer which produces word embeddings of size 64\n",
    "- a bidirectional LSTM layer\n",
    "- 2 dense (aka fully connected) layers that maps the 2 embedding vectors (of size 64) produced by the bidirectional LSTM down to a single neuron   \n",
    "\n",
    "This constitutes a relatively standard basic RNN architecture. (The details of why these specific components are chosen is beyond the scope of this tutorial.)  \n",
    "\n",
    "Once the model has been defined it is compiled in the following step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkBOSHb8IdNq"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorizer,                       \n",
    "    tf.keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()),output_dim=64,mask_zero=True), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'],optimizer=tf.keras.optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZ3xHUUdvdOp"
   },
   "source": [
    "Fit the model by running it for 10 epochs (iterations over the training data).\n",
    "- Note that we provide it with both the training dataset and the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8QbNIKxI375"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=10, validation_data=valid_dataset, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-Xzgsr-3Pl"
   },
   "source": [
    "Once we've trained the model we can check the final accuracy on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cB34Jys-I7p2"
   },
   "outputs": [],
   "source": [
    "valid_loss, valid_acc = model.evaluate(valid_dataset)\n",
    "\n",
    "print('Validation Loss: {}'.format(valid_loss))\n",
    "print('Validation Accuracy: ',valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sCKn20yCLDs"
   },
   "source": [
    "We can have a look at the predictions from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZtp5zCvgoVc"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "tweets.append('I can\\'t believe how much fun I\\'m having learning to train a text classifier with a bidirectional LSTM!')\n",
    "tweets.append('I am really confused. I want my mommy.')\n",
    "tweets.append('The internet connection has been pretty annoying today!')\n",
    "tweets.append('They just played my favourite song on the radio.')\n",
    "tweets.append(\"I don't like going to the dentist.\")\n",
    "tweets.append('I am so happy today!')\n",
    "tweets.append('I am so unhappy today!')\n",
    "\n",
    "predictions = model.predict(tweets)\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "  print('tweet: ',tweets[i])\n",
    "  encoding = vectorizer([tweets[i]]).numpy()[0]\n",
    "  print('encoded as: ',' '.join([vocab[j] for j in encoding]))\n",
    "  print('predicted value: ', predictions[i][0])\n",
    "  print('predicted label: ', 'negative' if (predictions[i]<0) else 'positive')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvAe7kWdCPpv"
   },
   "source": [
    "And calculate the usual evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8FABFCi693C"
   },
   "outputs": [],
   "source": [
    "pred_y = [0 if (pred < 0) else 1 for pred in model.predict(valid_x)]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy: '+ str(accuracy_score(pred_y, valid_y)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix(valid_y, pred_y,normalize='all'),display_labels=['negative','positive'])\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hNoR7FzIVSW"
   },
   "source": [
    "Finally, let's print out the model summary to get an understanding of the number of parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msGvq6XA_qxP"
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8yHHWsbJjtm"
   },
   "source": [
    "Most of the parameters are used to define the embeddiing, then the LSTMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrHVkXHzJrQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
