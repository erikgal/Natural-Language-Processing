{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giKsby3Lpdlp"
   },
   "source": [
    "# Session 5: Sequence Labeling & Sequence Classifiers\n",
    "\n",
    "In this session we'll first investigate Part-of-Speech (POS) tagging and Named-entity recognition (NER). \n",
    "- For this we will make use of the spaCy natural langauge processing API: https://spacy.io/\n",
    "- spaCy is an opensource API that provides state-of-the-art performance on sequence labeling tasks such as POS tagging and NER. \n",
    "- Parts of this tutorial are based on code from: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "In the second part of the tutorial we will train a Text classifier that makes use of a Bidirectional LSTM (Long Short-term Memory) model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OgRPEb6pdlv"
   },
   "source": [
    "## 5.1 Installing spaCy and downloading models\n",
    "\n",
    "First we need to check whether the spaCy library is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3083U81cpdlv"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ1xLIcHpdlw"
   },
   "source": [
    "---\n",
    "\n",
    "Then we need to download pretrained models for use with spaCy. We will load models for both English and Italian:\n",
    "- The models are called 'en_core_web_sm' and 'it_core_news_sm', where the 'web'/'news' indicates what type of collection the model was trained on and the 'sm' at the end indicates that we are using the 'small' version of the models\n",
    "- Other models are available here: https://spacy.io/models\n",
    "- The following code calls the python executable instructing it to run the module 'spacy', which in turn download the models. See discussion here: https://stackoverflow.com/questions/46148033/unable-to-load-en-from-spacy-in-jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyv-NpFbpdlw"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "!{sys.executable} -m spacy download it_core_news_sm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYRlbtvspdlx"
   },
   "source": [
    "---\n",
    "\n",
    "We are now ready to import spacy and load a model. Let's start with the English model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KHrsza0pdlx"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp_model = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy4K8dRApdlx"
   },
   "source": [
    "Consider the following piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MGjLJU0pdlx"
   },
   "outputs": [],
   "source": [
    "text = 'Melbourne is to re-enter Stage 3 lockdown after a record increase in cases. Victorian state premier Daniel Andrews said there was “simply no alternative” to reimposing stay at home restrictions in Australia’s second-biggest city.'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_rCI0Hbpdly"
   },
   "source": [
    "Parse the text using the NLP engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmpvXvjapdly"
   },
   "outputs": [],
   "source": [
    "parsed_text = nlp_model(text)\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnXyGOx0pdlz"
   },
   "source": [
    "Did it do something? It looks like it has just output the same text.\n",
    "- Actually, yes. It has parsed the input and built its internal datastructure from it. \n",
    "- Note that the length of the parsed object is in words, not characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbAOI2Mnpdlz"
   },
   "outputs": [],
   "source": [
    "print('The length of the original text is',len(text),'chacacters')\n",
    "print('The length of the parsed text is',len(parsed_text),'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xImJ-i_Lpdlz"
   },
   "source": [
    "## 5.2 Part-of-Speech Tagging\n",
    "\n",
    "While parsing the text, spaCy performs part-of-speech (POS) tagging. \n",
    "- We can see the POS tag for each token as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho4HEJTLpdlz"
   },
   "outputs": [],
   "source": [
    "[(w,w.pos_) for w in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFEzvleepdlz"
   },
   "source": [
    "Who remembers their grammar from high school? What do all those POS symbols mean?\n",
    "- You can find an explanation of the POS tags on this website https://spacy.io/api/annotation in the section \"Universal Part-of-speech Tags\" \n",
    "\n",
    "What can we do with POS tags? \n",
    "- Well, we could select all terms that have a certain tag, such as all adjectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kcjc28lpdl0"
   },
   "outputs": [],
   "source": [
    "set(w for w in parsed_text if w.pos_=='ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftw5HEYEpdl0"
   },
   "source": [
    "That was a little underwhelming. \n",
    "- Let's try it on Alice in Wonderland chapter 1 text. (You'll need to upload it again to Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGRt3HF8pdl0"
   },
   "outputs": [],
   "source": [
    "adjectives = sorted(set(w.text for w in nlp_model(open(\"Alice_Chapter1.txt\", \"r\").read()) if w.pos_=='ADJ'))\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pct1jNdEpdl0"
   },
   "source": [
    "You can see how descriptive a writer Lewis Carroll was! \n",
    "\n",
    "This leads us to one explanation as to why we might want to extract POS tags from text: \n",
    "- They can sometimes be useful for **extracting features** (often handcrafted ones) for certain text classification tasks (such as authorship identification).\n",
    "- This is particularly the case if only a small amount of training data is available.  \n",
    "- For example, in this article (https://towardsdatascience.com/automatically-detect-covid-19-misinformation-f7ceca1dc1c7) hand-crafted features are extracted for classifying covid misinformation. \n",
    "\n",
    "Another reason why we might consider POS tagging is to **reduce ambiguity** in our bag-of-words representation by appending POS tags to word occurrences. \n",
    "- Consider the following two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iO992Q-pdl0"
   },
   "outputs": [],
   "source": [
    "ex1 = 'I catch the train to and from work.'\n",
    "ex2 = 'I like to train at least 3 times a week.'\n",
    "print(ex1, '<-- \\'train\\' is a', nlp_model(ex1)[3].pos_)\n",
    "print(ex2, '<-- \\'train\\' is a', nlp_model(ex2)[3].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHlqof06pdl1"
   },
   "source": [
    "The second sentence has nothing to do with trains, despite containing the word 'train'!\n",
    "- We could deal with this issue by appending the POS tag to the observed literal to form vocabulary elements: train_NOUN, train_VERB\n",
    "\n",
    "A final reason why we might think about running POS tagging would be to extract proper nouns from the text, since they refer to real entities that are being discussed in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kW4TbHffpdl1"
   },
   "outputs": [],
   "source": [
    "[w.text for w in parsed_text if w.pos_=='PROPN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDl1pPoSpdl1"
   },
   "source": [
    "Shortly though, we will talk about Entity-extraction, which is the task of identifying and categorising the entities discussed in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDh0iS65pdl1"
   },
   "source": [
    "## 5.3 Lemmatization\n",
    "\n",
    "While parsing, spaCy also performs lemmatization. \n",
    "- Lemmatization is the process of extracting the 'lemma' for each token, which is the canonical form of the word that would be found in the dictionary, (see https://en.wikipedia.org/wiki/Lemma_(morphology))\n",
    "- Basically, verbs converted to their root form, e.g.: **went, going, goes, gone => go**\n",
    "- And nouns are retuned to singular form: **kittens => kitten**\n",
    "- Lemmatization is a more complicated POS-aware process than stemming (https://en.wikipedia.org/wiki/Stemming). Stemmers simply apply a set of language-specific syntax rules to recover the stem of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EevBrQRbpdl1"
   },
   "outputs": [],
   "source": [
    "[(x, x.lemma_) for x in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEIoq1RFpdl1"
   },
   "source": [
    "Why would one want to perfom lemmatization? -- Or stemming for that matter?\n",
    "- to **reduce the vocabulary size** and thereby generalise the representation. -- This used to be very important for improving performance of search engine performance (better similarity measures between documents) and also classifiers on small datasets, (before word embeddings came along).\n",
    "- to **look-up information** about the word in a dictionary/ontology, such as WordNet (https://en.wikipedia.org/wiki/WordNet). This used to be an important way to compute semantic similarity between words, but again, word embeddngs probably do a better job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vBQ7f0tpdl2"
   },
   "source": [
    "## 5.4 Dependency Parsing\n",
    "\n",
    "Tradititonally in Natural Language Processing, text is processed in a pipeline that first tokenizes, then POS tags, lemmatizes and finaly dependency parses a piece of text. \n",
    "- The idea with dependency parsing is to determine what function each of the word instances is fulfilling in the sentence. \n",
    "- What is the subject and object of the sentence? \n",
    "- Which noun is each adjective referring to?\n",
    "\n",
    "So while parsing the text, the spaCy model also generates a **dependency parse tree**, which can be displayed using 'displacy':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dbz57hc5pdl2"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(parsed_text, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quwWOjbOpdl3"
   },
   "source": [
    "Such dependency trees are interesting for understanding and visualising language (particularly for linguists) and could possibly be used for some downstream tasks (say checking ambiguity in legal documents).  \n",
    "\n",
    "Consider the sentences:\n",
    "- *The girl saw a man carrying a telescope.*\n",
    "- *The girl saw a man with a telescope.*\n",
    "\n",
    "Who had the telescope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AxJLNswpdl3"
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp_model('The girl saw a man carrying a telescope.'),jupyter=True,style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abgGOdwpdl3"
   },
   "outputs": [],
   "source": [
    "displacy.render(nlp_model('The girl saw a man with a telescope.'),jupyter=True,style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fIZq6zIhPbk"
   },
   "source": [
    "The second sentence is ambiguous: The girl may have made use of her telescope or the man may have been using the girl's telescope...\n",
    "- Language is full of such ambiguities which we as humans naturally deal with using our prior knowledge and abilty to construct mental models of the situations described.\n",
    "- This process is not without its biases:\n",
    "  - *The doctor went over to talk to the nurse. She told him that she had just given the patient 5mg of Vicodin and the child had started convulsing. He listened attentively as she explained what had happened. The doctor was worried that the patient should not be given any more painkillers. The nurse told the doctor not to worry, that the patient was in good hands, and that he would let her know immediately if the child's condition changed.*\n",
    "  - What gender are the doctor and the nurse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uE1efbqpdl3"
   },
   "source": [
    "## 5.5 Extracting Entities\n",
    "\n",
    "A more important output than the depency parse, from a text mining perspective, is the list of named-entities present in the text\n",
    "- **named entities** are objects in the real world, e.g. persons, products, organizations, locations, etc. \n",
    "  - see https://en.wikipedia.org/wiki/Named_entity\n",
    "- if spacy has found any named entities while parsing the text, we can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irYwH5WVpdl3"
   },
   "outputs": [],
   "source": [
    "parsed_text.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv6krbXmpdl3"
   },
   "source": [
    "Note that the entities are not single word tokens but short sequences of words: 'Stage 3' and 'Daniel Andrews'.\n",
    "\n",
    "Not only does spacy extract the entities, but also categorises them based on their type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXA2FRzXpdl4"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in parsed_text.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyp09Uelpdl4"
   },
   "source": [
    "The city and country locations have been labeled 'GPE' for 'geopolitical entity', while the Premier of Victoria has been correctly identified as a person. \n",
    "- Here is the list of all entity types that spaCy looks for: https://spacy.io/api/annotation#section-named-entities\n",
    "\n",
    "Internally, the output of the Named Entity Recogniser is a sequence annotated with entities using inside-outside-beginning encoding: \n",
    "- see https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "- We can print out this labeling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E1H-RM6pdl4"
   },
   "outputs": [],
   "source": [
    "[(X, X.ent_iob_, X.ent_type_) for X in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tU8n44pdl4"
   },
   "source": [
    "The above format is a bit hard to read though, so spaCy also provides a far more natural visualisation of the tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PmUl2y5pdl4"
   },
   "outputs": [],
   "source": [
    "displacy.render(parsed_text,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS1PK7ifpdl5"
   },
   "source": [
    "## 5.6 Extracting entities from a web document\n",
    "\n",
    "Now that we know how to perform entity recognition on text, let's apply it to a full document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pY-nN0Zpdl5"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news/world-latin-america-53319517'\n",
    "#url = 'https://en.wikipedia.org/wiki/Apple_(disambiguation)'\n",
    "\n",
    "import requests\n",
    "html_doc = requests.get(url).text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "parsed_doc = BeautifulSoup(html_doc, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRe5AUvCpdl5"
   },
   "source": [
    "Now lets extract the title and paragraph text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5YGoQISpdl5"
   },
   "outputs": [],
   "source": [
    "title = parsed_doc.find('title').text\n",
    "paragraphs = [p.text for p in parsed_doc.find_all('p')]\n",
    "\n",
    "# combine the title and paragraphs into a single text:\n",
    "article_text = title + '\\n\\n' + '\\n'.join(paragraphs)\n",
    "print(article_text)\n",
    "\n",
    "#article_text = parsed_doc.get_text()\n",
    "#print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_dqist0pdl5"
   },
   "source": [
    "---\n",
    "\n",
    "Parse the article to identify the entities and display them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMTsfAhYpdl6"
   },
   "outputs": [],
   "source": [
    "parsed_article = nlp_model(article_text)\n",
    "displacy.render(parsed_article,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I8_zJQrpdl6"
   },
   "source": [
    "---\n",
    "\n",
    "What do you think? Did it work?\n",
    "\n",
    "Let's have a bit of a better look at the entities found\n",
    "- List all the distinct entities found in the article, sorted alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SakcaXuypdl6"
   },
   "outputs": [],
   "source": [
    "sorted(set(x.text for x in parsed_article.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyGeLXs8pdl6"
   },
   "source": [
    "We can count the number of times each **entity type** occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcmsbiUkpdl6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [x.label_ for x in parsed_article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb3g9y5Kpdl6"
   },
   "source": [
    "We can also count the number of times each **entity name** occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPdOsoiApdl6"
   },
   "outputs": [],
   "source": [
    "items = [x.text for x in parsed_article.ents]\n",
    "Counter(items).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEV2BJCTpdl7"
   },
   "source": [
    "Note that some of the phrases refer to the same entity, e.g. 'Mr Bolsonaro' and just 'Bolsonaro'.\n",
    "- Entity Linking and Reference Resolution are the NLP problems that deal with resolving the different references to the same entity in the text.\n",
    "\n",
    "If we were only interested in what was being said about Bolsonaro, \n",
    "- we could select only sentences refering to him:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNXjt46Jpdl7"
   },
   "outputs": [],
   "source": [
    "sentences_containing_Bolsonaro = [x for x in parsed_article.sents if 'Bolsonaro' in x.text]\n",
    "displacy.render(sentences_containing_Bolsonaro,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkJvASO9pdl7"
   },
   "source": [
    "## 5.7 Named Entity Extraction in Italian\n",
    "\n",
    "But wait, SpaCy can speak Italian too!\n",
    "- Let's make use of the pretrained italian model that we downloaded earlier: https://spacy.io/models/it\n",
    "- to recognise entities in an article from 'Il Corriere'\n",
    "\n",
    "First download the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCTkJTsopdl7"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.ansa.it/sito/notizie/mondo/2020/07/07/bolsonaro-ha-i-sintomi-del-coronavirus_40d26967-e377-4455-9b42-83c2756cf5f1.html'\n",
    "html_doc = requests.get(url).text\n",
    "parsed_doc = BeautifulSoup(html_doc, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrnqQcMOpdl7"
   },
   "source": [
    "Now let's extract the title and paragraph text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIyIysghpdl7"
   },
   "outputs": [],
   "source": [
    "title = parsed_doc.find('title').text\n",
    "paragraphs = [p.text for p in parsed_doc.find_all('p')]\n",
    "\n",
    "# combine the title and paragraphs into a single text:\n",
    "article_text = title + '\\n\\n' + '\\n'.join(paragraphs)\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eluDQCqxpdl7"
   },
   "source": [
    "---\n",
    "\n",
    "Now we'll parse the text of the article with an Italian NLP engine to extract Named Entities.\n",
    "- First load the italian model 'it_core_news_sm' that we downloaded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj0IIHlVpdl7"
   },
   "outputs": [],
   "source": [
    "import it_core_news_sm\n",
    "nlp_it = it_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcWwDGlypdl8"
   },
   "source": [
    "Parse article and extract the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X-Kuw6lpdl8"
   },
   "outputs": [],
   "source": [
    "parsed_article = nlp_it(article_text)\n",
    "displacy.render(parsed_article,jupyter=True,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08h69wcxpdl8"
   },
   "source": [
    "That looks not great. \n",
    "- Here are the entities found in the news article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKefi_fspdl8"
   },
   "outputs": [],
   "source": [
    "sorted(set(x.text for x in parsed_article.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GNZaLqN4Dx-"
   },
   "source": [
    "You might try the medium or large sized model to see if you can get better performance, although for some reason they are currently not working in Google Colab unfortunately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzFM8cAUpdl8"
   },
   "source": [
    "## 5.8 Fine-tuning your own NER Model\n",
    "\n",
    "What if you want to update the Named Entity Extraction model yourself in order to customize it to your set of entities? We'll have a look at that now based on:\n",
    "- this instructions page: https://spacy.io/usage/training#ner\n",
    "- and this blog post: https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "\n",
    "In order to fine-tune the model, we need to prepare data in the following format: \n",
    "- a piece of text, \n",
    "- plus a list of entity types that occur it along with their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8aaj2uepdl8"
   },
   "outputs": [],
   "source": [
    "my_data = [\n",
    "    (\"Have you heard of an associate professor from the Politecnico di Milano called Mark Carman?\", {\"entities\": [(50, 71, \"ORG\"),(79, 90, \"PERSON\")]}),\n",
    "    (\"No, I haven't, but I don't know many people at the Politecnico. What does he work on?\", {\"entities\": [(51, 62, \"ORG\")]}),\n",
    "    (\"Mainly machine learning and text mining. I met him a couple of years ago at SIGIR in Tokyo.\", {\"entities\": [(76, 81, \"EVENT\"),(85, 90, \"GPE\")]}),\n",
    "]\n",
    "my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnVIriSEpdl9"
   },
   "source": [
    "Where would this data come from? \n",
    "- either created manually, perhaps by searching for known individuals in a text collection,\n",
    "- or by using an annotation tool such as https://doccano.herokuapp.com/, see for example: https://medium.com/@justindavies/training-spacy-ner-models-with-doccano-8d8203e29bfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s9T3Whe5lZJ"
   },
   "source": [
    "The following code comes from here: https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py\n",
    "- The only change made was to remove the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVjhWJ1Zpdl9"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1pM_VOIpdl9"
   },
   "source": [
    "Once the above model has been defined, we can update and save the model\n",
    "- Note that this code doesn't currently run in Google colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1cpkZGWpdl9"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA=my_data # the method expects the training data to have this name\n",
    "main(model='en_core_web_sm',output_dir='spacy_model',n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlicBTgepdl-"
   },
   "source": [
    "## 5.9 Entity Linking in spaCy\n",
    "\n",
    "We don't want to just find entity mentions in a document but link them to a known entity in a knowledge base. \n",
    "- The task of linking the entity mentions to the corresponding entity in the knowledge base is called 'Entity Linking'.\n",
    "- I don't have time here, but watch this video to learn more: \n",
    "https://spacy.io/universe/project/video-spacy-irl-entity-linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BKB-yMKpdl-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDHo75GQgekV"
   },
   "source": [
    "## 5.10 Text Classification with a Recurrent Neural Network\n",
    "\n",
    "In this last section of the notebook I will run through a quick example of using a Bidirectional LSTM (Long Short-term Memory) network for text classification. \n",
    "- RNNs extend embedding-based classification of text by taking word-order into account. They were, until relatively recently, the state-of-the-art when it came to training text classifiers.\n",
    "- Tensorflow is sophisticated toolkit for building Deep Neural Network models. We will use it to build the model. The tutorial follows mostly this Tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t58f_I_8lykt"
   },
   "source": [
    "First let's load the Twitter dataset we used in the second session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjT5uSDM-198"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD1xrLckmOgk"
   },
   "source": [
    "Remove emoticons from the positive and negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdoAOyvAmjjo"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
    "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBYSfDdNnyjt"
   },
   "source": [
    "And create the examples and labels as we did before. This time we will use numeric labels (0,1) instead of text labels ('negative','positive'), since the deep learning library we will use requires numeric class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m_yGUqZb_eC"
   },
   "outputs": [],
   "source": [
    "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
    "tweets_y = [1]*len(positive_tweets) + [0]*len(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NMm42BXoFPr"
   },
   "source": [
    "And again, split the data into training, validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWFm2EAUEttp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "temp_x, test_x, temp_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(temp_x, temp_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78Up263koMI9"
   },
   "source": [
    "Now that we have the training and validation data prepared, we can import the Tensorflow library, and use it to load the training and validaton datasets into the tensorflow format. Note that:\n",
    "- Tensorflow comes installed on Google Colab. \n",
    "- If you run this notebook on your own machine you will need to first install tensorflow using '!pip install'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tryj01SOHdQD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_tf = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_tf = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bns4PMHlosKG"
   },
   "source": [
    "Training will run on *batches* of the data at a time, so we need to create them.\n",
    "- We first use the shuffle command to randomise the order of the training data. (The buffer-size limits the number of instances loaded into memory when shuffling and is only for efficiency -- you could remove it.)\n",
    "- We then create the batches. Each batch will contain 64 examples.\n",
    "- The validation data needs to have the same format as the training data, so we batch it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYHJ3N_9Fk4M"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_tf.shuffle(buffer_size=10000).batch(batch_size=64).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_tf.batch(batch_size=64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_kUXbOTp_E5"
   },
   "source": [
    "Let's have a look at the first batch in the training data. It consists of:\n",
    "- an array of strings (tweets)\n",
    "- an array of binary values (class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO4o9OCmpjHP"
   },
   "outputs": [],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "  print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kecw1hDHqDwq"
   },
   "source": [
    "Now that we have the text data in the format required, we can vectorize it. We will need to make use a specific text vectorization module from tensorflow to do this.\n",
    "- We first limit the vocabulary of the vectorizer to 5000,\n",
    "- then extract only the text portion of the training dataset,\n",
    "- and finally fit the vectorizer to the text using the 'adapt' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWBSlNHYH63Q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=5000)\n",
    "train_text = train_dataset.map(lambda text, label: text)\n",
    "vectorizer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxBb8rBmrncY"
   },
   "source": [
    "Let's print out the first tokens form the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mx6bPwIHcs"
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew9vtoMusC5x"
   },
   "source": [
    "Note that the first two tokens in the vocabulary are the empty token '', and the unknown token '[UNK]'. The latter is used to mask out-of-vocabulary tokens in the text\n",
    "\n",
    "We can now use the vectorizer to encode a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T7q-VGpINji"
   },
   "outputs": [],
   "source": [
    "text = 'This is my first tweet! It contains one out-of-vocabulary term. Any suggestions for extending this tweet?'\n",
    "encoding = vectorizer([text]).numpy()[0]\n",
    "print('Tweet:     ', text)\n",
    "print('Encoded:   ', encoding) \n",
    "print('Recovered: ',' '.join([vocab[i] for i in encoding]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnpb08OAysG0"
   },
   "source": [
    "Note that the vectorizer is not turning the text into a single vector, but is simply replacing the vocabulary words by their indices. If a word is not present in the dictionary it is replaced by the unknown token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adhzBr8gxol6"
   },
   "source": [
    "Let's have a look at some actual examples from the dataset, printing out the first 6 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVhxZQhlb5pb"
   },
   "outputs": [],
   "source": [
    "for text in batch[0][:6].numpy():\n",
    "  encoding = vectorizer([text]).numpy()[0]\n",
    "  print('Tweet:     ', text.decode(\"utf-8\"))\n",
    "  print('Encoded:   ', encoding) \n",
    "  print('Recovered: ',' '.join([vocab[i] for i in encoding]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVu5mm0BlnIU"
   },
   "source": [
    "## 5.11 Defining the RNN model\n",
    "\n",
    "Now we can define the model, which contains four layers: \n",
    "- an input embedding layer which produces word embeddings of size 64\n",
    "- a bidirectional LSTM layer\n",
    "- 2 dense (aka fully connected) layers that maps the 2 embedding vectors (of size 64) produced by the bidirectional LSTM down to a single neuron   \n",
    "\n",
    "This constitutes a relatively standard basic RNN architecture. (The details of why these specific components are chosen is beyond the scope of this tutorial.)  \n",
    "\n",
    "Once the model has been defined it is compiled in the following step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkBOSHb8IdNq"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorizer,                       \n",
    "    tf.keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()),output_dim=64,mask_zero=True), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'],optimizer=tf.keras.optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZ3xHUUdvdOp"
   },
   "source": [
    "Fit the model by running it for 10 epochs (iterations over the training data).\n",
    "- Note that we provide it with both the training dataset and the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8QbNIKxI375"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=10, validation_data=valid_dataset, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-Xzgsr-3Pl"
   },
   "source": [
    "Once we've trained the model we can check the final accuracy on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cB34Jys-I7p2"
   },
   "outputs": [],
   "source": [
    "valid_loss, valid_acc = model.evaluate(valid_dataset)\n",
    "\n",
    "print('Validation Loss: {}'.format(valid_loss))\n",
    "print('Validation Accuracy: ',valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sCKn20yCLDs"
   },
   "source": [
    "We can have a look at the predictions from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZtp5zCvgoVc"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "tweets.append('I can\\'t believe how much fun I\\'m having learning to train a text classifier with a bidirectional LSTM!')\n",
    "tweets.append('I am really confused. I want my mommy.')\n",
    "tweets.append('The internet connection has been pretty annoying today!')\n",
    "tweets.append('They just played my favourite song on the radio.')\n",
    "tweets.append(\"I don't like going to the dentist.\")\n",
    "tweets.append('I am so happy today!')\n",
    "tweets.append('I am so unhappy today!')\n",
    "\n",
    "predictions = model.predict(tweets)\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "  print('tweet: ',tweets[i])\n",
    "  encoding = vectorizer([tweets[i]]).numpy()[0]\n",
    "  print('encoded as: ',' '.join([vocab[j] for j in encoding]))\n",
    "  print('predicted value: ', predictions[i][0])\n",
    "  print('predicted label: ', 'negative' if (predictions[i]<0) else 'positive')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvAe7kWdCPpv"
   },
   "source": [
    "And calculate the usual evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8FABFCi693C"
   },
   "outputs": [],
   "source": [
    "pred_y = [0 if (pred < 0) else 1 for pred in model.predict(valid_x)]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy: '+ str(accuracy_score(pred_y, valid_y)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix(valid_y, pred_y,normalize='all'),display_labels=['negative','positive'])\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hNoR7FzIVSW"
   },
   "source": [
    "Finally, let's print out the model summary to get an understanding of the number of parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msGvq6XA_qxP"
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8yHHWsbJjtm"
   },
   "source": [
    "Most of the parameters are used to define the embeddiing, then the LSTMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrHVkXHzJrQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
