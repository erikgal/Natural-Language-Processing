{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka6H0qnGSpW9"
      },
      "source": [
        "# Transformer-based Text Classfication\n",
        "\n",
        "This notebook shows hot to make use of pre-trained Transformer models for building various types of text classifiers, some of which are multi-lingual. In particular, the notebook demonstrates the training of BERT-based text classifiers for: \n",
        "- categorising news group messages \n",
        "- detecting sentiment in Twitter messages\n",
        "- determining if a sentence paraphrases another.\n",
        "\n",
        "The notebook makes use of: \n",
        "- Transformer models from HuggingFace for training: https://github.com/huggingface/transformers\n",
        "- Lime for explanations: https://lime-ml.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Much of the notebook is based on: \n",
        "- this blog post: https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "- this Google Colab notebook: https://colab.research.google.com/drive/1YxcceZxsNlvK35pRURgbwvkgejXwFxUt\n",
        "- this notebook: https://github.com/amaiya/ktrain/blob/master/examples/text/MRPC-BERT.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLWptUEpvTfh"
      },
      "source": [
        "## Prepare environment\n",
        "\n",
        "**NOTE**: To run this notebook, you will **need a GPU** (Graphics Processing Unit) card \n",
        "- the notebook has been tested on Google Colab and you are advised to run it there\n",
        "- to use a GPU, you need to click on \"Runtime\" above, then \"Change runtime type\" and for \"Hardware accelerator\" choose the \"GPU\" option\n",
        "\n",
        "Run the code below to check whether there is a gpu available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-l_oq6HmglH"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ktrain"
      ],
      "metadata": {
        "id": "fW5YE12VMOt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 uninstall flask\n",
        "!pip3 install -q eli5"
      ],
      "metadata": {
        "id": "_EfL3Ys_MSNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60uo8Ma8lruI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w7V8Na_gPv2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knkhmHlqgIjx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buLDJl-hZFhr"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type != 'cuda':\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdk5lPu3bxze"
      },
      "source": [
        "## Classification on the 20 Newsgroups dataset\n",
        "\n",
        "In the first example we'll build a multi-class classifier for newsgroup messages, as examples of longer text messages.\n",
        "\n",
        "Training on all of the documents in the dataset will take too long (a few hours), so we'll use just 4 today. \n",
        "- If you want to try on the full dataset, just remove the 'categories' attribute from the commands below. \n",
        "- Be warned though, that training time will be MUCH longer if you include all of the data, so first try subset of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rSbSqApYPBe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', 'alt.atheism']\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True, categories=categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xibNw613s4or"
      },
      "source": [
        "Print out some basic information about the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCyuWfrDtHn7"
      },
      "outputs": [],
      "source": [
        "print('# train instances: ', len(train.data))\n",
        "print('# test instances:  ', len(test.data))\n",
        "print('classes: ', train.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugMhwArFA64S"
      },
      "source": [
        "Let's compute the occurrences of the classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwZL6On6A64S"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(train.target)\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XOMhW8X1j-O"
      },
      "source": [
        "A class count shows that there is a relatively even distribution of messages across the categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VJnzlUStdLl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(counts.keys(), counts.values(), color=\"#3F5D7D\", width=0.8);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wrvSxqqA64U"
      },
      "source": [
        "### Model training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe5xxVPrb4IO"
      },
      "source": [
        "#### Creating a model and preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rctt-zMfkhME"
      },
      "source": [
        "For the moment we will use the standard pretrained Multilingual BERT model. Note that this **model is big**! \n",
        "- it contains 110 million parameters\n",
        "- it represents words (subword tokens to be precise) with embeddings of size 768 <-- compare this with the 50 dimensions we used with GloVe\n",
        "- it contains 12 self-attention layers (each with 12 heads) stacked on top each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-MljyocSpXG"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-multilingual-uncased'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaS6XnAqSpXG"
      },
      "source": [
        "There are many other pretrained models availalable here: https://huggingface.co/transformers/pretrained_models.html\n",
        "- Give them a try and see which ones are faster and/or more stable to train!\n",
        "\n",
        "Load the transformer setting the maximum text length to 500 and the classes to be the four newsgroup categories mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myYmdpaVi6XB"
      },
      "outputs": [],
      "source": [
        "bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Kn0RMEg1ZN"
      },
      "source": [
        "Move model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1rV7KHBwZA9"
      },
      "outputs": [],
      "source": [
        "bert = bert.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuk_C-3zaDeX"
      },
      "source": [
        "Now that we've downloaeded the base Multiligual BERT model, let's have a look at the architecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WnHq1_iZYdSG",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIeFrFNjgf9"
      },
      "source": [
        "Check which of the parameters are trainable: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vGGOvNFchCNB",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "for param in bert.parameters():\n",
        "    print(param.size(), param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGePPmboBHT"
      },
      "source": [
        "Load the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUbB1gjAoDnE"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4ju07NsN24"
      },
      "source": [
        "Now we can preprocess the training (and test) data using the transformer's data set API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBfcR-j-hjWI"
      },
      "source": [
        "First create a list of dictionaries with all the samples in the train and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pw8W1OIrK6R"
      },
      "outputs": [],
      "source": [
        "train_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(train.data, train.target)]\n",
        "test_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(test.data, test.target)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32C0MMkLhr2g"
      },
      "source": [
        "Convert the data sets into the Huggingface data set API format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvpGAb7us2ij"
      },
      "outputs": [],
      "source": [
        "train_data = Dataset.from_list(train_data)\n",
        "test_data = Dataset.from_list(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTYfG4twhyBL"
      },
      "source": [
        "Create a container with all the splits of the data set\n",
        "\n",
        "**Note**: We are using the same data for validation and testing, this is not a good practice, it's just for the tutoria, in general you should have separate data for validation and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSxUkZy6tcew"
      },
      "outputs": [],
      "source": [
        "data = DatasetDict()\n",
        "data['train'] = train_data\n",
        "data['validation'] = test_data\n",
        "data['test'] = test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM1xuM2th2SL"
      },
      "source": [
        "Finally use the tokenizer to convert the input strings into sequences of tokens (more on this later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPOAbmdwsMaG"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "tokenized_data = data.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jDlmkQAfFNI"
      },
      "source": [
        "Let's have a quick look at what the tokenised data looks like: \n",
        "- First print out the text of the first document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZVwnMckfgba"
      },
      "outputs": [],
      "source": [
        "print(train.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJc8ABEBSpXH"
      },
      "source": [
        "Now print out the result of preprocessing the first document: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO_FOkTdSpXH"
      },
      "outputs": [],
      "source": [
        "print(tokenizer(train.data[0]).input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VolZFjl9f61n"
      },
      "source": [
        "Each integer above is the index of a token from the vocabulary of the BERT model. \n",
        "- How big is the vocabulary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zYUTkJUgatH"
      },
      "outputs": [],
      "source": [
        "print(\"vocabulary size: \", len(tokenizer.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXZgNOrmgoTJ"
      },
      "source": [
        "This vocabulary is relatively small compared to many word embeddings discussed in the lecture, which often reach over 1 million distinct words. \n",
        "- The vocab size is kept relatively small through the use of sub-word tokens that are found using a byte-pair encoding scheme.\n",
        "- We can take a single string and run it through the tokenizer to see the tokens produced as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP3gdwRe1AVL"
      },
      "outputs": [],
      "source": [
        "example_text = \"I always wondered what those Transformer models were doing to my text. Here is a OutOfVocabWord.\"\n",
        "encoded_text = tokenizer(example_text).input_ids\n",
        "vocab_terms = list(tokenizer.vocab.keys())\n",
        "vocab_index = list(tokenizer.vocab.values())\n",
        "print(\"input text: \"+example_text)\n",
        "print(\"tokenized:  \", [vocab_terms[vocab_index.index(i)] for i in encoded_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQRSfSWWhFnF"
      },
      "source": [
        "Note that:\n",
        "- a '[CLS]' token has been added to the start of the tokenized text and a '[SEP]' token to the end.\n",
        "- BERT works as a masked autoencoder, meaning that it is trained to produce the same terms as output that are presented in input, including those that were masked out.\n",
        "- the '[CLS]' token is simply a special mask for the unknown class label that needs to be produced at the output. \n",
        "- the Byte-Pair Encoding has removed the suffixes from certain words (e.g. 'wondered' lost the 'ed'), but not all of them ('models' kept the 's'). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sGPJgOcBTd"
      },
      "source": [
        "#### Training the model\n",
        "\n",
        "Now we're ready to start training the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO6NOLH2qIOP"
      },
      "source": [
        "Prepare training arguments (including a name for the trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyLEhf2kvlxc"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"cool_trainer_name\", \n",
        "    per_device_train_batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUPaP_OPjmFd"
      },
      "source": [
        "Disable weights in model encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um37Z8bLwwHg"
      },
      "outputs": [],
      "source": [
        "for param in bert.bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewvcli7wkAWb"
      },
      "source": [
        "Build the traininer using the Huggingface trainer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doJDqHl6opqa"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=bert, \n",
        "    args=training_args, \n",
        "    train_dataset=tokenized_data['train'], \n",
        "    eval_dataset=tokenized_data['validation']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pji9wH6rkLfl"
      },
      "source": [
        "Finally, run the training process (we train for three epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVm_LxBEopvX"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljEgcP9Pkkb1"
      },
      "source": [
        "Sorry Mark, no learning rate plot :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6eSo9IcI3_"
      },
      "source": [
        "#### Evaluating the model\n",
        "\n",
        "Now that the model has finished training, let's evaluate in on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8NZVwVktii"
      },
      "source": [
        "First we can run the evaluation process included with the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB1pCPU10Jwd"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8AV-Qi9k7Te"
      },
      "source": [
        "Then we can get the predictions on the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPoK1yoq1IRf"
      },
      "outputs": [],
      "source": [
        "preds = trainer.predict(tokenized_data['test'])\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpVT2tEmh0S"
      },
      "source": [
        "Convert predicted logits to classes using the $\\mathrm{argmax}$ operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tjOw1YYmSHI"
      },
      "outputs": [],
      "source": [
        "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNOiC6iuQYg"
      },
      "source": [
        "Get target label names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CmMFBNouPA-"
      },
      "outputs": [],
      "source": [
        "label_names = test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyjAzeak_uV"
      },
      "source": [
        "... and use them to preprare a classification report using Scikit-Learn API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcxCvLOcOje"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test.target, y_pred, target_names=label_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRb1I-JlIMF"
      },
      "source": [
        "... or compute a confusion matrix using the Scikit-Learn API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73YiamzdlI6t"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(test.target, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhG3fPtPcVKe"
      },
      "source": [
        "We can have a look at the test examples on which the model made the worst predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzX6OeNZqYIu"
      },
      "source": [
        "Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCABLebacTWM"
      },
      "outputs": [],
      "source": [
        "loss = torch.nn.functional.cross_entropy(torch.tensor(preds.predictions), torch.tensor(preds.label_ids), reduction='none')\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j_ACNkMqcKL"
      },
      "source": [
        "Identify the index of the element with the highest loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4-Ts24aqWQW"
      },
      "outputs": [],
      "source": [
        "index_of_instance = torch.argmax(loss).item()\n",
        "index_of_instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymQd8xY3qugI"
      },
      "source": [
        "Loss of the corresponding sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJoHPh7VqWbN"
      },
      "outputs": [],
      "source": [
        "loss[index_of_instance].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjuo7nRcq7Bm"
      },
      "source": [
        "Predicted class and target class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXWwP5IDq-JP"
      },
      "outputs": [],
      "source": [
        "print(f'Target class:    {label_names[test.target[index_of_instance]]}')\n",
        "print(f'Predicted class: {label_names[y_pred[index_of_instance]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJG_H42_b3ra"
      },
      "source": [
        "Print out the post with the top loss (by changing INDEX_OF_INSTANCE below) to try to find out why it is confusing the classfier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHYRBdBycfne"
      },
      "outputs": [],
      "source": [
        "print(test.data[index_of_instance])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcZQ6HbqdMcF"
      },
      "source": [
        "### Making predictions on new data\n",
        "\n",
        "We can run the trained classification model on new examples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkP9KKyA64n"
      },
      "source": [
        "#### Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpnQiMKyrbvd"
      },
      "source": [
        "Define a simple function to make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Sqrv83frb5Z"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    lbl = label_names[torch.argmax(outputs.logits).item()]\n",
        "\n",
        "    return lbl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxQIB1-0rcCv"
      },
      "source": [
        "Define  simple function to predict the classes probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUYdUWthrcMz"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
        "\n",
        "    \n",
        "    return dict(zip(label_names, proba))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyjLvuMus_fw"
      },
      "source": [
        "We can now try the trained model on new samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp8tw3Y0cnJa"
      },
      "outputs": [],
      "source": [
        "sample_text = 'Both of the home runs hit by Fernando Tatís in the third inning for the St. Louis Cardinals on April 23, 1999, were grand slams.'\n",
        "predict(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYqzU7nJhROw"
      },
      "source": [
        "We can see the probabilities assigned by the model to each of the classes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHT1jVTGi0yJ"
      },
      "outputs": [],
      "source": [
        "predict_proba(sample_text).values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2p7Xojzi8uB"
      },
      "source": [
        "Where the classes are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg93RQvMjAFd"
      },
      "outputs": [],
      "source": [
        "predict_proba(sample_text).keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H3TOwIgSpXL"
      },
      "source": [
        "OK, so the model appears to work well on English text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmWpMvoiPha"
      },
      "source": [
        "#### Multi-lingual examples\n",
        "\n",
        "Let's try on a phrase in another language (Italian). \n",
        "- Note that the news group data we used to train the model is in English only, so the model has never seen any examples of news group messages in Italian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZOeu9cDdguM"
      },
      "outputs": [],
      "source": [
        "italian_text = 'I tasti su e giù sul mio portatile non funzionano più, quindi non posso più usarlo per giocare ;-('\n",
        "predict(italian_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiCLME7Dnoye"
      },
      "outputs": [],
      "source": [
        "predict_proba(italian_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWcw5VGIinK5"
      },
      "source": [
        "Did it get the prediction right? \n",
        "- Note that while the characer set for Italian and English is almost the same, there weren't any words (except perhaps for \"non\") in the above message that would have occurred in the training data. \n",
        "\n",
        "Let's try a message in a different character set: \n",
        "- Translate the text into Chinese using Google Translate: https://translate.google.com/\n",
        "- Then copy the text below and rerun the prediction. \n",
        "- Does it still work? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuMmx8f5dr45"
      },
      "outputs": [],
      "source": [
        "chinese_text = '笔记本电脑上的向上和向下键不再起作用，因此我无法再使用它来播放;-(' #'INSERT CHINESE TEXT HERE'\n",
        "predict(chinese_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OiNy8look-l"
      },
      "outputs": [],
      "source": [
        "russian_text = \"Клавиши «вверх» и «вниз» на моем ноутбуке больше не работают, поэтому я больше не могу использовать их для игры ;-(\"\n",
        "predict(russian_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1r31KjnjU2m"
      },
      "source": [
        "### Explaining the predictions\n",
        "\n",
        "Let's try another piece of text, this time in English, but talking about a politician who wasn't a politician (but rather a celebrity) at the time in which the 20 News Groups dataset was created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXJ9qVdIjmlI"
      },
      "outputs": [],
      "source": [
        "sample_text = 'Donald Trump is the greatest living ex-president in the history of living ex-presidents. He\\'s also a bad golfer.'\n",
        "predict(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tHos7V6d8RQ"
      },
      "source": [
        "Let's invoke the `explain` method to see which words contribute most to the classification.\n",
        "\n",
        "We will need to install the **eli5** library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjruXVE47G9d"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "from eli5.lime import TextExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3GfJCLl2sq"
      },
      "source": [
        "Define funxtion to output the probability distribution as a matrix `(n_samples, n_classes)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfYGGX-mlYnS"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba_values(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt', padding=True).to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
        "\n",
        "    return proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWR5fodljOV5"
      },
      "source": [
        "Run the explainer (based on LIME) to see which words in the sentence are most important for the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQxI9hUK59oc"
      },
      "outputs": [],
      "source": [
        "# See https://amaiya.github.io/ktrain/text/predictor.html\n",
        "# See https://eli5.readthedocs.io/en/latest/autodocs/lime.html\n",
        "def explain(text, truncate_len=512, all_targets=False, n_samples=2500):\n",
        "    prediction = [predict(text)] if not all_targets else None\n",
        "    text = \" \".join(text.split()[:truncate_len])\n",
        "    te = TextExplainer(random_state=42, n_samples=n_samples)\n",
        "    _ = te.fit(text, predict_proba_values)\n",
        "    return te.show_prediction(\n",
        "        target_names=label_names, targets=prediction\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HgZDLYUeVaM"
      },
      "outputs": [],
      "source": [
        "explain(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcph2bSLe5cW"
      },
      "source": [
        "The words in the darkest shade of green contribute most to the classification.\n",
        "- Do they agree with what you would have expected for this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wceyWpdC-C1H"
      },
      "source": [
        "### Inspecting the Model\n",
        "\n",
        "Let's have a look at the architecture of the model that we have used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISHuI-fpSpXT"
      },
      "outputs": [],
      "source": [
        "print(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03oHrJjZvVxT"
      },
      "source": [
        "Compute the total number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgL3V56lvWGD"
      },
      "outputs": [],
      "source": [
        "n_params = sum(param.numel() for param in bert.parameters())\n",
        "n_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erGkWyP_WJWg"
      },
      "source": [
        "The model contains over 100 million parameters consists of:\n",
        "- BERT (which contains alost all the parameters), \n",
        "- a drop-out layer (that is inserted to prevent overfitting), and \n",
        "- a dense (feedforward) layer to map the BERT embedding of size 768 to the 2 output neurons (representing the positive and negative classes).\n",
        "\n",
        "Let's print out information on the configuration of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiz0p_Zo7eB8"
      },
      "outputs": [],
      "source": [
        "bert.bert.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-YjRXLmXo6"
      },
      "source": [
        "### Saving and loading a fine-tuned model\n",
        "\n",
        "We can save predictor for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1nzxI_Jec-5"
      },
      "outputs": [],
      "source": [
        "bert.save_pretrained('/tmp/my_predictor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_ImfZ2Bf1M"
      },
      "source": [
        "Reload the predictor and use it to predict the class of a new piece of text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDEU2s03fHsw"
      },
      "outputs": [],
      "source": [
        "bert = AutoModelForSequenceClassification.from_pretrained('/tmp/my_predictor').to(device)\n",
        "predict('My computer monitor is really blurry.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bjyrTvao3CD"
      },
      "source": [
        "## Classifying the sentiment of Twitter messages\n",
        "\n",
        "We'll now train a different model to detect sentiment on Twitter data as we did for the previous tutorials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCJgsiUzg1wg"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "import re \n",
        "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
        "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
        "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]\n",
        "\n",
        "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
        "tweets_y = ['positive']*len(positive_tweets) + ['negative']*len(negative_tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LuKhzRHvmYA"
      },
      "source": [
        "Use scikit-learn to split the data into training, validation and test sets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWghucxwufOf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "temp_x, test_x, temp_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2)\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(temp_x, temp_y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8svgXSwbYc"
      },
      "source": [
        "This time we are going to use a different tool called [ktrain](https://github.com/amaiya/ktrain): "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ktrain\n",
        "from ktrain import text"
      ],
      "metadata": {
        "id": "0uwolFPLIB-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Load the model (this time we'll try DistilBERT, which is a smaller transformer model)"
      ],
      "metadata": {
        "id": "QPBbalh8IEOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM226FFCuhYk"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "transformer = text.Transformer(MODEL_NAME, maxlen=500, classes=['positive','negative'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFe12_blSpXP"
      },
      "source": [
        "2. Process the training/test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwgWazABSpXP"
      },
      "outputs": [],
      "source": [
        "processed_train = transformer.preprocess_train(train_x, train_y)\n",
        "processed_test = transformer.preprocess_test(valid_x, valid_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5PhBw9SpXP"
      },
      "source": [
        "3. Create a model and learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV_5oz6PSpXQ"
      },
      "outputs": [],
      "source": [
        "model = transformer.get_classifier()\n",
        "learner = ktrain.get_learner(model, train_data=processed_train, val_data=processed_test, batch_size=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5OI2QBSpXQ"
      },
      "source": [
        "4.   Train the model (We'll run for one epoch to make it faster, but it would be better to run for 4 or more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F886bGFlxoQL"
      },
      "outputs": [],
      "source": [
        "learner.fit_onecycle(5e-5, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtIofrVNSpXQ"
      },
      "source": [
        "5.   Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smj0NtNWx6sK"
      },
      "outputs": [],
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU0kI3s3SpXQ"
      },
      "source": [
        "6.   Make some predictions with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE6X0pAYx8Mi"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc=transformer)\n",
        "predictor.predict('Had a lot of fun today!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkORO7iQcCIl"
      },
      "source": [
        "Did it work?\n",
        "\n",
        "Let's print out some information about the architecture of this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brb3vCVpcCia"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag6GSzlohyph"
      },
      "source": [
        "We see that the distilbert model has fewer parameters than the BERT model we used previously. It's interesting to see the pre-classifier layer in this model that maps the output embeddings from the transformer into a second embedding space (of the same size) that is then used as features for the final classification layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVGz2LalY93G"
      },
      "source": [
        "## Pairwise classification task \n",
        "\n",
        "We now use the Microsoft Research Paraphrase Corpus (MRPC) to build a classifier for detecting paraphrased sentences. \n",
        "- This is an example of a paired sentence classfication task, where the prediction model takes in a **pair of texts as input** and produces a **binary label as output**.\n",
        "\n",
        "We'll first use pandas (Python's data analysis library) to download and parse the tab-separated data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNyAj2W4yElI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "train_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'\n",
        "test_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'\n",
        "train_df = pd.read_csv(train_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "test_df = pd.read_csv(test_file, delimiter='\\t', quoting=csv.QUOTE_NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0G1mLT1SpXR"
      },
      "source": [
        "We need to format the training data in the format needed for ktrain\n",
        "- so select the two columns containing strings \n",
        "- and get their values (as an 2d array) \n",
        "- then convert the array to a list of tuples (str,str) as required by ktrain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onWjDOIXSpXR"
      },
      "outputs": [],
      "source": [
        "x_train = train_df[['#1 String', '#2 String']].values\n",
        "# for sentence pair classification, ktrain expects a list of tuples of form (str, str)\n",
        "x_train = list(map(tuple, x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbuIu7pSpXR"
      },
      "source": [
        "Repeat for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u7EnB7WSpXR"
      },
      "outputs": [],
      "source": [
        "x_test = test_df[['#1 String', '#2 String']].values\n",
        "x_test = list(map(tuple, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xjeE5LySpXR"
      },
      "source": [
        "And of course, we need also the labels for the training / test examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PInAix7dSpXR"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['Quality'].values\n",
        "y_test = test_df['Quality'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-47XE1nf5ia"
      },
      "source": [
        "Print out sizes of dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PXCUQtKe0Hk"
      },
      "outputs": [],
      "source": [
        "print(\"# train instances: \",len(x_train))\n",
        "print(\"# test instances:  \",len(x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jKOFDNqkkTR"
      },
      "source": [
        "Have a look at a random sentence pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYhG2fI0f-Y8"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "i = random.randint(0,len(x_train)-1)\n",
        "print(\"Sentance 1: \"+x_train[i][0])\n",
        "print(\"Sentence 2: \"+x_train[i][1])\n",
        "print(\"Label:      \"+str(y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuK5uMvlk25Z"
      },
      "source": [
        "Build and train a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FTGq3kgkq5P"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'bert-base-uncased'\n",
        "transformer = text.Transformer(MODEL_NAME, maxlen=128, class_names=['not paraphrase', 'paraphrase'])\n",
        "processed_train = transformer.preprocess_train(x_train, y_train)\n",
        "processed_test = transformer.preprocess_test(x_test, y_test)\n",
        "model = transformer.get_classifier()\n",
        "learner = ktrain.get_learner(model, train_data=processed_train, val_data=processed_test, batch_size=32)\n",
        "learner.fit_onecycle(5e-5, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKlnZJim8L2L"
      },
      "source": [
        "Get the predictor object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBj4rbzdk-TR"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKPEWtkUSpXS"
      },
      "source": [
        "And try the model on a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vefxX5PllN8K"
      },
      "outputs": [],
      "source": [
        "#predictor.predict(('Their chestnut cat stretched out its legs on the antique wooden desk in the centre of his study.','The cat was on the table.'))\n",
        "predictor.predict(('The cat stretched out on the antique wooden desk in the study.','The cat was on the table.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hwiVBJD9Oci"
      },
      "source": [
        "Did it work? \n",
        "- Try it out on some of your own sentences!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wdznm-9MEE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}