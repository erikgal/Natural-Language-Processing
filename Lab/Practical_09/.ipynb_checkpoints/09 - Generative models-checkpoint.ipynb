{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee5ea9f-7715-4e08-b572-fa8116eb67f4",
   "metadata": {},
   "source": [
    "# Generative models\n",
    "\n",
    "This tutorial will show how to use generative models to solve:\n",
    "- Question answering\n",
    "- Dialogue generation\n",
    "However, the same prinicples are applicable to \n",
    "\n",
    "Yay we made it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf357018-5db6-4509-8ae8-dd3784f9023e",
   "metadata": {},
   "source": [
    "## Generative Question Answering (QA)\n",
    "\n",
    "Up to now we have seen how to retrieve relevant passages that may contain the answer to a question.\n",
    "- What if the answer is not written explicitly in the passage?\n",
    "- What if the passage is too long to read (and our lives are too dynamic to spend more than one minute reading)?\n",
    "\n",
    "We can train a model to output the answer to a question given \n",
    "- a relevant passage (and we know how to gather relevant documents)\n",
    "- the question (yes, the question contains relevant information to answer the question)\n",
    "... Or we can put a pre-trained model on top of our retreival pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d9c7d-7c90-4144-b463-db21b2e21248",
   "metadata": {},
   "source": [
    "### QA data preparation\n",
    "\n",
    "In this section we will be using the [WikiQA](https://aclanthology.org/D15-1237/) data set.\n",
    "It's a data set for open domain generative QA.\n",
    "\n",
    "It's avaialble via the HuggingFace data set package, let's install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db5f1d-1eb1-4f52-8c90-1e819bd311d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc87bcb-d94c-4060-8c2f-b13ab75fd3f4",
   "metadata": {},
   "source": [
    "Let's download the validation split of the WikiQA data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582af4f-05fb-4085-aaa8-2d83ea0d9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "wiki_qa = datasets.load_dataset('wiki_qa', split='validation')\n",
    "wiki_qa[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee165bb0-5ed0-4d08-a566-d19af55d471b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2ed46-242f-46fc-846b-90a07f85c871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a3cd78-2102-4b0d-b1c6-2a4e7f18e635",
   "metadata": {},
   "source": [
    "### Knowlege preparation\n",
    "\n",
    "We have the questions (and the target answers), now we need to prepare our knowledge source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c3a4a-1164-4d66-9b8b-0a03190d0791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "427fc98d-1d8b-412f-a804-574d6eac9201",
   "metadata": {},
   "source": [
    "### Answering a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f270d8-1cf4-4b39-8bd9-5312646378c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e342af-1b52-46b4-8c44-d83f4c854a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ad7316-03ba-4f90-a847-774bc3543bd8",
   "metadata": {},
   "source": [
    "### Putting all together\n",
    "\n",
    "We can finally set up an entire question answering pipeline:\n",
    "- We have the knowledge\n",
    "- We have the retreival system\n",
    "    - We also have the re-ranking system\n",
    "- We have the asnwering system\n",
    "\n",
    "Let's define a function that puts everything together and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88153175-be77-451b-8955-f5851a45d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6bb86-3d6c-4b2f-a963-5da9764593b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44595052-3562-4b0d-b06b-f290f9e7fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b6bb6-2714-4654-a2be-7fe4f9db2e1f",
   "metadata": {},
   "source": [
    "## Generative chatbots\n",
    "\n",
    "Generative chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e15b7-4cbf-4953-9bba-e7f48d5bc4af",
   "metadata": {},
   "source": [
    "### Pretrained models\n",
    "\n",
    "For starter let's play around with a pre-trained model.\n",
    "We can load the [DialoGPT](https://arxiv.org/abs/1911.00536) chatbot, a fine-tuning of GPT-2 trained o large collections of conversations crawled from Reddit.\n",
    "\n",
    "We can start seeing different ways to decode (generate) responses using this autoregressive model.\n",
    "What we want to do is use the output probability distribution to select a token compsing a response.\n",
    "Hopefully we select the most probable sequence, actually that's not feasible.\n",
    "\n",
    "Let's proceed step-by-step.\n",
    "First of all get model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d201fd9-c66d-429f-84f5-ec7090c13f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f7a3e35-8a2c-420c-86d1-bd832c1c1b8c",
   "metadata": {},
   "source": [
    "#### How does it work?\n",
    "\n",
    "First we need to understand how to provide data to our model\n",
    "\n",
    "Up to a couple of years ago, the standard appraoch to present the input to these models was to separate each utterance with a `end-of-sequence` token.\n",
    "The model would generate an answer and stop every time the `end-of-sequence` tokens is generated.\n",
    "\n",
    "```\n",
    "\"<|endoftext|>Summer loving had me a blast<|endoftext|>Summer loving happened so fast<|endoftext|>I met a girl crazy for me<|endoftext|>Met a boy cute as can be<|endoftext|>\"\n",
    "```\n",
    "\n",
    "Nowadays the appraoch is to have an uninterrupted stream of text, like a movie script\n",
    "\n",
    "```\n",
    "\"\n",
    "A: Hello.\n",
    "B: Is it me you're looking for?\n",
    "A: I can see it in your eyes...\n",
    "B: I can see it in your smile!\n",
    "\"\n",
    "```\n",
    "\n",
    "DialoGPT uses the `end-of-sequence` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017b027-48e2-49c9-9947-efc7175899f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"Hello, how are you?\", \n",
    "    \"I'm fine thaks, how about you?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8eae1-5c9a-4c31-8a18-119e5fb35f41",
   "metadata": {},
   "source": [
    "Encode input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b00b79-1ac5-4eff-8b9f-92c20afd5601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4b824b7-f7f9-4c43-ab88-54edb088faff",
   "metadata": {},
   "source": [
    "If we run the sequence through the model, we get a series of logits as output.\n",
    "Since we are using an autogressive models, in the rightmost position we will have the logits of next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e39f77-7c7c-4760-bba3-6a27399813ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36be972b-5239-453f-ba21-5848608d1b11",
   "metadata": {},
   "source": [
    "We can run these logits through a $\\mathrm{softmax}(\\cdot)$ and obtain the probability distribution over tokens:\n",
    "- for each possible token we have the probability of it being the next in the sequence\n",
    "- We can sample a token from this probability distribution and recurr itin input to get a new token\n",
    "- We can iterate this process to compose a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf5181-49e9-4a77-9aa6-e5396f9f130d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6599151a-ae4e-4442-90be-bae9d3d36342",
   "metadata": {},
   "source": [
    "#### Deterministic decoding\n",
    "\n",
    "Deterministic appraoches yield always the same output for a given input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac99e9-8968-4e17-84d2-629609a3ead4",
   "metadata": {},
   "source": [
    "##### Greedy decoding\n",
    "\n",
    "The most starightforward way is to pick each time the most probable token and recurr it as next step in input.\n",
    "Very suboptimal solution, usually yields dull responses like `\"I don't know\"` or causes degenerate generation (e.g., repeating the same token many times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7406082-e460-44af-beca-c256023c1e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a59866a-0764-4b86-a055-099fc99a1f83",
   "metadata": {},
   "source": [
    "##### Beam search\n",
    "\n",
    "We cannot do an exhaustuve search, but we can keep the top $n$ most probable sequences up to now.\n",
    "This is what beam search does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf9b6e-7e5c-4e5b-9dd1-597512bed9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "233a3836-857e-4e43-ae25-40fc042ae980",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "\n",
    "Sampling based decoding adds more spice to the output sampling the next token with a certain probability given by the language model.\n",
    "The nice thing is that given the same input the generated content may change (higher diversity in the text of responses), the bad thing is that given the same input the generated content may change (possibly inconsistent behaviour)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768d0db-61f4-4095-a6f8-169c5ebf54b0",
   "metadata": {},
   "source": [
    "##### Temperature rescoring\n",
    "\n",
    "Divide the logits by a value $\\tau$:\n",
    "- if $\\tau > 1$ (high temprature) the distribution get softer (reduces probability of most probable tokens and increases that of least probable)\n",
    "- if $\\tau = 1$ the distribution is unchanged\n",
    "- if $\\tau < 1$ (low temperature) the distribution get sharper (reduces probability of most probable tokens and increases that of least probable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d6b87-6852-46c3-91b5-f6cf25bd3ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b9ebd2-8734-49fa-bfa9-7c5066a3acfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Top-k\n",
    "\n",
    "Consider only first $k$ most probable tokens and zero out others probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4e683-53d5-429e-b0a4-36f532327e4b",
   "metadata": {},
   "source": [
    "##### Top-p (nucleus sampling)\n",
    "\n",
    "Consider only first most probable tokens so that their probability sum up to $p \\in [0, 1] \\subseteq \\mathbb{R}$ and zero out others probabilities \n",
    "Similar to top-$k$ but variable window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6ebb9-3a97-4c16-9f79-5578fd186fc7",
   "metadata": {},
   "source": [
    "##### Contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fe2bd-3208-401e-a2f2-875dbc66910f",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f452a-378d-4d1f-9059-6de7c163a85e",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fb3ec-83a3-4cf9-9aaf-1791e0a50e54",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfc6b7-46a9-4e2f-bf22-62de72cdb4a0",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027b0f6-1e3a-4db1-b826-2b1d73a3993d",
   "metadata": {},
   "source": [
    "## ELIZA meets DialoGPT\n",
    "\n",
    "In the 70s they made ELIZA and PARRY meet each other: https://www.theatlantic.com/technology/archive/2014/06/when-parry-met-eliza-a-ridiculous-chatbot-conversation-from-1972/372428/\n",
    "We could you have ELIZA meet ChatGPT, but since we are humble we will settle with DialoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426eff4-335d-4b33-b5ec-fa5ebd43102a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09c196a-feb5-4540-a96f-1619dcaa2e22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734f32d3-8fee-4316-b717-eb67bf5e6c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33559208-537b-436d-b4c3-a103fe951bff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aad7262-3e1c-42a1-b16c-8b3b492865c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
